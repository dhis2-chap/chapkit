{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Chapkit","text":"<p>Build production-ready ML services with train/predict workflows, artifact storage, config management, and job scheduling - all in a few lines of code.</p>"},{"location":"#quick-start-ml-service","title":"Quick Start: ML Service","text":"<pre><code>from chapkit import BaseConfig\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo\nfrom chapkit.artifact import ArtifactHierarchy\nfrom chapkit.ml import FunctionalModelRunner\nimport pandas as pd\n\nclass MyMLConfig(BaseConfig):\n    \"\"\"Configuration for your ML model.\"\"\"\n\nasync def train_model(config, data, geo=None):\n    \"\"\"Train your model - returns trained model object.\"\"\"\n    from sklearn.linear_model import LinearRegression\n    model = LinearRegression()\n    model.fit(data[[\"feature1\", \"feature2\"]], data[\"target\"])\n    return model\n\nasync def predict(config, model, historic, future, geo=None):\n    \"\"\"Make predictions using the trained model.\"\"\"\n    predictions = model.predict(future[[\"feature1\", \"feature2\"]])\n    future[\"predictions\"] = predictions\n    return future\n\n# Build complete ML service with one builder\napp = (\n    MLServiceBuilder(\n        info=MLServiceInfo(display_name=\"Disease Prediction Service\"),\n        config_schema=MyMLConfig,\n        hierarchy=ArtifactHierarchy(name=\"ml\", level_labels={0: \"model\", 1: \"predictions\"}),\n        runner=FunctionalModelRunner(on_train=train_model, on_predict=predict),\n    )\n    .with_monitoring()  # Optional: Add Prometheus metrics\n    .build()\n)\n</code></pre> <p>What you get: - <code>POST /api/v1/ml/train</code> - Train models with versioning - <code>POST /api/v1/ml/predict</code> - Make predictions - <code>GET /api/v1/configs</code> - Manage model configurations - <code>GET /api/v1/artifacts</code> - Browse trained models and predictions - <code>GET /api/v1/jobs</code> - Monitor training/prediction jobs - <code>GET /health</code> - Health checks - <code>GET /metrics</code> - Prometheus metrics (with <code>.with_monitoring()</code>)</p> <p>Run with: <code>fastapi dev your_file.py</code> \u2192 Service ready at <code>http://localhost:8000</code></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add chapkit\n</code></pre> <p>Chapkit automatically installs servicekit as a dependency.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Repository</li> <li>Issues</li> <li>Servicekit - Core framework foundation (docs)</li> </ul>"},{"location":"#license","title":"License","text":"<p>AGPL-3.0-or-later</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for all chapkit modules, classes, and functions.</p>"},{"location":"api-reference/#artifact-module","title":"Artifact Module","text":"<p>Hierarchical storage system for models, data, and experiment tracking.</p>"},{"location":"api-reference/#models","title":"Models","text":""},{"location":"api-reference/#chapkit.artifact.models","title":"<code>models</code>","text":"<p>Artifact ORM model for hierarchical data storage.</p>"},{"location":"api-reference/#chapkit.artifact.models-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.models.Artifact","title":"<code>Artifact</code>","text":"<p>               Bases: <code>Entity</code></p> <p>ORM model for hierarchical artifacts with parent-child relationships.</p> Source code in <code>src/chapkit/artifact/models.py</code> <pre><code>class Artifact(Entity):\n    \"\"\"ORM model for hierarchical artifacts with parent-child relationships.\"\"\"\n\n    __tablename__ = \"artifacts\"\n\n    parent_id: Mapped[ULID | None] = mapped_column(\n        ULIDType,\n        ForeignKey(\"artifacts.id\", ondelete=\"SET NULL\"),\n        nullable=True,\n        index=True,\n    )\n\n    parent: Mapped[Artifact | None] = relationship(\n        remote_side=\"Artifact.id\",\n        back_populates=\"children\",\n    )\n\n    children: Mapped[list[Artifact]] = relationship(\n        back_populates=\"parent\",\n    )\n\n    data: Mapped[Any] = mapped_column(PickleType(protocol=4), nullable=False)\n    level: Mapped[int] = mapped_column(default=0, nullable=False, index=True)\n</code></pre>"},{"location":"api-reference/#schemas","title":"Schemas","text":""},{"location":"api-reference/#chapkit.artifact.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for hierarchical artifacts with tree structures.</p>"},{"location":"api-reference/#chapkit.artifact.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactIn","title":"<code>ArtifactIn</code>","text":"<p>               Bases: <code>EntityIn</code></p> <p>Input schema for creating or updating artifacts.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactIn(EntityIn):\n    \"\"\"Input schema for creating or updating artifacts.\"\"\"\n\n    data: Any\n    parent_id: ULID | None = None\n    level: int | None = None\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactOut","title":"<code>ArtifactOut</code>","text":"<p>               Bases: <code>EntityOut</code></p> <p>Output schema for artifact entities.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactOut(EntityOut):\n    \"\"\"Output schema for artifact entities.\"\"\"\n\n    data: JsonSafe\n    parent_id: ULID | None = None\n    level: int\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode","title":"<code>ArtifactTreeNode</code>","text":"<p>               Bases: <code>ArtifactOut</code></p> <p>Artifact node with tree structure metadata.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactTreeNode(ArtifactOut):\n    \"\"\"Artifact node with tree structure metadata.\"\"\"\n\n    level_label: str | None = None\n    hierarchy: str | None = None\n    children: list[\"ArtifactTreeNode\"] | None = None\n\n    @classmethod\n    def from_artifact(cls, artifact: ArtifactOut) -&gt; Self:\n        \"\"\"Create a tree node from an artifact output schema.\"\"\"\n        return cls.model_validate(artifact.model_dump())\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode.from_artifact","title":"<code>from_artifact(artifact)</code>  <code>classmethod</code>","text":"<p>Create a tree node from an artifact output schema.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>@classmethod\ndef from_artifact(cls, artifact: ArtifactOut) -&gt; Self:\n    \"\"\"Create a tree node from an artifact output schema.\"\"\"\n    return cls.model_validate(artifact.model_dump())\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy","title":"<code>ArtifactHierarchy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for artifact hierarchy with level labels.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactHierarchy(BaseModel):\n    \"\"\"Configuration for artifact hierarchy with level labels.\"\"\"\n\n    name: str = Field(..., description=\"Human readable name of this hierarchy\")\n    level_labels: Mapping[int, str] = Field(\n        default_factory=dict,\n        description=\"Mapping of numeric levels to labels (0 -&gt; 'train', etc.)\",\n    )\n\n    model_config = {\"frozen\": True}\n\n    hierarchy_key: ClassVar[str] = \"hierarchy\"\n    depth_key: ClassVar[str] = \"level_depth\"\n    label_key: ClassVar[str] = \"level_label\"\n\n    def label_for(self, level: int) -&gt; str:\n        \"\"\"Get the label for a given level or return default.\"\"\"\n        return self.level_labels.get(level, f\"level_{level}\")\n\n    def describe(self, level: int) -&gt; dict[str, Any]:\n        \"\"\"Get hierarchy metadata dict for a given level.\"\"\"\n        return {\n            self.hierarchy_key: self.name,\n            self.depth_key: level,\n            self.label_key: self.label_for(level),\n        }\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy.label_for","title":"<code>label_for(level)</code>","text":"<p>Get the label for a given level or return default.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>def label_for(self, level: int) -&gt; str:\n    \"\"\"Get the label for a given level or return default.\"\"\"\n    return self.level_labels.get(level, f\"level_{level}\")\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy.describe","title":"<code>describe(level)</code>","text":"<p>Get hierarchy metadata dict for a given level.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>def describe(self, level: int) -&gt; dict[str, Any]:\n    \"\"\"Get hierarchy metadata dict for a given level.\"\"\"\n    return {\n        self.hierarchy_key: self.name,\n        self.depth_key: level,\n        self.label_key: self.label_for(level),\n    }\n</code></pre>"},{"location":"api-reference/#repository","title":"Repository","text":""},{"location":"api-reference/#chapkit.artifact.repository","title":"<code>repository</code>","text":"<p>Artifact repository for hierarchical data access with tree traversal.</p>"},{"location":"api-reference/#chapkit.artifact.repository-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository","title":"<code>ArtifactRepository</code>","text":"<p>               Bases: <code>BaseRepository[Artifact, ULID]</code></p> <p>Repository for Artifact entities with tree traversal operations.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>class ArtifactRepository(BaseRepository[Artifact, ULID]):\n    \"\"\"Repository for Artifact entities with tree traversal operations.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initialize artifact repository with database session.\"\"\"\n        super().__init__(session, Artifact)\n\n    async def find_by_id(self, id: ULID) -&gt; Artifact | None:\n        \"\"\"Find an artifact by ID with children eagerly loaded.\"\"\"\n        return await self.s.get(self.model, id, options=[selectinload(self.model.children)])\n\n    async def find_subtree(self, start_id: ULID) -&gt; Iterable[Artifact]:\n        \"\"\"Find all artifacts in the subtree rooted at the given ID using recursive CTE.\"\"\"\n        cte = select(self.model.id).where(self.model.id == start_id).cte(name=\"descendants\", recursive=True)\n        cte = cte.union_all(select(self.model.id).where(self.model.parent_id == cte.c.id))\n\n        subtree_ids = (await self.s.scalars(select(cte.c.id))).all()\n        rows = (await self.s.scalars(select(self.model).where(self.model.id.in_(subtree_ids)))).all()\n        return rows\n\n    async def get_root_artifact(self, artifact_id: ULID) -&gt; Artifact | None:\n        \"\"\"Find the root artifact by traversing up the parent chain.\"\"\"\n        artifact = await self.s.get(self.model, artifact_id)\n        if artifact is None:\n            return None\n\n        while artifact.parent_id is not None:\n            parent = await self.s.get(self.model, artifact.parent_id)\n            if parent is None:\n                break\n            artifact = parent\n\n        return artifact\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize artifact repository with database session.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initialize artifact repository with database session.\"\"\"\n    super().__init__(session, Artifact)\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.find_by_id","title":"<code>find_by_id(id)</code>  <code>async</code>","text":"<p>Find an artifact by ID with children eagerly loaded.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def find_by_id(self, id: ULID) -&gt; Artifact | None:\n    \"\"\"Find an artifact by ID with children eagerly loaded.\"\"\"\n    return await self.s.get(self.model, id, options=[selectinload(self.model.children)])\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.find_subtree","title":"<code>find_subtree(start_id)</code>  <code>async</code>","text":"<p>Find all artifacts in the subtree rooted at the given ID using recursive CTE.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def find_subtree(self, start_id: ULID) -&gt; Iterable[Artifact]:\n    \"\"\"Find all artifacts in the subtree rooted at the given ID using recursive CTE.\"\"\"\n    cte = select(self.model.id).where(self.model.id == start_id).cte(name=\"descendants\", recursive=True)\n    cte = cte.union_all(select(self.model.id).where(self.model.parent_id == cte.c.id))\n\n    subtree_ids = (await self.s.scalars(select(cte.c.id))).all()\n    rows = (await self.s.scalars(select(self.model).where(self.model.id.in_(subtree_ids)))).all()\n    return rows\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.get_root_artifact","title":"<code>get_root_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Find the root artifact by traversing up the parent chain.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def get_root_artifact(self, artifact_id: ULID) -&gt; Artifact | None:\n    \"\"\"Find the root artifact by traversing up the parent chain.\"\"\"\n    artifact = await self.s.get(self.model, artifact_id)\n    if artifact is None:\n        return None\n\n    while artifact.parent_id is not None:\n        parent = await self.s.get(self.model, artifact.parent_id)\n        if parent is None:\n            break\n        artifact = parent\n\n    return artifact\n</code></pre>"},{"location":"api-reference/#manager","title":"Manager","text":""},{"location":"api-reference/#chapkit.artifact.manager","title":"<code>manager</code>","text":"<p>Artifact manager for hierarchical data with parent-child relationships.</p>"},{"location":"api-reference/#chapkit.artifact.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager","title":"<code>ArtifactManager</code>","text":"<p>               Bases: <code>BaseManager[Artifact, ArtifactIn, ArtifactOut, ULID]</code></p> <p>Manager for Artifact entities with hierarchical tree operations.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>class ArtifactManager(BaseManager[Artifact, ArtifactIn, ArtifactOut, ULID]):\n    \"\"\"Manager for Artifact entities with hierarchical tree operations.\"\"\"\n\n    def __init__(\n        self,\n        repo: ArtifactRepository,\n        hierarchy: ArtifactHierarchy | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize artifact manager with repository and optional hierarchy.\"\"\"\n        super().__init__(repo, Artifact, ArtifactOut)\n        self.repository: ArtifactRepository = repo\n        self.hierarchy = hierarchy\n\n    # Public API ------------------------------------------------------\n\n    async def find_subtree(self, start_id: ULID) -&gt; list[ArtifactTreeNode]:\n        \"\"\"Find all artifacts in the subtree rooted at the given ID.\"\"\"\n        artifacts = await self.repository.find_subtree(start_id)\n        return [self._to_tree_node(artifact) for artifact in artifacts]\n\n    async def expand_artifact(self, artifact_id: ULID) -&gt; ArtifactTreeNode | None:\n        \"\"\"Expand a single artifact with hierarchy metadata but without children.\"\"\"\n        artifact = await self.repository.find_by_id(artifact_id)\n        if artifact is None:\n            return None\n\n        node = self._to_tree_node(artifact)\n        node.children = None\n\n        return node\n\n    async def build_tree(self, start_id: ULID) -&gt; ArtifactTreeNode | None:\n        \"\"\"Build a hierarchical tree structure rooted at the given artifact ID.\"\"\"\n        artifacts = await self.find_subtree(start_id)\n        if not artifacts:\n            return None\n\n        node_map: dict[ULID, ArtifactTreeNode] = {}\n        for node in artifacts:\n            node.children = []\n            node_map[node.id] = node\n\n        for node in artifacts:\n            if node.parent_id is None:\n                continue\n            parent = node_map.get(node.parent_id)\n            if parent is None:\n                continue\n            if parent.children is None:\n                parent.children = []\n            parent.children.append(node)\n\n        # Keep children as [] for leaf nodes (semantic: \"loaded but empty\")\n        # Only expand_artifact sets children=None (semantic: \"not loaded\")\n\n        root = node_map.get(start_id)\n\n        return root\n\n    # Lifecycle overrides --------------------------------------------\n\n    def _should_assign_field(self, field: str, value: object) -&gt; bool:\n        \"\"\"Prevent assigning None to level field during updates.\"\"\"\n        if field == \"level\" and value is None:\n            return False\n        return super()._should_assign_field(field, value)\n\n    async def pre_save(self, entity: Artifact, data: ArtifactIn) -&gt; None:\n        \"\"\"Compute and set artifact level before saving.\"\"\"\n        entity.level = await self._compute_level(entity.parent_id)\n\n    async def pre_update(self, entity: Artifact, data: ArtifactIn, old_values: dict[str, object]) -&gt; None:\n        \"\"\"Recalculate artifact level and cascade updates to descendants if parent changed.\"\"\"\n        previous_level = old_values.get(\"level\", entity.level)\n        entity.level = await self._compute_level(entity.parent_id)\n        parent_changed = old_values.get(\"parent_id\") != entity.parent_id\n        if parent_changed or previous_level != entity.level:\n            await self._recalculate_descendants(entity)\n\n    # Helper utilities ------------------------------------------------\n\n    async def _compute_level(self, parent_id: ULID | None) -&gt; int:\n        \"\"\"Compute the level of an artifact based on its parent.\"\"\"\n        if parent_id is None:\n            return 0\n        parent = await self.repository.find_by_id(parent_id)\n        if parent is None:\n            return 0  # pragma: no cover\n        return parent.level + 1\n\n    async def _recalculate_descendants(self, entity: Artifact) -&gt; None:\n        \"\"\"Recalculate levels for all descendants of an artifact.\"\"\"\n        subtree = await self.repository.find_subtree(entity.id)\n        by_parent: dict[ULID | None, list[Artifact]] = {}\n        for node in subtree:\n            by_parent.setdefault(node.parent_id, []).append(node)\n\n        queue: deque[Artifact] = deque([entity])\n        while queue:\n            current = queue.popleft()\n            for child in by_parent.get(current.id, []):\n                child.level = current.level + 1\n                queue.append(child)\n\n    def _to_tree_node(self, entity: Artifact) -&gt; ArtifactTreeNode:\n        \"\"\"Convert artifact entity to tree node with hierarchy metadata.\"\"\"\n        base = super()._to_output_schema(entity)\n        node = ArtifactTreeNode.from_artifact(base)\n        if self.hierarchy is not None:\n            meta = self.hierarchy.describe(node.level)\n            hierarchy_value = meta.get(self.hierarchy.hierarchy_key)\n            if hierarchy_value is not None:\n                node.hierarchy = str(hierarchy_value)\n            label_value = meta.get(self.hierarchy.label_key)\n            if label_value is not None:\n                node.level_label = str(label_value)\n\n        return node\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.__init__","title":"<code>__init__(repo, hierarchy=None)</code>","text":"<p>Initialize artifact manager with repository and optional hierarchy.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>def __init__(\n    self,\n    repo: ArtifactRepository,\n    hierarchy: ArtifactHierarchy | None = None,\n) -&gt; None:\n    \"\"\"Initialize artifact manager with repository and optional hierarchy.\"\"\"\n    super().__init__(repo, Artifact, ArtifactOut)\n    self.repository: ArtifactRepository = repo\n    self.hierarchy = hierarchy\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.find_subtree","title":"<code>find_subtree(start_id)</code>  <code>async</code>","text":"<p>Find all artifacts in the subtree rooted at the given ID.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def find_subtree(self, start_id: ULID) -&gt; list[ArtifactTreeNode]:\n    \"\"\"Find all artifacts in the subtree rooted at the given ID.\"\"\"\n    artifacts = await self.repository.find_subtree(start_id)\n    return [self._to_tree_node(artifact) for artifact in artifacts]\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.expand_artifact","title":"<code>expand_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Expand a single artifact with hierarchy metadata but without children.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def expand_artifact(self, artifact_id: ULID) -&gt; ArtifactTreeNode | None:\n    \"\"\"Expand a single artifact with hierarchy metadata but without children.\"\"\"\n    artifact = await self.repository.find_by_id(artifact_id)\n    if artifact is None:\n        return None\n\n    node = self._to_tree_node(artifact)\n    node.children = None\n\n    return node\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.build_tree","title":"<code>build_tree(start_id)</code>  <code>async</code>","text":"<p>Build a hierarchical tree structure rooted at the given artifact ID.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def build_tree(self, start_id: ULID) -&gt; ArtifactTreeNode | None:\n    \"\"\"Build a hierarchical tree structure rooted at the given artifact ID.\"\"\"\n    artifacts = await self.find_subtree(start_id)\n    if not artifacts:\n        return None\n\n    node_map: dict[ULID, ArtifactTreeNode] = {}\n    for node in artifacts:\n        node.children = []\n        node_map[node.id] = node\n\n    for node in artifacts:\n        if node.parent_id is None:\n            continue\n        parent = node_map.get(node.parent_id)\n        if parent is None:\n            continue\n        if parent.children is None:\n            parent.children = []\n        parent.children.append(node)\n\n    # Keep children as [] for leaf nodes (semantic: \"loaded but empty\")\n    # Only expand_artifact sets children=None (semantic: \"not loaded\")\n\n    root = node_map.get(start_id)\n\n    return root\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.pre_save","title":"<code>pre_save(entity, data)</code>  <code>async</code>","text":"<p>Compute and set artifact level before saving.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def pre_save(self, entity: Artifact, data: ArtifactIn) -&gt; None:\n    \"\"\"Compute and set artifact level before saving.\"\"\"\n    entity.level = await self._compute_level(entity.parent_id)\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.pre_update","title":"<code>pre_update(entity, data, old_values)</code>  <code>async</code>","text":"<p>Recalculate artifact level and cascade updates to descendants if parent changed.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def pre_update(self, entity: Artifact, data: ArtifactIn, old_values: dict[str, object]) -&gt; None:\n    \"\"\"Recalculate artifact level and cascade updates to descendants if parent changed.\"\"\"\n    previous_level = old_values.get(\"level\", entity.level)\n    entity.level = await self._compute_level(entity.parent_id)\n    parent_changed = old_values.get(\"parent_id\") != entity.parent_id\n    if parent_changed or previous_level != entity.level:\n        await self._recalculate_descendants(entity)\n</code></pre>"},{"location":"api-reference/#router","title":"Router","text":""},{"location":"api-reference/#chapkit.artifact.router","title":"<code>router</code>","text":"<p>Artifact CRUD router with hierarchical tree operations.</p>"},{"location":"api-reference/#chapkit.artifact.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter","title":"<code>ArtifactRouter</code>","text":"<p>               Bases: <code>CrudRouter[ArtifactIn, ArtifactOut]</code></p> <p>CRUD router for Artifact entities with tree operations.</p> Source code in <code>src/chapkit/artifact/router.py</code> <pre><code>class ArtifactRouter(CrudRouter[ArtifactIn, ArtifactOut]):\n    \"\"\"CRUD router for Artifact entities with tree operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        manager_factory: Any,\n        entity_in_type: type[ArtifactIn],\n        entity_out_type: type[ArtifactOut],\n        permissions: CrudPermissions | None = None,\n        enable_config_access: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize artifact router with entity types and manager factory.\"\"\"\n        # Store enable_config_access to conditionally register config endpoint\n        self.enable_config_access = enable_config_access\n\n        super().__init__(\n            prefix=prefix,\n            tags=list(tags),\n            entity_in_type=entity_in_type,\n            entity_out_type=entity_out_type,\n            manager_factory=manager_factory,\n            permissions=permissions,\n            **kwargs,\n        )\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register artifact CRUD routes and tree operations.\"\"\"\n        super()._register_routes()\n\n        manager_factory = self.manager_factory\n\n        async def expand_artifact(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; ArtifactTreeNode:\n            ulid_id = self._parse_ulid(entity_id)\n\n            expanded = await manager.expand_artifact(ulid_id)\n            if expanded is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n            return expanded\n\n        async def build_tree(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; ArtifactTreeNode:\n            ulid_id = self._parse_ulid(entity_id)\n\n            tree = await manager.build_tree(ulid_id)\n            if tree is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n            return tree\n\n        self.register_entity_operation(\n            \"expand\",\n            expand_artifact,\n            response_model=ArtifactTreeNode,\n            summary=\"Expand artifact\",\n            description=\"Get artifact with hierarchy metadata but without children\",\n        )\n\n        self.register_entity_operation(\n            \"tree\",\n            build_tree,\n            response_model=ArtifactTreeNode,\n            summary=\"Build artifact tree\",\n            description=\"Build hierarchical tree structure rooted at the given artifact\",\n        )\n\n        # Conditionally register config access endpoint\n        if self.enable_config_access:\n            from ..api.dependencies import get_config_manager\n            from ..config.manager import ConfigManager\n\n            async def get_config(\n                entity_id: str,\n                artifact_manager: ArtifactManager = Depends(manager_factory),\n                config_manager: ConfigManager[BaseConfig] = Depends(get_config_manager),\n            ) -&gt; ConfigOut[BaseConfig]:\n                \"\"\"Get the config linked to this artifact.\"\"\"\n                ulid_id = self._parse_ulid(entity_id)\n\n                # Get config by traversing to root artifact\n                config = await config_manager.get_config_for_artifact(\n                    artifact_id=ulid_id, artifact_repo=artifact_manager.repository\n                )\n\n                if config is None:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=f\"No config linked to artifact {entity_id}\",\n                    )\n\n                return config\n\n            self.register_entity_operation(\n                \"config\",\n                get_config,\n                response_model=ConfigOut[BaseConfig],\n                summary=\"Get artifact config\",\n                description=\"Get configuration linked to this artifact by traversing to root\",\n            )\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, entity_in_type, entity_out_type, permissions=None, enable_config_access=False, **kwargs)</code>","text":"<p>Initialize artifact router with entity types and manager factory.</p> Source code in <code>src/chapkit/artifact/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    manager_factory: Any,\n    entity_in_type: type[ArtifactIn],\n    entity_out_type: type[ArtifactOut],\n    permissions: CrudPermissions | None = None,\n    enable_config_access: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize artifact router with entity types and manager factory.\"\"\"\n    # Store enable_config_access to conditionally register config endpoint\n    self.enable_config_access = enable_config_access\n\n    super().__init__(\n        prefix=prefix,\n        tags=list(tags),\n        entity_in_type=entity_in_type,\n        entity_out_type=entity_out_type,\n        manager_factory=manager_factory,\n        permissions=permissions,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-reference/#task-module","title":"Task Module","text":"<p>Reusable command templates for shell and Python task execution.</p>"},{"location":"api-reference/#models_1","title":"Models","text":""},{"location":"api-reference/#chapkit.task.models","title":"<code>models</code>","text":"<p>Task ORM model for reusable command templates.</p>"},{"location":"api-reference/#chapkit.task.models-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.models.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>Entity</code></p> <p>ORM model for reusable task templates containing commands to execute.</p> Source code in <code>src/chapkit/task/models.py</code> <pre><code>class Task(Entity):\n    \"\"\"ORM model for reusable task templates containing commands to execute.\"\"\"\n\n    __tablename__ = \"tasks\"\n\n    command: Mapped[str] = mapped_column(Text, nullable=False)\n    task_type: Mapped[str] = mapped_column(Text, nullable=False, default=\"shell\", server_default=\"shell\")\n    parameters: Mapped[dict | None] = mapped_column(JSON, nullable=True)\n    enabled: Mapped[bool] = mapped_column(Boolean, nullable=False, default=True, server_default=\"1\")\n</code></pre>"},{"location":"api-reference/#schemas_1","title":"Schemas","text":""},{"location":"api-reference/#chapkit.task.schemas","title":"<code>schemas</code>","text":"<p>Task schemas for reusable command templates.</p>"},{"location":"api-reference/#chapkit.task.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.schemas.TaskIn","title":"<code>TaskIn</code>","text":"<p>               Bases: <code>EntityIn</code></p> <p>Input schema for creating or updating task templates.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class TaskIn(EntityIn):\n    \"\"\"Input schema for creating or updating task templates.\"\"\"\n\n    command: str = Field(description=\"Shell command or Python function name to execute\")\n    task_type: Literal[\"shell\", \"python\"] = Field(default=\"shell\", description=\"Type of task: 'shell' or 'python'\")\n    parameters: dict[str, Any] | None = Field(\n        default=None, description=\"Parameters to pass to Python function (ignored for shell tasks)\"\n    )\n    enabled: bool = Field(default=True, description=\"Whether task is enabled for execution\")\n</code></pre>"},{"location":"api-reference/#chapkit.task.schemas.TaskOut","title":"<code>TaskOut</code>","text":"<p>               Bases: <code>EntityOut</code></p> <p>Output schema for task template entities.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class TaskOut(EntityOut):\n    \"\"\"Output schema for task template entities.\"\"\"\n\n    command: str = Field(description=\"Shell command or Python function name to execute\")\n    task_type: str = Field(description=\"Type of task: 'shell' or 'python'\")\n    parameters: dict[str, Any] | None = Field(default=None, description=\"Parameters to pass to Python function\")\n    enabled: bool = Field(description=\"Whether task is enabled for execution\")\n</code></pre>"},{"location":"api-reference/#repository_1","title":"Repository","text":""},{"location":"api-reference/#chapkit.task.repository","title":"<code>repository</code>","text":"<p>Task repository for database access and querying.</p>"},{"location":"api-reference/#chapkit.task.repository-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.repository.TaskRepository","title":"<code>TaskRepository</code>","text":"<p>               Bases: <code>BaseRepository[Task, ULID]</code></p> <p>Repository for Task template entities.</p> Source code in <code>src/chapkit/task/repository.py</code> <pre><code>class TaskRepository(BaseRepository[Task, ULID]):\n    \"\"\"Repository for Task template entities.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initialize task repository with database session.\"\"\"\n        super().__init__(session, Task)\n\n    async def find_by_enabled(self, enabled: bool) -&gt; list[Task]:\n        \"\"\"Find all tasks by enabled status.\"\"\"\n        stmt = select(Task).where(Task.enabled == enabled).order_by(Task.created_at.desc())\n        result = await self.s.execute(stmt)\n        return list(result.scalars().all())\n\n    async def find_all(self, *, enabled: bool | None = None) -&gt; list[Task]:\n        \"\"\"Find all tasks, optionally filtered by enabled status.\"\"\"\n        if enabled is None:\n            result = await super().find_all()\n            return list(result)\n        return await self.find_by_enabled(enabled)\n</code></pre>"},{"location":"api-reference/#chapkit.task.repository.TaskRepository-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.repository.TaskRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize task repository with database session.</p> Source code in <code>src/chapkit/task/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initialize task repository with database session.\"\"\"\n    super().__init__(session, Task)\n</code></pre>"},{"location":"api-reference/#chapkit.task.repository.TaskRepository.find_by_enabled","title":"<code>find_by_enabled(enabled)</code>  <code>async</code>","text":"<p>Find all tasks by enabled status.</p> Source code in <code>src/chapkit/task/repository.py</code> <pre><code>async def find_by_enabled(self, enabled: bool) -&gt; list[Task]:\n    \"\"\"Find all tasks by enabled status.\"\"\"\n    stmt = select(Task).where(Task.enabled == enabled).order_by(Task.created_at.desc())\n    result = await self.s.execute(stmt)\n    return list(result.scalars().all())\n</code></pre>"},{"location":"api-reference/#chapkit.task.repository.TaskRepository.find_all","title":"<code>find_all(*, enabled=None)</code>  <code>async</code>","text":"<p>Find all tasks, optionally filtered by enabled status.</p> Source code in <code>src/chapkit/task/repository.py</code> <pre><code>async def find_all(self, *, enabled: bool | None = None) -&gt; list[Task]:\n    \"\"\"Find all tasks, optionally filtered by enabled status.\"\"\"\n    if enabled is None:\n        result = await super().find_all()\n        return list(result)\n    return await self.find_by_enabled(enabled)\n</code></pre>"},{"location":"api-reference/#manager_1","title":"Manager","text":""},{"location":"api-reference/#chapkit.task.manager","title":"<code>manager</code>","text":"<p>Task manager for reusable command templates with artifact-based execution results.</p>"},{"location":"api-reference/#chapkit.task.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.manager.TaskManager","title":"<code>TaskManager</code>","text":"<p>               Bases: <code>BaseManager[Task, TaskIn, TaskOut, ULID]</code></p> <p>Manager for Task template entities with artifact-based execution.</p> Source code in <code>src/chapkit/task/manager.py</code> <pre><code>class TaskManager(BaseManager[Task, TaskIn, TaskOut, ULID]):\n    \"\"\"Manager for Task template entities with artifact-based execution.\"\"\"\n\n    def __init__(\n        self,\n        repo: TaskRepository,\n        scheduler: ChapkitJobScheduler | None = None,\n        database: Database | None = None,\n        artifact_manager: ArtifactManager | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize task manager with repository, scheduler, database, and artifact manager.\"\"\"\n        super().__init__(repo, Task, TaskOut)\n        self.repository: TaskRepository = repo\n        self.scheduler = scheduler\n        self.database = database\n        self.artifact_manager = artifact_manager\n\n    async def find_all(self, *, enabled: bool | None = None) -&gt; list[TaskOut]:\n        \"\"\"Find all tasks, optionally filtered by enabled status.\"\"\"\n        tasks = await self.repository.find_all(enabled=enabled)\n        return [self._to_output_schema(task) for task in tasks]\n\n    def _is_injectable_type(self, param_type: type | None) -&gt; bool:\n        \"\"\"Check if a parameter type should be injected by the framework.\"\"\"\n        if param_type is None:\n            return False\n\n        # Handle Optional[Type] -&gt; extract the non-None type\n        origin = get_origin(param_type)\n        if origin is types.UnionType or origin is Union:  # Union type (both syntaxes)\n            # For Optional types, we still want to inject if the non-None type is injectable\n            # This allows Optional[AsyncSession] to work\n            args = getattr(param_type, \"__args__\", ())\n            non_none_types = [arg for arg in args if arg is not type(None)]\n            if len(non_none_types) == 1:\n                param_type = non_none_types[0]\n\n        # Check if type is in injectable set\n        return param_type in INJECTABLE_TYPES\n\n    def _build_injection_map(self, task_id: ULID, session: AsyncSession | None) -&gt; dict[type, Any]:\n        \"\"\"Build map of injectable types to their instances.\"\"\"\n        return {\n            AsyncSession: session,\n            Database: self.database,\n            ArtifactManager: self.artifact_manager,\n            ChapkitJobScheduler: self.scheduler,\n        }\n\n    def _inject_parameters(\n        self, func: Any, user_params: dict[str, Any], task_id: ULID, session: AsyncSession | None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Merge user parameters with framework injections based on function signature.\"\"\"\n        sig = inspect.signature(func)\n        type_hints = get_type_hints(func)\n\n        # Build injection map\n        injection_map = self._build_injection_map(task_id, session)\n\n        # Start with user parameters\n        final_params = dict(user_params)\n\n        # Inspect each parameter in function signature\n        for param_name, param in sig.parameters.items():\n            # Skip self, *args, **kwargs\n            if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n                continue\n\n            # Get type hint for this parameter\n            param_type = type_hints.get(param_name)\n\n            # Check if this type should be injected\n            if self._is_injectable_type(param_type):\n                # Get the actual type (handle Optional)\n                actual_type = param_type\n                origin = get_origin(param_type)\n                if origin is types.UnionType or origin is Union:\n                    args = getattr(param_type, \"__args__\", ())\n                    non_none_types = [arg for arg in args if arg is not type(None)]\n                    if non_none_types:\n                        actual_type = non_none_types[0]\n\n                # Inject if we have an instance of this type\n                if actual_type in injection_map:\n                    injectable_value = injection_map[actual_type]\n                    # For required parameters, inject even if None\n                    # For optional parameters, only inject if not None\n                    if param.default is param.empty:\n                        # Required parameter - inject whatever we have (even None)\n                        final_params[param_name] = injectable_value\n                    elif injectable_value is not None:\n                        # Optional parameter - only inject if we have a value\n                        final_params[param_name] = injectable_value\n                continue\n\n            # Not injectable - must come from user parameters\n            if param_name not in final_params:\n                # Check if parameter has a default value\n                if param.default is not param.empty:\n                    continue  # Will use default\n\n                # Required parameter missing\n                raise ValueError(\n                    f\"Missing required parameter '{param_name}' for task function. \"\n                    f\"Parameter is not injectable and not provided in task.parameters.\"\n                )\n\n        return final_params\n\n    async def execute_task(self, task_id: ULID) -&gt; ULID:\n        \"\"\"Execute a task by submitting it to the scheduler and return the job ID.\"\"\"\n        if self.scheduler is None:\n            raise ValueError(\"Task execution requires a scheduler. Use ServiceBuilder.with_jobs() to enable.\")\n\n        if self.artifact_manager is None:\n            raise ValueError(\n                \"Task execution requires artifacts. Use ServiceBuilder.with_artifacts() before with_tasks().\"\n            )\n\n        task = await self.repository.find_by_id(task_id)\n        if task is None:\n            raise ValueError(f\"Task {task_id} not found\")\n\n        # Check if task is enabled\n        if not task.enabled:\n            raise ValueError(f\"Cannot execute disabled task {task_id}\")\n\n        # Route based on task type\n        if task.task_type == \"python\":\n            job_id = await self.scheduler.add_job(self._execute_python, task_id)\n        else:  # shell\n            job_id = await self.scheduler.add_job(self._execute_command, task_id)\n\n        return job_id\n\n    async def _execute_command(self, task_id: ULID) -&gt; ULID:\n        \"\"\"Execute command and return artifact_id containing results.\"\"\"\n        if self.database is None:\n            raise RuntimeError(\"Database instance required for task execution\")\n\n        if self.artifact_manager is None:\n            raise RuntimeError(\"ArtifactManager instance required for task execution\")\n\n        # Fetch task and serialize snapshot before execution\n        async with self.database.session() as session:\n            task_repo = TaskRepository(session)\n            task = await task_repo.find_by_id(task_id)\n            if task is None:\n                raise ValueError(f\"Task {task_id} not found\")\n\n            # Capture task snapshot\n            task_snapshot = {\n                \"id\": str(task.id),\n                \"command\": task.command,\n                \"created_at\": task.created_at.isoformat(),\n                \"updated_at\": task.updated_at.isoformat(),\n            }\n\n        # Execute command using asyncio subprocess\n        process = await asyncio.create_subprocess_shell(\n            task.command,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n\n        # Wait for completion and capture output\n        stdout_bytes, stderr_bytes = await process.communicate()\n\n        # Decode outputs\n        stdout_text = stdout_bytes.decode(\"utf-8\") if stdout_bytes else \"\"\n        stderr_text = stderr_bytes.decode(\"utf-8\") if stderr_bytes else \"\"\n\n        # Create artifact with execution results\n        result_data: dict[str, Any] = {\n            \"task\": task_snapshot,\n            \"stdout\": stdout_text,\n            \"stderr\": stderr_text,\n            \"exit_code\": process.returncode,\n        }\n\n        async with self.database.session() as session:\n            artifact_repo = ArtifactRepository(session)\n            artifact_mgr = ArtifactManager(artifact_repo)\n\n            artifact_out = await artifact_mgr.save(\n                ArtifactIn(\n                    data=result_data,\n                    parent_id=None,\n                )\n            )\n\n        return artifact_out.id\n\n    async def _execute_python(self, task_id: ULID) -&gt; ULID:\n        \"\"\"Execute Python function and return artifact_id containing results.\"\"\"\n        if self.database is None:\n            raise RuntimeError(\"Database instance required for task execution\")\n\n        if self.artifact_manager is None:\n            raise RuntimeError(\"ArtifactManager instance required for task execution\")\n\n        # Create a database session for potential injection\n        session_context = self.database.session()\n        session = await session_context.__aenter__()\n\n        try:\n            # Fetch task and serialize snapshot\n            task_repo = TaskRepository(session)\n            task = await task_repo.find_by_id(task_id)\n            if task is None:\n                raise ValueError(f\"Task {task_id} not found\")\n\n            # Capture task snapshot\n            task_snapshot = {\n                \"id\": str(task.id),\n                \"command\": task.command,\n                \"task_type\": task.task_type,\n                \"parameters\": task.parameters,\n                \"created_at\": task.created_at.isoformat(),\n                \"updated_at\": task.updated_at.isoformat(),\n            }\n\n            # Get function from registry\n            try:\n                func = TaskRegistry.get(task.command)\n            except KeyError:\n                raise ValueError(f\"Python function '{task.command}' not found in registry\")\n\n            # Execute function with type-based injection\n            result_data: dict[str, Any]\n            try:\n                user_params = task.parameters or {}\n\n                # Inject framework dependencies based on function signature\n                final_params = self._inject_parameters(func, user_params, task_id, session)\n\n                # Handle sync/async functions\n                if inspect.iscoroutinefunction(func):\n                    result = await func(**final_params)\n                else:\n                    result = await asyncio.to_thread(func, **final_params)\n\n                result_data = {\n                    \"task\": task_snapshot,\n                    \"result\": result,\n                    \"error\": None,\n                }\n            except Exception as e:\n                result_data = {\n                    \"task\": task_snapshot,\n                    \"result\": None,\n                    \"error\": {\n                        \"type\": type(e).__name__,\n                        \"message\": str(e),\n                        \"traceback\": traceback.format_exc(),\n                    },\n                }\n        finally:\n            # Always close the session\n            await session_context.__aexit__(None, None, None)\n\n        # Create artifact (with a new session)\n        async with self.database.session() as artifact_session:\n            artifact_repo = ArtifactRepository(artifact_session)\n            artifact_mgr = ArtifactManager(artifact_repo)\n            artifact_out = await artifact_mgr.save(ArtifactIn(data=result_data, parent_id=None))\n\n        return artifact_out.id\n</code></pre>"},{"location":"api-reference/#chapkit.task.manager.TaskManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.manager.TaskManager.__init__","title":"<code>__init__(repo, scheduler=None, database=None, artifact_manager=None)</code>","text":"<p>Initialize task manager with repository, scheduler, database, and artifact manager.</p> Source code in <code>src/chapkit/task/manager.py</code> <pre><code>def __init__(\n    self,\n    repo: TaskRepository,\n    scheduler: ChapkitJobScheduler | None = None,\n    database: Database | None = None,\n    artifact_manager: ArtifactManager | None = None,\n) -&gt; None:\n    \"\"\"Initialize task manager with repository, scheduler, database, and artifact manager.\"\"\"\n    super().__init__(repo, Task, TaskOut)\n    self.repository: TaskRepository = repo\n    self.scheduler = scheduler\n    self.database = database\n    self.artifact_manager = artifact_manager\n</code></pre>"},{"location":"api-reference/#chapkit.task.manager.TaskManager.find_all","title":"<code>find_all(*, enabled=None)</code>  <code>async</code>","text":"<p>Find all tasks, optionally filtered by enabled status.</p> Source code in <code>src/chapkit/task/manager.py</code> <pre><code>async def find_all(self, *, enabled: bool | None = None) -&gt; list[TaskOut]:\n    \"\"\"Find all tasks, optionally filtered by enabled status.\"\"\"\n    tasks = await self.repository.find_all(enabled=enabled)\n    return [self._to_output_schema(task) for task in tasks]\n</code></pre>"},{"location":"api-reference/#chapkit.task.manager.TaskManager.execute_task","title":"<code>execute_task(task_id)</code>  <code>async</code>","text":"<p>Execute a task by submitting it to the scheduler and return the job ID.</p> Source code in <code>src/chapkit/task/manager.py</code> <pre><code>async def execute_task(self, task_id: ULID) -&gt; ULID:\n    \"\"\"Execute a task by submitting it to the scheduler and return the job ID.\"\"\"\n    if self.scheduler is None:\n        raise ValueError(\"Task execution requires a scheduler. Use ServiceBuilder.with_jobs() to enable.\")\n\n    if self.artifact_manager is None:\n        raise ValueError(\n            \"Task execution requires artifacts. Use ServiceBuilder.with_artifacts() before with_tasks().\"\n        )\n\n    task = await self.repository.find_by_id(task_id)\n    if task is None:\n        raise ValueError(f\"Task {task_id} not found\")\n\n    # Check if task is enabled\n    if not task.enabled:\n        raise ValueError(f\"Cannot execute disabled task {task_id}\")\n\n    # Route based on task type\n    if task.task_type == \"python\":\n        job_id = await self.scheduler.add_job(self._execute_python, task_id)\n    else:  # shell\n        job_id = await self.scheduler.add_job(self._execute_command, task_id)\n\n    return job_id\n</code></pre>"},{"location":"api-reference/#router_1","title":"Router","text":""},{"location":"api-reference/#chapkit.task.router","title":"<code>router</code>","text":"<p>Task CRUD router with execution operation.</p>"},{"location":"api-reference/#chapkit.task.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.router.TaskExecuteResponse","title":"<code>TaskExecuteResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for task execution.</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>class TaskExecuteResponse(BaseModel):\n    \"\"\"Response schema for task execution.\"\"\"\n\n    job_id: str = Field(description=\"ID of the scheduler job\")\n    message: str = Field(description=\"Human-readable message\")\n</code></pre>"},{"location":"api-reference/#chapkit.task.router.TaskRouter","title":"<code>TaskRouter</code>","text":"<p>               Bases: <code>CrudRouter[TaskIn, TaskOut]</code></p> <p>CRUD router for Task entities with execution operation.</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>class TaskRouter(CrudRouter[TaskIn, TaskOut]):\n    \"\"\"CRUD router for Task entities with execution operation.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        manager_factory: Any,\n        entity_in_type: type[TaskIn],\n        entity_out_type: type[TaskOut],\n        permissions: CrudPermissions | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize task router with entity types and manager factory.\"\"\"\n        super().__init__(\n            prefix=prefix,\n            tags=list(tags),\n            entity_in_type=entity_in_type,\n            entity_out_type=entity_out_type,\n            manager_factory=manager_factory,\n            permissions=permissions,\n            **kwargs,\n        )\n\n    def _register_find_all_route(self, manager_dependency: Any, manager_annotation: Any) -&gt; None:\n        \"\"\"Register find all route with enabled filtering support.\"\"\"\n        entity_out_annotation: Any = self.entity_out_type\n        collection_response_model: Any = list[entity_out_annotation] | PaginatedResponse[entity_out_annotation]\n\n        @self.router.get(\"\", response_model=collection_response_model)\n        async def find_all(\n            page: int | None = None,\n            size: int | None = None,\n            enabled: bool | None = Query(None, description=\"Filter by enabled status\"),\n            manager: Manager[TaskIn, TaskOut, ULID] = manager_dependency,\n        ) -&gt; list[TaskOut] | PaginatedResponse[TaskOut]:\n            from servicekit.api.pagination import create_paginated_response\n\n            # Pagination is opt-in: both page and size must be provided\n            if page is not None and size is not None:\n                items, total = await manager.find_paginated(page, size)\n                return create_paginated_response(items, total, page, size)\n\n            # Use TaskRepository's find_all with enabled filtering\n            # Cast manager to access repository with enabled parameter\n            task_manager = manager  # TaskManager with TaskRepository\n            return await task_manager.find_all(enabled=enabled)  # type: ignore[call-arg]  # pyright: ignore[reportCallIssue]\n\n        self._annotate_manager(find_all, manager_annotation)\n        find_all.__annotations__[\"return\"] = list[entity_out_annotation] | PaginatedResponse[entity_out_annotation]\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register task CRUD routes and execution operation.\"\"\"\n        super()._register_routes()\n\n        manager_factory = self.manager_factory\n\n        async def execute_task(\n            entity_id: str,\n            manager: TaskManager = Depends(manager_factory),\n        ) -&gt; TaskExecuteResponse:\n            \"\"\"Execute a task asynchronously via the job scheduler.\"\"\"\n            task_id = self._parse_ulid(entity_id)\n\n            try:\n                job_id = await manager.execute_task(task_id)\n                return TaskExecuteResponse(\n                    job_id=str(job_id),\n                    message=f\"Task submitted for execution. Job ID: {job_id}\",\n                )\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n            except RuntimeError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=str(e),\n                )\n\n        self.register_entity_operation(\n            \"execute\",\n            execute_task,\n            http_method=\"POST\",\n            response_model=TaskExecuteResponse,\n            status_code=status.HTTP_202_ACCEPTED,\n            summary=\"Execute task\",\n            description=\"Submit the task to the scheduler for execution\",\n        )\n</code></pre>"},{"location":"api-reference/#chapkit.task.router.TaskRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.router.TaskRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, entity_in_type, entity_out_type, permissions=None, **kwargs)</code>","text":"<p>Initialize task router with entity types and manager factory.</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    manager_factory: Any,\n    entity_in_type: type[TaskIn],\n    entity_out_type: type[TaskOut],\n    permissions: CrudPermissions | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize task router with entity types and manager factory.\"\"\"\n    super().__init__(\n        prefix=prefix,\n        tags=list(tags),\n        entity_in_type=entity_in_type,\n        entity_out_type=entity_out_type,\n        manager_factory=manager_factory,\n        permissions=permissions,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-reference/#registry","title":"Registry","text":""},{"location":"api-reference/#chapkit.task.registry","title":"<code>registry</code>","text":"<p>Global registry for Python task functions.</p>"},{"location":"api-reference/#chapkit.task.registry-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.registry.TaskRegistry","title":"<code>TaskRegistry</code>","text":"<p>Global registry for Python task functions.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>class TaskRegistry:\n    \"\"\"Global registry for Python task functions.\"\"\"\n\n    _registry: dict[str, Callable[..., Any]] = {}\n\n    @classmethod\n    def register(cls, name: str) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n        \"\"\"Decorator to register a task function with support for type-based dependency injection.\"\"\"\n\n        def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n            if name in cls._registry:\n                raise ValueError(f\"Task '{name}' already registered\")\n            cls._registry[name] = func\n            return func\n\n        return decorator\n\n    @classmethod\n    def register_function(cls, name: str, func: Callable[..., Any]) -&gt; None:\n        \"\"\"Imperatively register a task function.\"\"\"\n        if name in cls._registry:\n            raise ValueError(f\"Task '{name}' already registered\")\n        cls._registry[name] = func\n\n    @classmethod\n    def get(cls, name: str) -&gt; Callable[..., Any]:\n        \"\"\"Retrieve a registered task function.\"\"\"\n        if name not in cls._registry:\n            raise KeyError(f\"Task '{name}' not found in registry\")\n        return cls._registry[name]\n\n    @classmethod\n    def list_all(cls) -&gt; list[str]:\n        \"\"\"List all registered task names.\"\"\"\n        return sorted(cls._registry.keys())\n\n    @classmethod\n    def clear(cls) -&gt; None:\n        \"\"\"Clear all registered tasks (useful for testing).\"\"\"\n        cls._registry.clear()\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.register","title":"<code>register(name)</code>  <code>classmethod</code>","text":"<p>Decorator to register a task function with support for type-based dependency injection.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef register(cls, name: str) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Decorator to register a task function with support for type-based dependency injection.\"\"\"\n\n    def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n        if name in cls._registry:\n            raise ValueError(f\"Task '{name}' already registered\")\n        cls._registry[name] = func\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.register_function","title":"<code>register_function(name, func)</code>  <code>classmethod</code>","text":"<p>Imperatively register a task function.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef register_function(cls, name: str, func: Callable[..., Any]) -&gt; None:\n    \"\"\"Imperatively register a task function.\"\"\"\n    if name in cls._registry:\n        raise ValueError(f\"Task '{name}' already registered\")\n    cls._registry[name] = func\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Retrieve a registered task function.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; Callable[..., Any]:\n    \"\"\"Retrieve a registered task function.\"\"\"\n    if name not in cls._registry:\n        raise KeyError(f\"Task '{name}' not found in registry\")\n    return cls._registry[name]\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.list_all","title":"<code>list_all()</code>  <code>classmethod</code>","text":"<p>List all registered task names.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef list_all(cls) -&gt; list[str]:\n    \"\"\"List all registered task names.\"\"\"\n    return sorted(cls._registry.keys())\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clear all registered tasks (useful for testing).</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef clear(cls) -&gt; None:\n    \"\"\"Clear all registered tasks (useful for testing).\"\"\"\n    cls._registry.clear()\n</code></pre>"},{"location":"api-reference/#validation","title":"Validation","text":""},{"location":"api-reference/#chapkit.task.validation","title":"<code>validation</code>","text":"<p>Task validation utilities for detecting orphaned Python tasks.</p>"},{"location":"api-reference/#chapkit.task.validation-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.validation-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.validation.validate_and_disable_orphaned_tasks","title":"<code>validate_and_disable_orphaned_tasks(app)</code>  <code>async</code>","text":"<p>Validate Python tasks and disable orphaned ones that reference missing functions.</p> Source code in <code>src/chapkit/task/validation.py</code> <pre><code>async def validate_and_disable_orphaned_tasks(app: FastAPI) -&gt; int:\n    \"\"\"Validate Python tasks and disable orphaned ones that reference missing functions.\"\"\"\n    database: Database | None = getattr(app.state, \"database\", None)\n    if database is None:\n        logger.debug(\"No database configured, skipping task validation\")\n        return 0\n\n    disabled_count = 0\n\n    async with database.session() as session:\n        task_repo = TaskRepository(session)\n        task_manager = TaskManager(task_repo, scheduler=None, database=None, artifact_manager=None)\n\n        # Get all tasks\n        all_tasks = await task_manager.find_all()\n\n        # Get registered function names\n        registered_functions = set(TaskRegistry.list_all())\n\n        # Find orphaned Python tasks\n        orphaned_tasks = [\n            task for task in all_tasks if task.task_type == \"python\" and task.command not in registered_functions\n        ]\n\n        if orphaned_tasks:\n            logger.warning(\n                \"Found orphaned Python tasks - disabling them\",\n                extra={\n                    \"count\": len(orphaned_tasks),\n                    \"task_ids\": [str(task.id) for task in orphaned_tasks],\n                    \"commands\": [task.command for task in orphaned_tasks],\n                },\n            )\n\n            # Disable each orphaned task\n            for task in orphaned_tasks:\n                logger.info(\n                    f\"Disabling orphaned task {task.id}: function '{task.command}' not found in registry\",\n                    extra={\"task_id\": str(task.id), \"command\": task.command, \"task_type\": task.task_type},\n                )\n\n                # Create TaskIn with enabled=False\n                task_type_value = task.task_type if task.task_type in (\"shell\", \"python\") else \"shell\"\n                task_in = TaskIn(\n                    id=task.id,\n                    command=task.command,\n                    task_type=task_type_value,  # type: ignore[arg-type]\n                    parameters=task.parameters,\n                    enabled=False,\n                )\n                await task_manager.save(task_in)\n                disabled_count += 1\n\n    if disabled_count &gt; 0:\n        logger.warning(f\"Disabled {disabled_count} orphaned Python task(s)\")\n    else:\n        logger.debug(\"No orphaned Python tasks found\")\n\n    return disabled_count\n</code></pre>"},{"location":"api-reference/#config-module","title":"Config Module","text":"<p>Key-value configuration storage with Pydantic schema validation.</p>"},{"location":"api-reference/#models_2","title":"Models","text":""},{"location":"api-reference/#chapkit.config.models","title":"<code>models</code>","text":"<p>Config ORM models for key-value configuration storage and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.models-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.models.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>Entity</code></p> <p>ORM model for configuration with JSON data storage.</p> Source code in <code>src/chapkit/config/models.py</code> <pre><code>class Config(Entity):\n    \"\"\"ORM model for configuration with JSON data storage.\"\"\"\n\n    __tablename__ = \"configs\"\n\n    name: Mapped[str] = mapped_column(index=True)\n    _data_json: Mapped[dict[str, Any]] = mapped_column(\"data\", JSON, nullable=False)\n\n    @property\n    def data(self) -&gt; dict[str, Any]:\n        \"\"\"Return JSON data as dict.\"\"\"\n        return self._data_json\n\n    @data.setter\n    def data(self, value: BaseConfig | dict[str, Any]) -&gt; None:\n        \"\"\"Serialize Pydantic model to JSON or store dict directly.\"\"\"\n        if isinstance(value, dict):\n            self._data_json = value\n        elif hasattr(value, \"model_dump\") and callable(value.model_dump):\n            # BaseConfig or other Pydantic model\n            self._data_json = value.model_dump(mode=\"json\")\n        else:\n            raise TypeError(f\"data must be a BaseConfig subclass or dict, got {type(value)}\")\n</code></pre>"},{"location":"api-reference/#chapkit.config.models.Config-attributes","title":"Attributes","text":""},{"location":"api-reference/#chapkit.config.models.Config.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>Return JSON data as dict.</p>"},{"location":"api-reference/#chapkit.config.models.ConfigArtifact","title":"<code>ConfigArtifact</code>","text":"<p>               Bases: <code>Base</code></p> <p>Junction table linking Configs to root Artifacts.</p> Source code in <code>src/chapkit/config/models.py</code> <pre><code>class ConfigArtifact(Base):\n    \"\"\"Junction table linking Configs to root Artifacts.\"\"\"\n\n    __tablename__ = \"config_artifacts\"\n\n    config_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"configs.id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n    )\n\n    artifact_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"artifacts.id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n        unique=True,\n    )\n\n    __table_args__ = (UniqueConstraint(\"artifact_id\", name=\"uq_artifact_id\"),)\n</code></pre>"},{"location":"api-reference/#schemas_2","title":"Schemas","text":""},{"location":"api-reference/#chapkit.config.schemas","title":"<code>schemas</code>","text":"<p>Config schemas for key-value configuration with JSON data.</p>"},{"location":"api-reference/#chapkit.config.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.schemas.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for configuration schemas with arbitrary extra fields allowed.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class BaseConfig(BaseModel):\n    \"\"\"Base class for configuration schemas with arbitrary extra fields allowed.\"\"\"\n\n    model_config = {\"extra\": \"allow\"}\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigIn","title":"<code>ConfigIn</code>","text":"<p>               Bases: <code>EntityIn</code></p> <p>Input schema for creating or updating configurations.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class ConfigIn[DataT: BaseConfig](EntityIn):\n    \"\"\"Input schema for creating or updating configurations.\"\"\"\n\n    name: str\n    data: DataT\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut","title":"<code>ConfigOut</code>","text":"<p>               Bases: <code>EntityOut</code></p> <p>Output schema for configuration entities.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class ConfigOut[DataT: BaseConfig](EntityOut):\n    \"\"\"Output schema for configuration entities.\"\"\"\n\n    name: str\n    data: DataT\n\n    model_config = {\"ser_json_timedelta\": \"float\", \"ser_json_bytes\": \"base64\"}\n\n    @field_validator(\"data\", mode=\"before\")\n    @classmethod\n    def convert_dict_to_model(cls, v: Any, info: ValidationInfo) -&gt; Any:\n        \"\"\"Convert dict to BaseConfig model if data_cls is provided in validation context.\"\"\"\n        if isinstance(v, BaseConfig):\n            return v\n        if isinstance(v, dict):\n            if info.context and \"data_cls\" in info.context:\n                data_cls = info.context[\"data_cls\"]\n                return data_cls.model_validate(v)\n        return v\n\n    @field_serializer(\"data\", when_used=\"json\")\n    def serialize_data(self, value: DataT) -&gt; dict[str, Any]:\n        \"\"\"Serialize BaseConfig data to JSON dict.\"\"\"\n        if isinstance(value, BaseConfig):  # pyright: ignore[reportUnnecessaryIsInstance]\n            return value.model_dump(mode=\"json\")\n        return value\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.schemas.ConfigOut.convert_dict_to_model","title":"<code>convert_dict_to_model(v, info)</code>  <code>classmethod</code>","text":"<p>Convert dict to BaseConfig model if data_cls is provided in validation context.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>@field_validator(\"data\", mode=\"before\")\n@classmethod\ndef convert_dict_to_model(cls, v: Any, info: ValidationInfo) -&gt; Any:\n    \"\"\"Convert dict to BaseConfig model if data_cls is provided in validation context.\"\"\"\n    if isinstance(v, BaseConfig):\n        return v\n    if isinstance(v, dict):\n        if info.context and \"data_cls\" in info.context:\n            data_cls = info.context[\"data_cls\"]\n            return data_cls.model_validate(v)\n    return v\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut.serialize_data","title":"<code>serialize_data(value)</code>","text":"<p>Serialize BaseConfig data to JSON dict.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>@field_serializer(\"data\", when_used=\"json\")\ndef serialize_data(self, value: DataT) -&gt; dict[str, Any]:\n    \"\"\"Serialize BaseConfig data to JSON dict.\"\"\"\n    if isinstance(value, BaseConfig):  # pyright: ignore[reportUnnecessaryIsInstance]\n        return value.model_dump(mode=\"json\")\n    return value\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.LinkArtifactRequest","title":"<code>LinkArtifactRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for linking an artifact to a config.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class LinkArtifactRequest(BaseModel):\n    \"\"\"Request schema for linking an artifact to a config.\"\"\"\n\n    artifact_id: ULID\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.UnlinkArtifactRequest","title":"<code>UnlinkArtifactRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for unlinking an artifact from a config.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class UnlinkArtifactRequest(BaseModel):\n    \"\"\"Request schema for unlinking an artifact from a config.\"\"\"\n\n    artifact_id: ULID\n</code></pre>"},{"location":"api-reference/#repository_2","title":"Repository","text":""},{"location":"api-reference/#chapkit.config.repository","title":"<code>repository</code>","text":"<p>Config repository for database access and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.repository-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.repository.ConfigRepository","title":"<code>ConfigRepository</code>","text":"<p>               Bases: <code>BaseRepository[Config, ULID]</code></p> <p>Repository for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>class ConfigRepository(BaseRepository[Config, ULID]):\n    \"\"\"Repository for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initialize config repository with database session.\"\"\"\n        super().__init__(session, Config)\n\n    async def find_by_name(self, name: str) -&gt; Config | None:\n        \"\"\"Find a config by its unique name.\"\"\"\n        result = await self.s.scalars(select(self.model).where(self.model.name == name))\n        return result.one_or_none()\n\n    async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n        \"\"\"Link a config to a root artifact.\"\"\"\n        artifact = await self.s.get(Artifact, artifact_id)\n        if artifact is None:\n            raise ValueError(f\"Artifact {artifact_id} not found\")\n        if artifact.parent_id is not None:\n            raise ValueError(f\"Artifact {artifact_id} is not a root artifact (parent_id={artifact.parent_id})\")\n\n        link = ConfigArtifact(config_id=config_id, artifact_id=artifact_id)\n        self.s.add(link)\n\n    async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n        \"\"\"Unlink an artifact from its config.\"\"\"\n        stmt = sql_delete(ConfigArtifact).where(ConfigArtifact.artifact_id == artifact_id)\n        await self.s.execute(stmt)\n\n    async def delete_by_id(self, id: ULID) -&gt; None:\n        \"\"\"Delete a config and cascade delete all linked artifact trees.\"\"\"\n        from chapkit.artifact.repository import ArtifactRepository\n\n        linked_artifacts = await self.find_artifacts_for_config(id)\n\n        artifact_repo = ArtifactRepository(self.s)\n        for root_artifact in linked_artifacts:\n            subtree = await artifact_repo.find_subtree(root_artifact.id)\n            for artifact in subtree:\n                await self.s.delete(artifact)\n\n        await super().delete_by_id(id)\n\n    async def find_by_root_artifact_id(self, artifact_id: ULID) -&gt; Config | None:\n        \"\"\"Find the config linked to a root artifact.\"\"\"\n        stmt = (\n            select(Config)\n            .join(ConfigArtifact, Config.id == ConfigArtifact.config_id)\n            .where(ConfigArtifact.artifact_id == artifact_id)\n        )\n        result = await self.s.scalars(stmt)\n        return result.one_or_none()\n\n    async def find_artifacts_for_config(self, config_id: ULID) -&gt; list[Artifact]:\n        \"\"\"Find all root artifacts linked to a config.\"\"\"\n        stmt = (\n            select(Artifact)\n            .join(ConfigArtifact, Artifact.id == ConfigArtifact.artifact_id)\n            .where(ConfigArtifact.config_id == config_id)\n        )\n        result = await self.s.scalars(stmt)\n        return list(result.all())\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize config repository with database session.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initialize config repository with database session.\"\"\"\n    super().__init__(session, Config)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_by_name","title":"<code>find_by_name(name)</code>  <code>async</code>","text":"<p>Find a config by its unique name.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_by_name(self, name: str) -&gt; Config | None:\n    \"\"\"Find a config by its unique name.\"\"\"\n    result = await self.s.scalars(select(self.model).where(self.model.name == name))\n    return result.one_or_none()\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.link_artifact","title":"<code>link_artifact(config_id, artifact_id)</code>  <code>async</code>","text":"<p>Link a config to a root artifact.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n    \"\"\"Link a config to a root artifact.\"\"\"\n    artifact = await self.s.get(Artifact, artifact_id)\n    if artifact is None:\n        raise ValueError(f\"Artifact {artifact_id} not found\")\n    if artifact.parent_id is not None:\n        raise ValueError(f\"Artifact {artifact_id} is not a root artifact (parent_id={artifact.parent_id})\")\n\n    link = ConfigArtifact(config_id=config_id, artifact_id=artifact_id)\n    self.s.add(link)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.unlink_artifact","title":"<code>unlink_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Unlink an artifact from its config.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n    \"\"\"Unlink an artifact from its config.\"\"\"\n    stmt = sql_delete(ConfigArtifact).where(ConfigArtifact.artifact_id == artifact_id)\n    await self.s.execute(stmt)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>async</code>","text":"<p>Delete a config and cascade delete all linked artifact trees.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def delete_by_id(self, id: ULID) -&gt; None:\n    \"\"\"Delete a config and cascade delete all linked artifact trees.\"\"\"\n    from chapkit.artifact.repository import ArtifactRepository\n\n    linked_artifacts = await self.find_artifacts_for_config(id)\n\n    artifact_repo = ArtifactRepository(self.s)\n    for root_artifact in linked_artifacts:\n        subtree = await artifact_repo.find_subtree(root_artifact.id)\n        for artifact in subtree:\n            await self.s.delete(artifact)\n\n    await super().delete_by_id(id)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_by_root_artifact_id","title":"<code>find_by_root_artifact_id(artifact_id)</code>  <code>async</code>","text":"<p>Find the config linked to a root artifact.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_by_root_artifact_id(self, artifact_id: ULID) -&gt; Config | None:\n    \"\"\"Find the config linked to a root artifact.\"\"\"\n    stmt = (\n        select(Config)\n        .join(ConfigArtifact, Config.id == ConfigArtifact.config_id)\n        .where(ConfigArtifact.artifact_id == artifact_id)\n    )\n    result = await self.s.scalars(stmt)\n    return result.one_or_none()\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_artifacts_for_config","title":"<code>find_artifacts_for_config(config_id)</code>  <code>async</code>","text":"<p>Find all root artifacts linked to a config.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_artifacts_for_config(self, config_id: ULID) -&gt; list[Artifact]:\n    \"\"\"Find all root artifacts linked to a config.\"\"\"\n    stmt = (\n        select(Artifact)\n        .join(ConfigArtifact, Artifact.id == ConfigArtifact.artifact_id)\n        .where(ConfigArtifact.config_id == config_id)\n    )\n    result = await self.s.scalars(stmt)\n    return list(result.all())\n</code></pre>"},{"location":"api-reference/#manager_2","title":"Manager","text":""},{"location":"api-reference/#chapkit.config.manager","title":"<code>manager</code>","text":"<p>Config manager for CRUD operations and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.manager.ConfigManager","title":"<code>ConfigManager</code>","text":"<p>               Bases: <code>BaseManager[Config, ConfigIn[DataT], ConfigOut[DataT], ULID]</code></p> <p>Manager for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>class ConfigManager[DataT: BaseConfig](BaseManager[Config, ConfigIn[DataT], ConfigOut[DataT], ULID]):\n    \"\"\"Manager for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(self, repo: ConfigRepository, data_cls: type[DataT]) -&gt; None:\n        \"\"\"Initialize config manager with repository and data class.\"\"\"\n        super().__init__(repo, Config, ConfigOut)\n        self.repository: ConfigRepository = repo\n        self.data_cls = data_cls\n\n    async def find_by_name(self, name: str) -&gt; ConfigOut[DataT] | None:\n        \"\"\"Find a config by its unique name.\"\"\"\n        config = await self.repository.find_by_name(name)\n        if config:\n            return self._to_output_schema(config)\n        return None\n\n    async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n        \"\"\"Link a config to a root artifact.\"\"\"\n        await self.repository.link_artifact(config_id, artifact_id)\n        await self.repository.commit()\n\n    async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n        \"\"\"Unlink an artifact from its config.\"\"\"\n        await self.repository.unlink_artifact(artifact_id)\n        await self.repository.commit()\n\n    async def get_config_for_artifact(\n        self, artifact_id: ULID, artifact_repo: ArtifactRepository\n    ) -&gt; ConfigOut[DataT] | None:\n        \"\"\"Get the config for an artifact by traversing to its root.\"\"\"\n        root = await artifact_repo.get_root_artifact(artifact_id)\n        if root is None:\n            return None\n\n        config = await self.repository.find_by_root_artifact_id(root.id)\n        if config is None:\n            return None\n\n        return self._to_output_schema(config)\n\n    async def get_linked_artifacts(self, config_id: ULID) -&gt; list[ArtifactOut]:\n        \"\"\"Get all root artifacts linked to a config.\"\"\"\n        artifacts = await self.repository.find_artifacts_for_config(config_id)\n        return [ArtifactOut.model_validate(artifact, from_attributes=True) for artifact in artifacts]\n\n    def _to_output_schema(self, entity: Config) -&gt; ConfigOut[DataT]:\n        \"\"\"Convert ORM entity to output schema with proper data class validation.\"\"\"\n        return ConfigOut[DataT].model_validate(entity, from_attributes=True, context={\"data_cls\": self.data_cls})\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.manager.ConfigManager.__init__","title":"<code>__init__(repo, data_cls)</code>","text":"<p>Initialize config manager with repository and data class.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>def __init__(self, repo: ConfigRepository, data_cls: type[DataT]) -&gt; None:\n    \"\"\"Initialize config manager with repository and data class.\"\"\"\n    super().__init__(repo, Config, ConfigOut)\n    self.repository: ConfigRepository = repo\n    self.data_cls = data_cls\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.find_by_name","title":"<code>find_by_name(name)</code>  <code>async</code>","text":"<p>Find a config by its unique name.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def find_by_name(self, name: str) -&gt; ConfigOut[DataT] | None:\n    \"\"\"Find a config by its unique name.\"\"\"\n    config = await self.repository.find_by_name(name)\n    if config:\n        return self._to_output_schema(config)\n    return None\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.link_artifact","title":"<code>link_artifact(config_id, artifact_id)</code>  <code>async</code>","text":"<p>Link a config to a root artifact.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n    \"\"\"Link a config to a root artifact.\"\"\"\n    await self.repository.link_artifact(config_id, artifact_id)\n    await self.repository.commit()\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.unlink_artifact","title":"<code>unlink_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Unlink an artifact from its config.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n    \"\"\"Unlink an artifact from its config.\"\"\"\n    await self.repository.unlink_artifact(artifact_id)\n    await self.repository.commit()\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.get_config_for_artifact","title":"<code>get_config_for_artifact(artifact_id, artifact_repo)</code>  <code>async</code>","text":"<p>Get the config for an artifact by traversing to its root.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def get_config_for_artifact(\n    self, artifact_id: ULID, artifact_repo: ArtifactRepository\n) -&gt; ConfigOut[DataT] | None:\n    \"\"\"Get the config for an artifact by traversing to its root.\"\"\"\n    root = await artifact_repo.get_root_artifact(artifact_id)\n    if root is None:\n        return None\n\n    config = await self.repository.find_by_root_artifact_id(root.id)\n    if config is None:\n        return None\n\n    return self._to_output_schema(config)\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.get_linked_artifacts","title":"<code>get_linked_artifacts(config_id)</code>  <code>async</code>","text":"<p>Get all root artifacts linked to a config.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def get_linked_artifacts(self, config_id: ULID) -&gt; list[ArtifactOut]:\n    \"\"\"Get all root artifacts linked to a config.\"\"\"\n    artifacts = await self.repository.find_artifacts_for_config(config_id)\n    return [ArtifactOut.model_validate(artifact, from_attributes=True) for artifact in artifacts]\n</code></pre>"},{"location":"api-reference/#router_2","title":"Router","text":""},{"location":"api-reference/#chapkit.config.router","title":"<code>router</code>","text":"<p>Config CRUD router with artifact linking operations.</p>"},{"location":"api-reference/#chapkit.config.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.router.ConfigRouter","title":"<code>ConfigRouter</code>","text":"<p>               Bases: <code>CrudRouter[ConfigIn[BaseConfig], ConfigOut[BaseConfig]]</code></p> <p>CRUD router for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/router.py</code> <pre><code>class ConfigRouter(CrudRouter[ConfigIn[BaseConfig], ConfigOut[BaseConfig]]):\n    \"\"\"CRUD router for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        manager_factory: Any,\n        entity_in_type: type[ConfigIn[BaseConfig]],\n        entity_out_type: type[ConfigOut[BaseConfig]],\n        permissions: CrudPermissions | None = None,\n        enable_artifact_operations: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize config router with entity types and manager factory.\"\"\"\n        self.enable_artifact_operations = enable_artifact_operations\n        super().__init__(\n            prefix=prefix,\n            tags=list(tags),\n            entity_in_type=entity_in_type,\n            entity_out_type=entity_out_type,\n            manager_factory=manager_factory,\n            permissions=permissions,\n            **kwargs,\n        )\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register config CRUD routes and artifact linking operations.\"\"\"\n        super()._register_routes()\n\n        if not self.enable_artifact_operations:\n            return\n\n        manager_factory = self.manager_factory\n\n        async def link_artifact(\n            entity_id: str,\n            request: LinkArtifactRequest,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; None:\n            config_id = self._parse_ulid(entity_id)\n\n            try:\n                await manager.link_artifact(config_id, request.artifact_id)\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n\n        async def unlink_artifact(\n            entity_id: str,\n            request: UnlinkArtifactRequest,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; None:\n            try:\n                await manager.unlink_artifact(request.artifact_id)\n            except Exception as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n\n        async def get_linked_artifacts(\n            entity_id: str,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; list[ArtifactOut]:\n            config_id = self._parse_ulid(entity_id)\n            return await manager.get_linked_artifacts(config_id)\n\n        self.register_entity_operation(\n            \"link-artifact\",\n            link_artifact,\n            http_method=\"POST\",\n            status_code=status.HTTP_204_NO_CONTENT,\n            summary=\"Link artifact to config\",\n            description=\"Link a config to a root artifact (parent_id IS NULL)\",\n        )\n\n        self.register_entity_operation(\n            \"unlink-artifact\",\n            unlink_artifact,\n            http_method=\"POST\",\n            status_code=status.HTTP_204_NO_CONTENT,\n            summary=\"Unlink artifact from config\",\n            description=\"Remove the link between a config and an artifact\",\n        )\n\n        self.register_entity_operation(\n            \"artifacts\",\n            get_linked_artifacts,\n            http_method=\"GET\",\n            response_model=list[ArtifactOut],\n            summary=\"Get linked artifacts\",\n            description=\"Get all root artifacts linked to this config\",\n        )\n</code></pre>"},{"location":"api-reference/#chapkit.config.router.ConfigRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.router.ConfigRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, entity_in_type, entity_out_type, permissions=None, enable_artifact_operations=False, **kwargs)</code>","text":"<p>Initialize config router with entity types and manager factory.</p> Source code in <code>src/chapkit/config/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    manager_factory: Any,\n    entity_in_type: type[ConfigIn[BaseConfig]],\n    entity_out_type: type[ConfigOut[BaseConfig]],\n    permissions: CrudPermissions | None = None,\n    enable_artifact_operations: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize config router with entity types and manager factory.\"\"\"\n    self.enable_artifact_operations = enable_artifact_operations\n    super().__init__(\n        prefix=prefix,\n        tags=list(tags),\n        entity_in_type=entity_in_type,\n        entity_out_type=entity_out_type,\n        manager_factory=manager_factory,\n        permissions=permissions,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-reference/#ml-module","title":"ML Module","text":"<p>Train/predict workflows with artifact-based model storage and timing metadata.</p>"},{"location":"api-reference/#schemas_3","title":"Schemas","text":""},{"location":"api-reference/#chapkit.ml.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for ML train/predict operations.</p>"},{"location":"api-reference/#chapkit.ml.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.schemas.TrainRequest","title":"<code>TrainRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for training a model.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class TrainRequest(BaseModel):\n    \"\"\"Request schema for training a model.\"\"\"\n\n    config_id: ULID = Field(description=\"ID of the config to use for training\")\n    data: DataFrame = Field(description=\"Training data as DataFrame\")\n    geo: FeatureCollection | None = Field(default=None, description=\"Optional geospatial data\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.TrainResponse","title":"<code>TrainResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for train operation submission.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class TrainResponse(BaseModel):\n    \"\"\"Response schema for train operation submission.\"\"\"\n\n    job_id: str = Field(description=\"ID of the training job in the scheduler\")\n    model_artifact_id: str = Field(description=\"ID that will contain the trained model artifact\")\n    message: str = Field(description=\"Human-readable message\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.PredictRequest","title":"<code>PredictRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for making predictions.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class PredictRequest(BaseModel):\n    \"\"\"Request schema for making predictions.\"\"\"\n\n    model_artifact_id: ULID = Field(description=\"ID of the artifact containing the trained model\")\n    historic: DataFrame = Field(description=\"Historic data as DataFrame\")\n    future: DataFrame = Field(description=\"Future/prediction data as DataFrame\")\n    geo: FeatureCollection | None = Field(default=None, description=\"Optional geospatial data\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.PredictResponse","title":"<code>PredictResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for predict operation submission.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class PredictResponse(BaseModel):\n    \"\"\"Response schema for predict operation submission.\"\"\"\n\n    job_id: str = Field(description=\"ID of the prediction job in the scheduler\")\n    prediction_artifact_id: str = Field(description=\"ID that will contain the prediction artifact\")\n    message: str = Field(description=\"Human-readable message\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.TrainedModelArtifactData","title":"<code>TrainedModelArtifactData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for trained model artifact data stored in the artifact system.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class TrainedModelArtifactData(BaseModel):\n    \"\"\"Schema for trained model artifact data stored in the artifact system.\"\"\"\n\n    ml_type: Literal[\"trained_model\"] = Field(description=\"Artifact type identifier\")\n    config_id: str = Field(description=\"ID of the config used for training\")\n    started_at: str = Field(description=\"ISO format timestamp when operation started\")\n    completed_at: str = Field(description=\"ISO format timestamp when operation completed\")\n    duration_seconds: float = Field(description=\"Operation duration in seconds (rounded to 2 decimals)\")\n    model: Any = Field(description=\"The trained model object (must be pickleable)\")\n    model_type: str | None = Field(default=None, description=\"Fully qualified class name of the model\")\n    model_size_bytes: int | None = Field(default=None, description=\"Serialized pickle size of the model in bytes\")\n\n    model_config = {\"arbitrary_types_allowed\": True}\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.PredictionArtifactData","title":"<code>PredictionArtifactData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for prediction artifact data stored in the artifact system.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class PredictionArtifactData(BaseModel):\n    \"\"\"Schema for prediction artifact data stored in the artifact system.\"\"\"\n\n    ml_type: Literal[\"prediction\"] = Field(description=\"Artifact type identifier\")\n    config_id: str = Field(description=\"ID of the config used for prediction\")\n    model_artifact_id: str = Field(description=\"ID of the trained model artifact used for prediction\")\n    started_at: str = Field(description=\"ISO format timestamp when operation started\")\n    completed_at: str = Field(description=\"ISO format timestamp when operation completed\")\n    duration_seconds: float = Field(description=\"Operation duration in seconds (rounded to 2 decimals)\")\n    predictions: DataFrame = Field(description=\"Prediction results as structured DataFrame\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol","title":"<code>ModelRunnerProtocol</code>","text":"<p>               Bases: <code>Protocol[ConfigT]</code></p> <p>Protocol defining the interface for model runners.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class ModelRunnerProtocol(Protocol[ConfigT]):\n    \"\"\"Protocol defining the interface for model runners.\"\"\"\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n        ...\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model and return the trained model object (must be pickleable).</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions using a trained model and return predictions as DataFrame.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#manager_3","title":"Manager","text":""},{"location":"api-reference/#chapkit.ml.manager","title":"<code>manager</code>","text":"<p>Manager for ML train/predict operations with artifact-based storage.</p>"},{"location":"api-reference/#chapkit.ml.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.manager.MLManager","title":"<code>MLManager</code>","text":"<p>               Bases: <code>Generic[ConfigT]</code></p> <p>Manager for ML train/predict operations with job scheduling and artifact storage.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>class MLManager(Generic[ConfigT]):\n    \"\"\"Manager for ML train/predict operations with job scheduling and artifact storage.\"\"\"\n\n    def __init__(\n        self,\n        runner: ModelRunnerProtocol[ConfigT],\n        scheduler: ChapkitJobScheduler,\n        database: Database,\n        config_schema: type[ConfigT],\n    ) -&gt; None:\n        \"\"\"Initialize ML manager with runner, scheduler, database, and config schema.\"\"\"\n        self.runner = runner\n        self.scheduler = scheduler\n        self.database = database\n        self.config_schema = config_schema\n\n    async def execute_train(self, request: TrainRequest) -&gt; TrainResponse:\n        \"\"\"Submit a training job to the scheduler and return job/artifact IDs.\"\"\"\n        # Pre-allocate artifact ID for the trained model\n        model_artifact_id = ULID()\n\n        # Submit job to scheduler\n        job_id = await self.scheduler.add_job(\n            self._train_task,\n            request,\n            model_artifact_id,\n        )\n\n        return TrainResponse(\n            job_id=str(job_id),\n            model_artifact_id=str(model_artifact_id),\n            message=f\"Training job submitted. Job ID: {job_id}\",\n        )\n\n    async def execute_predict(self, request: PredictRequest) -&gt; PredictResponse:\n        \"\"\"Submit a prediction job to the scheduler and return job/artifact IDs.\"\"\"\n        # Pre-allocate artifact ID for predictions\n        prediction_artifact_id = ULID()\n\n        # Submit job to scheduler\n        job_id = await self.scheduler.add_job(\n            self._predict_task,\n            request,\n            prediction_artifact_id,\n        )\n\n        return PredictResponse(\n            job_id=str(job_id),\n            prediction_artifact_id=str(prediction_artifact_id),\n            message=f\"Prediction job submitted. Job ID: {job_id}\",\n        )\n\n    async def _train_task(self, request: TrainRequest, model_artifact_id: ULID) -&gt; ULID:\n        \"\"\"Execute training task and store trained model in artifact.\"\"\"\n        # Load config\n        async with self.database.session() as session:\n            config_repo = ConfigRepository(session)\n            config_manager: ConfigManager[ConfigT] = ConfigManager(config_repo, self.config_schema)\n            config = await config_manager.find_by_id(request.config_id)\n\n            if config is None:\n                raise ValueError(f\"Config {request.config_id} not found\")\n\n        # Train model with timing\n        training_started_at = datetime.datetime.now(datetime.UTC)\n        trained_model = await self.runner.on_train(\n            config=config.data,\n            data=request.data,\n            geo=request.geo,\n        )\n        training_completed_at = datetime.datetime.now(datetime.UTC)\n        training_duration = (training_completed_at - training_started_at).total_seconds()\n\n        # Calculate model metrics\n        model_type = _extract_model_type(trained_model)\n        model_size_bytes = _calculate_model_size(trained_model)\n\n        # Store trained model in artifact with metadata\n        async with self.database.session() as session:\n            artifact_repo = ArtifactRepository(session)\n            artifact_manager = ArtifactManager(artifact_repo)\n            config_repo = ConfigRepository(session)\n\n            # Create and validate artifact data with Pydantic\n            artifact_data_model = TrainedModelArtifactData(\n                ml_type=\"trained_model\",\n                config_id=str(request.config_id),\n                model=trained_model,\n                started_at=training_started_at.isoformat(),\n                completed_at=training_completed_at.isoformat(),\n                duration_seconds=round(training_duration, 2),\n                model_type=model_type,\n                model_size_bytes=model_size_bytes,\n            )\n\n            await artifact_manager.save(\n                ArtifactIn(\n                    id=model_artifact_id,\n                    data=artifact_data_model.model_dump(),\n                    parent_id=None,\n                    level=0,\n                )\n            )\n\n            # Link config to root artifact for tree traversal\n            await config_repo.link_artifact(request.config_id, model_artifact_id)\n            await config_repo.commit()\n\n        return model_artifact_id\n\n    async def _predict_task(self, request: PredictRequest, prediction_artifact_id: ULID) -&gt; ULID:\n        \"\"\"Execute prediction task and store predictions in artifact.\"\"\"\n        # Load model artifact\n        async with self.database.session() as session:\n            artifact_repo = ArtifactRepository(session)\n            artifact_manager = ArtifactManager(artifact_repo)\n            model_artifact = await artifact_manager.find_by_id(request.model_artifact_id)\n\n            if model_artifact is None:\n                raise ValueError(f\"Model artifact {request.model_artifact_id} not found\")\n\n        # Extract model and config_id from artifact\n        model_data = model_artifact.data\n        if not isinstance(model_data, dict) or model_data.get(\"ml_type\") != \"trained_model\":\n            raise ValueError(f\"Artifact {request.model_artifact_id} is not a trained model\")\n\n        trained_model = model_data[\"model\"]\n        config_id = ULID.from_str(model_data[\"config_id\"])\n\n        # Load config\n        async with self.database.session() as session:\n            config_repo = ConfigRepository(session)\n            config_manager: ConfigManager[ConfigT] = ConfigManager(config_repo, self.config_schema)\n            config = await config_manager.find_by_id(config_id)\n\n            if config is None:\n                raise ValueError(f\"Config {config_id} not found\")\n\n        # Make predictions with timing\n        prediction_started_at = datetime.datetime.now(datetime.UTC)\n        predictions = await self.runner.on_predict(\n            config=config.data,\n            model=trained_model,\n            historic=request.historic,\n            future=request.future,\n            geo=request.geo,\n        )\n        prediction_completed_at = datetime.datetime.now(datetime.UTC)\n        prediction_duration = (prediction_completed_at - prediction_started_at).total_seconds()\n\n        # Store predictions in artifact with parent linkage\n        async with self.database.session() as session:\n            artifact_repo = ArtifactRepository(session)\n            artifact_manager = ArtifactManager(artifact_repo)\n\n            # Create and validate artifact data with Pydantic\n            artifact_data_model = PredictionArtifactData(\n                ml_type=\"prediction\",\n                model_artifact_id=str(request.model_artifact_id),\n                config_id=str(config_id),\n                predictions=predictions,\n                started_at=prediction_started_at.isoformat(),\n                completed_at=prediction_completed_at.isoformat(),\n                duration_seconds=round(prediction_duration, 2),\n            )\n\n            await artifact_manager.save(\n                ArtifactIn(\n                    id=prediction_artifact_id,\n                    data=artifact_data_model.model_dump(),\n                    parent_id=request.model_artifact_id,\n                    level=1,\n                )\n            )\n\n        return prediction_artifact_id\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.manager.MLManager.__init__","title":"<code>__init__(runner, scheduler, database, config_schema)</code>","text":"<p>Initialize ML manager with runner, scheduler, database, and config schema.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>def __init__(\n    self,\n    runner: ModelRunnerProtocol[ConfigT],\n    scheduler: ChapkitJobScheduler,\n    database: Database,\n    config_schema: type[ConfigT],\n) -&gt; None:\n    \"\"\"Initialize ML manager with runner, scheduler, database, and config schema.\"\"\"\n    self.runner = runner\n    self.scheduler = scheduler\n    self.database = database\n    self.config_schema = config_schema\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager.execute_train","title":"<code>execute_train(request)</code>  <code>async</code>","text":"<p>Submit a training job to the scheduler and return job/artifact IDs.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>async def execute_train(self, request: TrainRequest) -&gt; TrainResponse:\n    \"\"\"Submit a training job to the scheduler and return job/artifact IDs.\"\"\"\n    # Pre-allocate artifact ID for the trained model\n    model_artifact_id = ULID()\n\n    # Submit job to scheduler\n    job_id = await self.scheduler.add_job(\n        self._train_task,\n        request,\n        model_artifact_id,\n    )\n\n    return TrainResponse(\n        job_id=str(job_id),\n        model_artifact_id=str(model_artifact_id),\n        message=f\"Training job submitted. Job ID: {job_id}\",\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager.execute_predict","title":"<code>execute_predict(request)</code>  <code>async</code>","text":"<p>Submit a prediction job to the scheduler and return job/artifact IDs.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>async def execute_predict(self, request: PredictRequest) -&gt; PredictResponse:\n    \"\"\"Submit a prediction job to the scheduler and return job/artifact IDs.\"\"\"\n    # Pre-allocate artifact ID for predictions\n    prediction_artifact_id = ULID()\n\n    # Submit job to scheduler\n    job_id = await self.scheduler.add_job(\n        self._predict_task,\n        request,\n        prediction_artifact_id,\n    )\n\n    return PredictResponse(\n        job_id=str(job_id),\n        prediction_artifact_id=str(prediction_artifact_id),\n        message=f\"Prediction job submitted. Job ID: {job_id}\",\n    )\n</code></pre>"},{"location":"api-reference/#router_3","title":"Router","text":""},{"location":"api-reference/#chapkit.ml.router","title":"<code>router</code>","text":"<p>REST API router for ML train/predict operations.</p>"},{"location":"api-reference/#chapkit.ml.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.router.MLRouter","title":"<code>MLRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>Router with $train and $predict collection operations.</p> Source code in <code>src/chapkit/ml/router.py</code> <pre><code>class MLRouter(Router):\n    \"\"\"Router with $train and $predict collection operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        manager_factory: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize ML router with manager factory.\"\"\"\n        self.manager_factory = manager_factory\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register ML train and predict routes.\"\"\"\n        from fastapi import HTTPException\n\n        manager_factory = self.manager_factory\n\n        @self.router.post(\n            \"/$train\",\n            response_model=TrainResponse,\n            status_code=status.HTTP_202_ACCEPTED,\n            summary=\"Train model\",\n            description=\"Submit a training job to the scheduler\",\n        )\n        async def train(\n            request: TrainRequest,\n            manager: MLManager = Depends(manager_factory),\n        ) -&gt; TrainResponse:\n            \"\"\"Train a model asynchronously and return job/artifact IDs.\"\"\"\n            try:\n                response = await manager.execute_train(request)\n                train_counter, _ = _get_counters()\n                train_counter.add(1)\n                return response\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n            except RuntimeError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=str(e),\n                )\n\n        @self.router.post(\n            \"/$predict\",\n            response_model=PredictResponse,\n            status_code=status.HTTP_202_ACCEPTED,\n            summary=\"Make predictions\",\n            description=\"Submit a prediction job to the scheduler\",\n        )\n        async def predict(\n            request: PredictRequest,\n            manager: MLManager = Depends(manager_factory),\n        ) -&gt; PredictResponse:\n            \"\"\"Make predictions asynchronously and return job/artifact IDs.\"\"\"\n            try:\n                response = await manager.execute_predict(request)\n                _, predict_counter = _get_counters()\n                predict_counter.add(1)\n                return response\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n            except RuntimeError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=str(e),\n                )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.router.MLRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.router.MLRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, **kwargs)</code>","text":"<p>Initialize ML router with manager factory.</p> Source code in <code>src/chapkit/ml/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    manager_factory: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize ML router with manager factory.\"\"\"\n    self.manager_factory = manager_factory\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#model-runners","title":"Model Runners","text":"<p>Protocol and implementations for ML model training and prediction.</p>"},{"location":"api-reference/#basemodelrunner","title":"BaseModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner","title":"<code>BaseModelRunner</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[ConfigT]</code></p> <p>Abstract base class for model runners with lifecycle hooks.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class BaseModelRunner(ABC, Generic[ConfigT]):\n    \"\"\"Abstract base class for model runners with lifecycle hooks.\"\"\"\n\n    async def on_init(self) -&gt; None:\n        \"\"\"Optional initialization hook called before training or prediction.\"\"\"\n        pass\n\n    async def on_cleanup(self) -&gt; None:\n        \"\"\"Optional cleanup hook called after training or prediction.\"\"\"\n        pass\n\n    @abstractmethod\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n        ...\n\n    @abstractmethod\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_init","title":"<code>on_init()</code>  <code>async</code>","text":"<p>Optional initialization hook called before training or prediction.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_init(self) -&gt; None:\n    \"\"\"Optional initialization hook called before training or prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_cleanup","title":"<code>on_cleanup()</code>  <code>async</code>","text":"<p>Optional cleanup hook called after training or prediction.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_cleanup(self) -&gt; None:\n    \"\"\"Optional cleanup hook called after training or prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Train a model and return the trained model object (must be pickleable).</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>@abstractmethod\nasync def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Make predictions using a trained model and return predictions as DataFrame.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>@abstractmethod\nasync def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#functionalmodelrunner","title":"FunctionalModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner","title":"<code>FunctionalModelRunner</code>","text":"<p>               Bases: <code>BaseModelRunner[ConfigT]</code></p> <p>Functional model runner wrapping train and predict functions.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class FunctionalModelRunner(BaseModelRunner[ConfigT]):\n    \"\"\"Functional model runner wrapping train and predict functions.\"\"\"\n\n    def __init__(\n        self,\n        on_train: TrainFunction[ConfigT],\n        on_predict: PredictFunction[ConfigT],\n    ) -&gt; None:\n        \"\"\"Initialize functional runner with train and predict functions.\"\"\"\n        self._on_train = on_train\n        self._on_predict = on_predict\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return the trained model object.\"\"\"\n        return await self._on_train(config, data, geo)\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Make predictions using a trained model.\"\"\"\n        return await self._on_predict(config, model, historic, future, geo)\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.__init__","title":"<code>__init__(on_train, on_predict)</code>","text":"<p>Initialize functional runner with train and predict functions.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>def __init__(\n    self,\n    on_train: TrainFunction[ConfigT],\n    on_predict: PredictFunction[ConfigT],\n) -&gt; None:\n    \"\"\"Initialize functional runner with train and predict functions.\"\"\"\n    self._on_train = on_train\n    self._on_predict = on_predict\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model and return the trained model object.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return the trained model object.\"\"\"\n    return await self._on_train(config, data, geo)\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions using a trained model.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions using a trained model.\"\"\"\n    return await self._on_predict(config, model, historic, future, geo)\n</code></pre>"},{"location":"api-reference/#shellmodelrunner","title":"ShellModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner","title":"<code>ShellModelRunner</code>","text":"<p>               Bases: <code>BaseModelRunner[ConfigT]</code></p> <p>Shell-based model runner that executes external scripts for train/predict operations.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class ShellModelRunner(BaseModelRunner[ConfigT]):\n    \"\"\"Shell-based model runner that executes external scripts for train/predict operations.\"\"\"\n\n    def __init__(\n        self,\n        train_command: str,\n        predict_command: str,\n        model_format: str = \"pickle\",\n    ) -&gt; None:\n        \"\"\"Initialize shell runner with command templates for train/predict operations.\"\"\"\n        self.train_command = train_command\n        self.predict_command = predict_command\n        self.model_format = model_format\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model by executing external training script.\"\"\"\n        temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_train_\"))\n\n        try:\n            # Write config to YAML file\n            config_file = temp_dir / \"config.yml\"\n            config_file.write_text(yaml.safe_dump(config.model_dump(), indent=2))\n\n            # Write training data to CSV\n            data_file = temp_dir / \"data.csv\"\n            data.to_csv(data_file)\n\n            # Write geo data if provided\n            geo_file = temp_dir / \"geo.json\" if geo else None\n            if geo:\n                assert geo_file is not None  # For type checker\n                geo_file.write_text(geo.model_dump_json(indent=2))\n\n            # Model file path\n            model_file = temp_dir / f\"model.{self.model_format}\"\n\n            # Substitute variables in command\n            command = self.train_command.format(\n                config_file=str(config_file),\n                data_file=str(data_file),\n                model_file=str(model_file),\n                geo_file=str(geo_file) if geo_file else \"\",\n            )\n\n            logger.info(\"executing_train_script\", command=command, temp_dir=str(temp_dir))\n\n            # Execute subprocess\n            process = await asyncio.create_subprocess_shell(\n                command,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd=str(temp_dir),\n            )\n\n            stdout_bytes, stderr_bytes = await process.communicate()\n            stdout = stdout_bytes.decode(\"utf-8\") if stdout_bytes else \"\"\n            stderr = stderr_bytes.decode(\"utf-8\") if stderr_bytes else \"\"\n\n            if process.returncode != 0:\n                logger.error(\"train_script_failed\", exit_code=process.returncode, stderr=stderr)\n                raise RuntimeError(f\"Training script failed with exit code {process.returncode}: {stderr}\")\n\n            logger.info(\"train_script_completed\", stdout=stdout[:500], stderr=stderr[:500])\n\n            # Load trained model from file\n            if not model_file.exists():\n                raise RuntimeError(f\"Training script did not create model file at {model_file}\")\n\n            with open(model_file, \"rb\") as f:\n                model = pickle.load(f)\n\n            return model\n\n        finally:\n            # Cleanup temp files\n            import shutil\n\n            shutil.rmtree(temp_dir, ignore_errors=True)\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Make predictions by executing external prediction script.\"\"\"\n        temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_predict_\"))\n\n        try:\n            # Write config to YAML file\n            config_file = temp_dir / \"config.yml\"\n            config_file.write_text(yaml.safe_dump(config.model_dump(), indent=2))\n\n            # Write model to file\n            model_file = temp_dir / f\"model.{self.model_format}\"\n            with open(model_file, \"wb\") as f:\n                pickle.dump(model, f)\n\n            # Write historic data\n            historic_file = temp_dir / \"historic.csv\"\n            historic.to_csv(historic_file)\n\n            # Write future data to CSV\n            future_file = temp_dir / \"future.csv\"\n            future.to_csv(future_file)\n\n            # Write geo data if provided\n            geo_file = temp_dir / \"geo.json\" if geo else None\n            if geo:\n                assert geo_file is not None  # For type checker\n                geo_file.write_text(geo.model_dump_json(indent=2))\n\n            # Output file path\n            output_file = temp_dir / \"predictions.csv\"\n\n            # Substitute variables in command\n            command = self.predict_command.format(\n                config_file=str(config_file),\n                model_file=str(model_file),\n                historic_file=str(historic_file),\n                future_file=str(future_file),\n                output_file=str(output_file),\n                geo_file=str(geo_file) if geo_file else \"\",\n            )\n\n            logger.info(\"executing_predict_script\", command=command, temp_dir=str(temp_dir))\n\n            # Execute subprocess\n            process = await asyncio.create_subprocess_shell(\n                command,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd=str(temp_dir),\n            )\n\n            stdout_bytes, stderr_bytes = await process.communicate()\n            stdout = stdout_bytes.decode(\"utf-8\") if stdout_bytes else \"\"\n            stderr = stderr_bytes.decode(\"utf-8\") if stderr_bytes else \"\"\n\n            if process.returncode != 0:\n                logger.error(\"predict_script_failed\", exit_code=process.returncode, stderr=stderr)\n                raise RuntimeError(f\"Prediction script failed with exit code {process.returncode}: {stderr}\")\n\n            logger.info(\"predict_script_completed\", stdout=stdout[:500], stderr=stderr[:500])\n\n            # Load predictions from file\n            if not output_file.exists():\n                raise RuntimeError(f\"Prediction script did not create output file at {output_file}\")\n\n            predictions = DataFrame.from_csv(output_file)\n            return predictions\n\n        finally:\n            # Cleanup temp files\n            import shutil\n\n            shutil.rmtree(temp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.__init__","title":"<code>__init__(train_command, predict_command, model_format='pickle')</code>","text":"<p>Initialize shell runner with command templates for train/predict operations.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>def __init__(\n    self,\n    train_command: str,\n    predict_command: str,\n    model_format: str = \"pickle\",\n) -&gt; None:\n    \"\"\"Initialize shell runner with command templates for train/predict operations.\"\"\"\n    self.train_command = train_command\n    self.predict_command = predict_command\n    self.model_format = model_format\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model by executing external training script.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model by executing external training script.\"\"\"\n    temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_train_\"))\n\n    try:\n        # Write config to YAML file\n        config_file = temp_dir / \"config.yml\"\n        config_file.write_text(yaml.safe_dump(config.model_dump(), indent=2))\n\n        # Write training data to CSV\n        data_file = temp_dir / \"data.csv\"\n        data.to_csv(data_file)\n\n        # Write geo data if provided\n        geo_file = temp_dir / \"geo.json\" if geo else None\n        if geo:\n            assert geo_file is not None  # For type checker\n            geo_file.write_text(geo.model_dump_json(indent=2))\n\n        # Model file path\n        model_file = temp_dir / f\"model.{self.model_format}\"\n\n        # Substitute variables in command\n        command = self.train_command.format(\n            config_file=str(config_file),\n            data_file=str(data_file),\n            model_file=str(model_file),\n            geo_file=str(geo_file) if geo_file else \"\",\n        )\n\n        logger.info(\"executing_train_script\", command=command, temp_dir=str(temp_dir))\n\n        # Execute subprocess\n        process = await asyncio.create_subprocess_shell(\n            command,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            cwd=str(temp_dir),\n        )\n\n        stdout_bytes, stderr_bytes = await process.communicate()\n        stdout = stdout_bytes.decode(\"utf-8\") if stdout_bytes else \"\"\n        stderr = stderr_bytes.decode(\"utf-8\") if stderr_bytes else \"\"\n\n        if process.returncode != 0:\n            logger.error(\"train_script_failed\", exit_code=process.returncode, stderr=stderr)\n            raise RuntimeError(f\"Training script failed with exit code {process.returncode}: {stderr}\")\n\n        logger.info(\"train_script_completed\", stdout=stdout[:500], stderr=stderr[:500])\n\n        # Load trained model from file\n        if not model_file.exists():\n            raise RuntimeError(f\"Training script did not create model file at {model_file}\")\n\n        with open(model_file, \"rb\") as f:\n            model = pickle.load(f)\n\n        return model\n\n    finally:\n        # Cleanup temp files\n        import shutil\n\n        shutil.rmtree(temp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions by executing external prediction script.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions by executing external prediction script.\"\"\"\n    temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_predict_\"))\n\n    try:\n        # Write config to YAML file\n        config_file = temp_dir / \"config.yml\"\n        config_file.write_text(yaml.safe_dump(config.model_dump(), indent=2))\n\n        # Write model to file\n        model_file = temp_dir / f\"model.{self.model_format}\"\n        with open(model_file, \"wb\") as f:\n            pickle.dump(model, f)\n\n        # Write historic data\n        historic_file = temp_dir / \"historic.csv\"\n        historic.to_csv(historic_file)\n\n        # Write future data to CSV\n        future_file = temp_dir / \"future.csv\"\n        future.to_csv(future_file)\n\n        # Write geo data if provided\n        geo_file = temp_dir / \"geo.json\" if geo else None\n        if geo:\n            assert geo_file is not None  # For type checker\n            geo_file.write_text(geo.model_dump_json(indent=2))\n\n        # Output file path\n        output_file = temp_dir / \"predictions.csv\"\n\n        # Substitute variables in command\n        command = self.predict_command.format(\n            config_file=str(config_file),\n            model_file=str(model_file),\n            historic_file=str(historic_file),\n            future_file=str(future_file),\n            output_file=str(output_file),\n            geo_file=str(geo_file) if geo_file else \"\",\n        )\n\n        logger.info(\"executing_predict_script\", command=command, temp_dir=str(temp_dir))\n\n        # Execute subprocess\n        process = await asyncio.create_subprocess_shell(\n            command,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            cwd=str(temp_dir),\n        )\n\n        stdout_bytes, stderr_bytes = await process.communicate()\n        stdout = stdout_bytes.decode(\"utf-8\") if stdout_bytes else \"\"\n        stderr = stderr_bytes.decode(\"utf-8\") if stderr_bytes else \"\"\n\n        if process.returncode != 0:\n            logger.error(\"predict_script_failed\", exit_code=process.returncode, stderr=stderr)\n            raise RuntimeError(f\"Prediction script failed with exit code {process.returncode}: {stderr}\")\n\n        logger.info(\"predict_script_completed\", stdout=stdout[:500], stderr=stderr[:500])\n\n        # Load predictions from file\n        if not output_file.exists():\n            raise RuntimeError(f\"Prediction script did not create output file at {output_file}\")\n\n        predictions = DataFrame.from_csv(output_file)\n        return predictions\n\n    finally:\n        # Cleanup temp files\n        import shutil\n\n        shutil.rmtree(temp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api-reference/#scheduler","title":"Scheduler","text":"<p>Chapkit job scheduler with artifact tracking for ML/task workflows.</p>"},{"location":"api-reference/#chapkit.scheduler","title":"<code>scheduler</code>","text":"<p>Chapkit-specific job scheduler with artifact tracking.</p>"},{"location":"api-reference/#chapkit.scheduler-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.scheduler.ChapkitJobRecord","title":"<code>ChapkitJobRecord</code>","text":"<p>               Bases: <code>JobRecord</code></p> <p>Job record extended with artifact_id tracking for ML/task workflows.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>class ChapkitJobRecord(JobRecord):\n    \"\"\"Job record extended with artifact_id tracking for ML/task workflows.\"\"\"\n\n    artifact_id: ULID | None = Field(default=None, description=\"ID of artifact created by job (if job returns a ULID)\")\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitJobScheduler","title":"<code>ChapkitJobScheduler</code>","text":"<p>               Bases: <code>AIOJobScheduler</code></p> <p>Chapkit job scheduler with automatic artifact tracking for jobs that return ULIDs.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>class ChapkitJobScheduler(AIOJobScheduler):\n    \"\"\"Chapkit job scheduler with automatic artifact tracking for jobs that return ULIDs.\"\"\"\n\n    # Override with ChapkitJobRecord type to support artifact_id tracking\n    # dict is invariant, but we always use ChapkitJobRecord in this subclass\n    _records: dict[ULID, ChapkitJobRecord] = PrivateAttr(default_factory=dict)  # type: ignore[assignment]  # pyright: ignore[reportIncompatibleVariableOverride]\n\n    async def add_job(\n        self,\n        target: JobTarget,\n        /,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; ULID:\n        \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n        now = datetime.now(timezone.utc)\n        jid = ULID()\n\n        record = ChapkitJobRecord(\n            id=jid,\n            status=JobStatus.pending,\n            submitted_at=now,\n        )\n\n        async with self._lock:\n            if jid in self._tasks:\n                raise RuntimeError(f\"Job {jid!r} already scheduled\")\n            self._records[jid] = record\n\n        async def _execute_target() -&gt; Any:\n            if inspect.isawaitable(target):\n                if args or kwargs:\n                    # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                    if inspect.iscoroutine(target):\n                        target.close()\n                    raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n                return await target\n            if inspect.iscoroutinefunction(target):\n                return await target(*args, **kwargs)\n            return await asyncio.to_thread(target, *args, **kwargs)\n\n        async def _runner() -&gt; Any:\n            if self._sema:\n                async with self._sema:\n                    return await self._run_with_state(jid, _execute_target)\n            else:\n                return await self._run_with_state(jid, _execute_target)\n\n        task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n        def _drain(t: asyncio.Task[Any]) -&gt; None:\n            try:\n                t.result()\n            except Exception:\n                pass\n\n        task.add_done_callback(_drain)\n\n        async with self._lock:\n            self._tasks[jid] = task\n\n        return jid\n\n    async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n        \"\"\"Get complete job record with artifact_id if available.\"\"\"\n        async with self._lock:\n            if job_id not in self._records:\n                raise KeyError(f\"Job {job_id} not found\")\n            return self._records[job_id]\n\n    async def list_records(\n        self, *, status_filter: JobStatus | None = None, reverse: bool = False\n    ) -&gt; list[ChapkitJobRecord]:\n        \"\"\"List all job records with optional status filtering.\"\"\"\n        async with self._lock:\n            records = list(self._records.values())\n            if status_filter:\n                records = [r for r in records if r.status == status_filter]\n            if reverse:\n                records = list(reversed(records))\n            return records\n\n    async def _run_with_state(\n        self,\n        jid: ULID,\n        exec_fn: JobExecutor,\n    ) -&gt; Any:\n        \"\"\"Execute job function and track artifact_id if result is a ULID.\"\"\"\n        async with self._lock:\n            rec = self._records[jid]\n            rec.status = JobStatus.running\n            rec.started_at = datetime.now(timezone.utc)\n\n        try:\n            result = await exec_fn()\n\n            # Track artifact_id if job returns a ULID\n            artifact_id: ULID | None = result if isinstance(result, ULID) else None\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.completed\n                rec.finished_at = datetime.now(timezone.utc)\n                rec.artifact_id = artifact_id\n                self._results[jid] = result\n\n            return result\n\n        except asyncio.CancelledError:\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.canceled\n                rec.finished_at = datetime.now(timezone.utc)\n\n            raise\n\n        except Exception as e:\n            tb = traceback.format_exc()\n            # Extract clean error message (exception type and message only)\n            error_lines = tb.strip().split(\"\\n\")\n            clean_error = error_lines[-1] if error_lines else str(e)\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.failed\n                rec.finished_at = datetime.now(timezone.utc)\n                rec.error = clean_error\n                rec.error_traceback = tb\n\n            raise\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitJobScheduler-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.scheduler.ChapkitJobScheduler.add_job","title":"<code>add_job(target, /, *args, **kwargs)</code>  <code>async</code>","text":"<p>Add a job to the scheduler and return its ID.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def add_job(\n    self,\n    target: JobTarget,\n    /,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ULID:\n    \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n    now = datetime.now(timezone.utc)\n    jid = ULID()\n\n    record = ChapkitJobRecord(\n        id=jid,\n        status=JobStatus.pending,\n        submitted_at=now,\n    )\n\n    async with self._lock:\n        if jid in self._tasks:\n            raise RuntimeError(f\"Job {jid!r} already scheduled\")\n        self._records[jid] = record\n\n    async def _execute_target() -&gt; Any:\n        if inspect.isawaitable(target):\n            if args or kwargs:\n                # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                if inspect.iscoroutine(target):\n                    target.close()\n                raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n            return await target\n        if inspect.iscoroutinefunction(target):\n            return await target(*args, **kwargs)\n        return await asyncio.to_thread(target, *args, **kwargs)\n\n    async def _runner() -&gt; Any:\n        if self._sema:\n            async with self._sema:\n                return await self._run_with_state(jid, _execute_target)\n        else:\n            return await self._run_with_state(jid, _execute_target)\n\n    task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n    def _drain(t: asyncio.Task[Any]) -&gt; None:\n        try:\n            t.result()\n        except Exception:\n            pass\n\n    task.add_done_callback(_drain)\n\n    async with self._lock:\n        self._tasks[jid] = task\n\n    return jid\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitJobScheduler.get_record","title":"<code>get_record(job_id)</code>  <code>async</code>","text":"<p>Get complete job record with artifact_id if available.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n    \"\"\"Get complete job record with artifact_id if available.\"\"\"\n    async with self._lock:\n        if job_id not in self._records:\n            raise KeyError(f\"Job {job_id} not found\")\n        return self._records[job_id]\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitJobScheduler.list_records","title":"<code>list_records(*, status_filter=None, reverse=False)</code>  <code>async</code>","text":"<p>List all job records with optional status filtering.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def list_records(\n    self, *, status_filter: JobStatus | None = None, reverse: bool = False\n) -&gt; list[ChapkitJobRecord]:\n    \"\"\"List all job records with optional status filtering.\"\"\"\n    async with self._lock:\n        records = list(self._records.values())\n        if status_filter:\n            records = [r for r in records if r.status == status_filter]\n        if reverse:\n            records = list(reversed(records))\n        return records\n</code></pre>"},{"location":"api-reference/#api-layer","title":"API Layer","text":"<p>FastAPI-specific components built on servicekit.</p>"},{"location":"api-reference/#dependencies","title":"Dependencies","text":"<p>FastAPI dependency injection functions.</p>"},{"location":"api-reference/#chapkit.api.dependencies","title":"<code>dependencies</code>","text":"<p>Feature-specific FastAPI dependency injection for managers.</p>"},{"location":"api-reference/#chapkit.api.dependencies-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.api.dependencies-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.api.dependencies.get_config_manager","title":"<code>get_config_manager(session)</code>  <code>async</code>","text":"<p>Get a config manager instance for dependency injection.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_config_manager(session: Annotated[AsyncSession, Depends(get_session)]) -&gt; ConfigManager[BaseConfig]:\n    \"\"\"Get a config manager instance for dependency injection.\"\"\"\n    repo = ConfigRepository(session)\n    return ConfigManager[BaseConfig](repo, BaseConfig)\n</code></pre>"},{"location":"api-reference/#chapkit.api.dependencies.get_artifact_manager","title":"<code>get_artifact_manager(session)</code>  <code>async</code>","text":"<p>Get an artifact manager instance for dependency injection.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_artifact_manager(session: Annotated[AsyncSession, Depends(get_session)]) -&gt; ArtifactManager:\n    \"\"\"Get an artifact manager instance for dependency injection.\"\"\"\n    artifact_repo = ArtifactRepository(session)\n    return ArtifactManager(artifact_repo)\n</code></pre>"},{"location":"api-reference/#chapkit.api.dependencies.get_ml_manager","title":"<code>get_ml_manager()</code>  <code>async</code>","text":"<p>Get an ML manager instance for dependency injection.</p> <p>Note: This is a placeholder. The actual dependency is built by ServiceBuilder with the runner in closure, then overridden via app.dependency_overrides.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_ml_manager() -&gt; MLManager:\n    \"\"\"Get an ML manager instance for dependency injection.\n\n    Note: This is a placeholder. The actual dependency is built by ServiceBuilder\n    with the runner in closure, then overridden via app.dependency_overrides.\n    \"\"\"\n    raise RuntimeError(\"ML manager dependency not configured. Use ServiceBuilder.with_ml() to enable ML operations.\")\n</code></pre>"},{"location":"api-reference/#alembic-helpers","title":"Alembic Helpers","text":"<p>Reusable migration helpers for chapkit database tables.</p>"},{"location":"api-reference/#chapkit.alembic_helpers","title":"<code>alembic_helpers</code>","text":"<p>Reusable Alembic migration helpers for chapkit tables.</p> <p>This module provides helper functions for creating and dropping chapkit's database tables in Alembic migrations. Using helpers instead of raw Alembic operations provides:</p> <ul> <li>Reusability across migrations</li> <li>Consistent table definitions</li> <li>Clear documentation</li> <li>Easier maintenance</li> </ul> <p>Users can create their own helper modules following this pattern for custom tables.</p> Example Creating Your Own Helpers <p>Follow the same pattern for your custom tables:</p> <p>See examples/custom_migrations/ for a complete working example.</p>"},{"location":"api-reference/#chapkit.alembic_helpers--in-your-migration-file","title":"In your migration file","text":"<p>from chapkit.alembic_helpers import create_configs_table, drop_configs_table</p> <p>def upgrade() -&gt; None:     create_configs_table(op)</p> <p>def downgrade() -&gt; None:     drop_configs_table(op)</p>"},{"location":"api-reference/#chapkit.alembic_helpers--myappalembic_helperspy","title":"myapp/alembic_helpers.py","text":"<p>def create_users_table(op: Any) -&gt; None:     '''Create users table.'''     op.create_table(         'users',         sa.Column('email', sa.String(), nullable=False),         sa.Column('name', sa.String(), nullable=False),         sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),         sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),         sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),         sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),         sa.PrimaryKeyConstraint('id'),     )     op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)</p> <p>def drop_users_table(op: Any) -&gt; None:     '''Drop users table.'''     op.drop_index(op.f('ix_users_email'), table_name='users')     op.drop_table('users')</p>"},{"location":"api-reference/#chapkit.alembic_helpers-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.alembic_helpers.create_artifacts_table","title":"<code>create_artifacts_table(op)</code>","text":"<p>Create artifacts table for hierarchical artifact storage.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Create artifacts table for hierarchical artifact storage.\"\"\"\n    op.create_table(\n        \"artifacts\",\n        sa.Column(\"parent_id\", servicekit.types.ULIDType(length=26), nullable=True),\n        sa.Column(\"data\", sa.PickleType(), nullable=False),\n        sa.Column(\"level\", sa.Integer(), nullable=False),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.ForeignKeyConstraint([\"parent_id\"], [\"artifacts.id\"], ondelete=\"SET NULL\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_artifacts_level\"), \"artifacts\", [\"level\"], unique=False)\n    op.create_index(op.f(\"ix_artifacts_parent_id\"), \"artifacts\", [\"parent_id\"], unique=False)\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_artifacts_table","title":"<code>drop_artifacts_table(op)</code>","text":"<p>Drop artifacts table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Drop artifacts table.\"\"\"\n    op.drop_index(op.f(\"ix_artifacts_parent_id\"), table_name=\"artifacts\")\n    op.drop_index(op.f(\"ix_artifacts_level\"), table_name=\"artifacts\")\n    op.drop_table(\"artifacts\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_configs_table","title":"<code>create_configs_table(op)</code>","text":"<p>Create configs table for configuration storage.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_configs_table(op: Any) -&gt; None:\n    \"\"\"Create configs table for configuration storage.\"\"\"\n    op.create_table(\n        \"configs\",\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"data\", sa.JSON(), nullable=False),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_configs_name\"), \"configs\", [\"name\"], unique=False)\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_configs_table","title":"<code>drop_configs_table(op)</code>","text":"<p>Drop configs table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_configs_table(op: Any) -&gt; None:\n    \"\"\"Drop configs table.\"\"\"\n    op.drop_index(op.f(\"ix_configs_name\"), table_name=\"configs\")\n    op.drop_table(\"configs\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_config_artifacts_table","title":"<code>create_config_artifacts_table(op)</code>","text":"<p>Create config_artifacts junction table linking configs to artifacts.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_config_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Create config_artifacts junction table linking configs to artifacts.\"\"\"\n    op.create_table(\n        \"config_artifacts\",\n        sa.Column(\"config_id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"artifact_id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.ForeignKeyConstraint([\"artifact_id\"], [\"artifacts.id\"], ondelete=\"CASCADE\"),\n        sa.ForeignKeyConstraint([\"config_id\"], [\"configs.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"config_id\", \"artifact_id\"),\n        sa.UniqueConstraint(\"artifact_id\"),\n        sa.UniqueConstraint(\"artifact_id\", name=\"uq_artifact_id\"),\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_config_artifacts_table","title":"<code>drop_config_artifacts_table(op)</code>","text":"<p>Drop config_artifacts junction table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_config_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Drop config_artifacts junction table.\"\"\"\n    op.drop_table(\"config_artifacts\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_tasks_table","title":"<code>create_tasks_table(op)</code>","text":"<p>Create tasks table for task execution infrastructure.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_tasks_table(op: Any) -&gt; None:\n    \"\"\"Create tasks table for task execution infrastructure.\"\"\"\n    op.create_table(\n        \"tasks\",\n        sa.Column(\"command\", sa.Text(), nullable=False),\n        sa.Column(\"task_type\", sa.Text(), nullable=False, server_default=\"shell\"),\n        sa.Column(\"parameters\", sa.JSON(), nullable=True),\n        sa.Column(\"enabled\", sa.Boolean(), nullable=False, server_default=\"1\"),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_tasks_table","title":"<code>drop_tasks_table(op)</code>","text":"<p>Drop tasks table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_tasks_table(op: Any) -&gt; None:\n    \"\"\"Drop tasks table.\"\"\"\n    op.drop_table(\"tasks\")\n</code></pre>"},{"location":"guides/artifact-storage/","title":"Artifact Storage","text":"<p>Servicekit provides a hierarchical artifact storage system for managing ML models, datasets, experimental results, document versions, and any other structured data that needs parent-child relationships and tree-based organization.</p>"},{"location":"guides/artifact-storage/#quick-start","title":"Quick Start","text":"<pre><code>from chapkit.artifact import ArtifactHierarchy, ArtifactManager, ArtifactIn, ArtifactRepository\nfrom servicekit.api import BaseServiceBuilder, ServiceInfo\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n# Define artifact hierarchy\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",\n    level_labels={0: \"experiment\", 1: \"model\", 2: \"predictions\"}\n)\n\n# Create dependency for artifact manager\nasync def get_artifact_manager(session: AsyncSession = Depends(get_session)) -&gt; ArtifactManager:\n    return ArtifactManager(ArtifactRepository(session), hierarchy=hierarchy)\n\n# Build service\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()\n    .with_database(\"sqlite+aiosqlite:///./data.db\")\n    .build()\n)\n\n# Add artifact router\nfrom chapkit.artifact import ArtifactRouter, ArtifactOut\n\nartifact_router = ArtifactRouter.create(\n    prefix=\"/api/v1/artifacts\",\n    tags=[\"Artifacts\"],\n    manager_factory=get_artifact_manager,\n    entity_in_type=ArtifactIn,\n    entity_out_type=ArtifactOut,\n)\napp.include_router(artifact_router)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/artifact-storage/#architecture","title":"Architecture","text":""},{"location":"guides/artifact-storage/#hierarchical-storage","title":"Hierarchical Storage","text":"<p>Artifacts are organized in parent-child trees with configurable level labels:</p> <pre><code>Experiment (level 0, parent_id=None)\n  \u251c\u2500&gt; Model V1 (level 1, parent_id=experiment_id)\n  \u2502    \u251c\u2500&gt; Predictions A (level 2, parent_id=model_v1_id)\n  \u2502    \u2514\u2500&gt; Predictions B (level 2, parent_id=model_v1_id)\n  \u2514\u2500&gt; Model V2 (level 1, parent_id=experiment_id)\n       \u2514\u2500&gt; Predictions C (level 2, parent_id=model_v2_id)\n</code></pre> <p>Benefits: - Complete lineage tracking - Immutable versioning - Multiple children per parent - Flexible hierarchy depths</p>"},{"location":"guides/artifact-storage/#level-labels","title":"Level Labels","text":"<p>Define semantic meaning for each hierarchy level:</p> <pre><code>ArtifactHierarchy(\n    name=\"document_versions\",\n    level_labels={\n        0: \"project\",\n        1: \"document\",\n        2: \"version\"\n    }\n)\n</code></pre>"},{"location":"guides/artifact-storage/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/artifact-storage/#artifacthierarchy","title":"ArtifactHierarchy","text":"<p>Defines the structure and validation rules for artifact trees.</p> <pre><code>from chapkit.artifact import ArtifactHierarchy\n\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",           # Unique identifier\n    level_labels={                # Semantic labels\n        0: \"experiment\",\n        1: \"trained_model\",\n        2: \"predictions\"\n    }\n)\n</code></pre> <p>Properties: - <code>name</code>: Unique identifier for the hierarchy - <code>level_labels</code>: Dict mapping level numbers to semantic labels - Validates parent-child relationships - Enforces level constraints</p>"},{"location":"guides/artifact-storage/#artifact","title":"Artifact","text":"<p>Base entity for hierarchical storage.</p> <pre><code>from chapkit.artifact import ArtifactIn\n\nartifact = ArtifactIn(\n    parent_id=parent_artifact_id,  # Optional: link to parent\n    data={\"key\": \"value\"},         # Any JSON-serializable data\n)\n</code></pre> <p>Fields: - <code>id</code>: ULID (auto-generated) - <code>parent_id</code>: Optional parent artifact ID - <code>level</code>: Hierarchy level (computed from parent) - <code>data</code>: JSON-serializable dictionary - <code>created_at</code>, <code>updated_at</code>: Timestamps</p>"},{"location":"guides/artifact-storage/#artifactmanager","title":"ArtifactManager","text":"<p>Business logic layer for artifact operations.</p> <pre><code>from chapkit.artifact import ArtifactManager, ArtifactRepository\n\nmanager = ArtifactManager(repository, hierarchy=hierarchy)\n\n# Create root artifact\nroot = await manager.save(ArtifactIn(data={\"experiment\": \"v1\"}))\n\n# Create child artifact\nchild = await manager.save(ArtifactIn(\n    parent_id=root.id,\n    data={\"model\": \"trained\"}\n))\n\n# Query tree\ntree = await manager.build_tree(root.id)\n</code></pre>"},{"location":"guides/artifact-storage/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/artifact-storage/#post-apiv1artifacts","title":"POST /api/v1/artifacts","text":"<p>Create new artifact.</p> <p>Request: <pre><code>{\n  \"parent_id\": \"01PARENT123...\",\n  \"data\": {\n    \"experiment\": \"weather_prediction\",\n    \"version\": \"1.0.0\"\n  }\n}\n</code></pre></p> <p>Response (201 Created): <pre><code>{\n  \"id\": \"01ARTIFACT456...\",\n  \"parent_id\": \"01PARENT123...\",\n  \"level\": 1,\n  \"data\": {\n    \"experiment\": \"weather_prediction\",\n    \"version\": \"1.0.0\"\n  },\n  \"created_at\": \"2025-10-18T12:00:00Z\",\n  \"updated_at\": \"2025-10-18T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifacts","title":"GET /api/v1/artifacts","text":"<p>List all artifacts with pagination.</p> <p>Query Parameters: - <code>page</code>: Page number (default: 1) - <code>size</code>: Page size (default: 50)</p> <p>Response: <pre><code>{\n  \"items\": [...],\n  \"total\": 100,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 2\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifactsid","title":"GET /api/v1/artifacts/{id}","text":"<p>Get artifact by ID.</p>"},{"location":"guides/artifact-storage/#put-apiv1artifactsid","title":"PUT /api/v1/artifacts/{id}","text":"<p>Update artifact data.</p> <p>Request: <pre><code>{\n  \"data\": {\n    \"updated\": \"value\"\n  }\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#delete-apiv1artifactsid","title":"DELETE /api/v1/artifacts/{id}","text":"<p>Delete artifact (fails if artifact has children).</p>"},{"location":"guides/artifact-storage/#post-apiv1artifactstree","title":"POST /api/v1/artifacts/$tree","text":"<p>Get artifact tree structure.</p> <p>Request: <pre><code>{\n  \"root_id\": \"01ARTIFACT456...\",\n  \"max_depth\": 3\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": \"01ARTIFACT456...\",\n  \"level\": 0,\n  \"data\": {...},\n  \"children\": [\n    {\n      \"id\": \"01CHILD789...\",\n      \"level\": 1,\n      \"data\": {...},\n      \"children\": [...]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#post-apiv1artifactsexpand","title":"POST /api/v1/artifacts/$expand","text":"<p>Get artifact with all ancestors and descendants.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01ARTIFACT456...\"\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#data-storage-patterns","title":"Data Storage Patterns","text":""},{"location":"guides/artifact-storage/#storing-dataframes","title":"Storing DataFrames","text":"<p>Use <code>DataFrame</code> schema for tabular data:</p> <pre><code>from chapkit.artifact import ArtifactIn\nfrom servicekit.data import DataFrame\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"feature1\": [1.0, 2.0, 3.0],\n    \"feature2\": [4.0, 5.0, 6.0]\n})\n\ndata_frame = DataFrame.from_pandas(df)\n\nartifact = await manager.save(ArtifactIn(\n    data={\"dataframe\": data_frame.model_dump()}\n))\n\n# Retrieve and convert back\nartifact = await manager.find_by_id(artifact_id)\ndf = DataFrame(**artifact.data[\"dataframe\"]).to_pandas()\n</code></pre>"},{"location":"guides/artifact-storage/#storing-binary-data","title":"Storing Binary Data","text":"<p>Store large binary data externally and reference in artifact:</p> <pre><code># Save binary to external storage\nblob_url = await save_to_s3(binary_data)\n\n# Store reference in artifact\nartifact = await manager.save(ArtifactIn(\n    data={\n        \"blob_url\": blob_url,\n        \"blob_size\": len(binary_data),\n        \"blob_type\": \"model/pytorch\"\n    }\n))\n</code></pre>"},{"location":"guides/artifact-storage/#storing-model-objects","title":"Storing Model Objects","text":"<p>Use pickle for Python objects:</p> <pre><code>import pickle\nimport base64\n\n# Serialize model\nmodel_bytes = pickle.dumps(model)\nmodel_b64 = base64.b64encode(model_bytes).decode('utf-8')\n\nartifact = await manager.save(ArtifactIn(\n    data={\n        \"model\": model_b64,\n        \"model_type\": type(model).__name__,\n        \"model_size\": len(model_bytes)\n    }\n))\n\n# Deserialize\nmodel_bytes = base64.b64decode(artifact.data[\"model\"])\nmodel = pickle.loads(model_bytes)\n</code></pre>"},{"location":"guides/artifact-storage/#use-cases","title":"Use Cases","text":""},{"location":"guides/artifact-storage/#ml-model-lineage","title":"ML Model Lineage","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"ml_lineage\",\n    level_labels={\n        0: \"experiment\",\n        1: \"trained_model\",\n        2: \"predictions\"\n    }\n)\n\n# Track experiment\nexperiment = await manager.save(ArtifactIn(\n    data={\n        \"experiment_name\": \"weather_forecast\",\n        \"started_at\": \"2025-10-18T10:00:00Z\"\n    }\n))\n\n# Track trained model\nmodel = await manager.save(ArtifactIn(\n    parent_id=experiment.id,\n    data={\n        \"model_type\": \"LinearRegression\",\n        \"hyperparameters\": {\"alpha\": 0.01},\n        \"metrics\": {\"rmse\": 0.25}\n    }\n))\n\n# Track predictions\npredictions = await manager.save(ArtifactIn(\n    parent_id=model.id,\n    data={\n        \"prediction_date\": \"2025-10-20\",\n        \"num_predictions\": 1000\n    }\n))\n</code></pre>"},{"location":"guides/artifact-storage/#document-versioning","title":"Document Versioning","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"documents\",\n    level_labels={\n        0: \"project\",\n        1: \"document\",\n        2: \"version\"\n    }\n)\n\n# Project\nproject = await manager.save(ArtifactIn(\n    data={\"name\": \"API Documentation\"}\n))\n\n# Document\ndoc = await manager.save(ArtifactIn(\n    parent_id=project.id,\n    data={\"title\": \"Authentication Guide\"}\n))\n\n# Versions\nv1 = await manager.save(ArtifactIn(\n    parent_id=doc.id,\n    data={\"version\": \"1.0\", \"content\": \"...\"}\n))\n\nv2 = await manager.save(ArtifactIn(\n    parent_id=doc.id,\n    data={\"version\": \"1.1\", \"content\": \"...\"}\n))\n</code></pre>"},{"location":"guides/artifact-storage/#dataset-versioning","title":"Dataset Versioning","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"datasets\",\n    level_labels={\n        0: \"dataset\",\n        1: \"version\",\n        2: \"partition\"\n    }\n)\n\n# Dataset\ndataset = await manager.save(ArtifactIn(\n    data={\"name\": \"customer_data\"}\n))\n\n# Version\nversion = await manager.save(ArtifactIn(\n    parent_id=dataset.id,\n    data={\"version\": \"2025-10\", \"rows\": 1000000}\n))\n\n# Partitions\ntrain = await manager.save(ArtifactIn(\n    parent_id=version.id,\n    data={\"partition\": \"train\", \"rows\": 800000}\n))\n\ntest = await manager.save(ArtifactIn(\n    parent_id=version.id,\n    data={\"partition\": \"test\", \"rows\": 200000}\n))\n</code></pre>"},{"location":"guides/artifact-storage/#testing","title":"Testing","text":""},{"location":"guides/artifact-storage/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chapkit.artifact import ArtifactManager, ArtifactRepository, ArtifactIn\n\n@pytest.mark.asyncio\nasync def test_artifact_hierarchy(session):\n    \"\"\"Test parent-child relationships.\"\"\"\n    repo = ArtifactRepository(session)\n    manager = ArtifactManager(repo, hierarchy=test_hierarchy)\n\n    # Create parent\n    parent = await manager.save(ArtifactIn(data={\"level\": \"root\"}))\n    assert parent.level == 0\n    assert parent.parent_id is None\n\n    # Create child\n    child = await manager.save(ArtifactIn(\n        parent_id=parent.id,\n        data={\"level\": \"child\"}\n    ))\n    assert child.level == 1\n    assert child.parent_id == parent.id\n\n    # Build tree\n    tree = await manager.build_tree(parent.id)\n    assert len(tree.children) == 1\n    assert tree.children[0].id == child.id\n</code></pre>"},{"location":"guides/artifact-storage/#curl-testing","title":"cURL Testing","text":"<pre><code># Create root artifact\nROOT_ID=$(curl -s -X POST http://localhost:8000/api/v1/artifacts \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\": {\"experiment\": \"test\"}}' | jq -r '.id')\n\n# Create child artifact\nCHILD_ID=$(curl -s -X POST http://localhost:8000/api/v1/artifacts \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"parent_id\": \"'$ROOT_ID'\",\n    \"data\": {\"model\": \"trained\"}\n  }' | jq -r '.id')\n\n# Get tree\ncurl -X POST http://localhost:8000/api/v1/artifacts/\\$tree \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"root_id\": \"'$ROOT_ID'\"}' | jq\n</code></pre>"},{"location":"guides/artifact-storage/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/artifact-storage/#database-size-management","title":"Database Size Management","text":"<p>Artifacts are stored in SQLite by default. Monitor database growth:</p> <pre><code># Check database size\ndu -h data.db\n\n# Count artifacts\nsqlite3 data.db \"SELECT COUNT(*) FROM artifacts;\"\n</code></pre> <p>Best Practices: - Implement retention policies - Archive old artifacts externally - Monitor BLOB storage growth - Use PostgreSQL for large deployments</p>"},{"location":"guides/artifact-storage/#indexing","title":"Indexing","text":"<p>Default indexes on <code>id</code>, <code>parent_id</code>, <code>level</code>, <code>created_at</code>. Add custom indexes for queries:</p> <pre><code>from sqlalchemy import Index\n\n# Add index on data-&gt;&gt;'experiment_name' for fast lookups\nIndex('ix_artifact_experiment', Artifact.data['experiment_name'].astext)\n</code></pre>"},{"location":"guides/artifact-storage/#backup-strategy","title":"Backup Strategy","text":"<pre><code># SQLite backup\nsqlite3 data.db \".backup backup_$(date +%Y%m%d).db\"\n\n# Automated backups\n0 2 * * * sqlite3 /app/data.db \".backup /backups/data_$(date +\\%Y\\%m\\%d).db\"\n</code></pre>"},{"location":"guides/artifact-storage/#complete-example","title":"Complete Example","text":"<p>See <code>examples/artifact_storage_api.py</code> for a complete working example with document versioning system.</p>"},{"location":"guides/artifact-storage/#next-steps","title":"Next Steps","text":"<ul> <li>Task Execution: Combine artifacts with tasks for ML pipelines</li> <li>Job Scheduler: Use background jobs for expensive artifact operations</li> <li>Monitoring: Track artifact creation rates with Prometheus</li> </ul>"},{"location":"guides/cli-scaffolding/","title":"CLI Scaffolding","text":"<p>Chapkit provides a CLI tool for quickly scaffolding new ML service projects with all necessary configuration files, Docker setup, and optional monitoring stack.</p>"},{"location":"guides/cli-scaffolding/#installation","title":"Installation","text":""},{"location":"guides/cli-scaffolding/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>Use chapkit with uvx (no installation needed):</p> <pre><code># One-off project creation\nuvx chapkit init my-ml-service\n</code></pre> <p>Or install it permanently:</p> <pre><code># Install globally\nuv tool install chapkit\n\n# Use the installed tool\nchapkit init my-ml-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#from-github-development","title":"From GitHub (Development)","text":"<p>To use the latest development version:</p> <pre><code># One-off project creation from GitHub\nuvx --from git+https://github.com/dhis2-chap/chapkit chapkit init my-ml-service\n\n# Or install from GitHub\nuv tool install git+https://github.com/dhis2-chap/chapkit\nchapkit init my-ml-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#managing-installed-tool","title":"Managing Installed Tool","text":"<p>If you installed chapkit with <code>uv tool install</code>, you can manage the installation:</p>"},{"location":"guides/cli-scaffolding/#check-installed-version","title":"Check Installed Version","text":"<pre><code># List installed tools and versions\nuv tool list\n\n# Check version\nchapkit --version\n</code></pre>"},{"location":"guides/cli-scaffolding/#upgrade-to-latest","title":"Upgrade to Latest","text":"<pre><code># Upgrade to latest version\nuv tool upgrade chapkit\n\n# Upgrade to specific version\nuv tool upgrade [email protected]\n</code></pre>"},{"location":"guides/cli-scaffolding/#uninstall","title":"Uninstall","text":"<pre><code># Remove installed tool\nuv tool uninstall chapkit\n</code></pre> <p>Note: When using <code>uvx chapkit</code>, the latest version is used automatically unless you specify a version with <code>@</code>:</p> <pre><code># Always uses latest\nuvx chapkit init my-service\n\n# Pin to specific version\nuvx [email protected] init my-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#quick-start","title":"Quick Start","text":"<p>Create and run a new project:</p> <pre><code># Create project (using uvx for one-off usage)\nuvx chapkit init my-ml-service\ncd my-ml-service\n\n# Install dependencies and run\nuv sync\nuv run python main.py\n</code></pre> <p>Visit http://localhost:8000/docs to interact with the ML API.</p>"},{"location":"guides/cli-scaffolding/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/cli-scaffolding/#chapkit-init","title":"<code>chapkit init</code>","text":"<p>Initialize a new chapkit ML service project.</p> <p>Usage:</p> <pre><code>chapkit init PROJECT_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PROJECT_NAME</code> - Name of the project to create (required)</li> </ul> <p>Options:</p> <ul> <li><code>--path PATH</code> - Target directory (default: current directory)</li> <li><code>--monitoring</code> - Include Prometheus and Grafana monitoring stack</li> <li><code>--help</code> - Show help message</li> </ul> <p>Examples:</p> <pre><code># Using uvx (one-off, no installation needed)\nuvx chapkit init my-service\n\n# Using installed tool\nchapkit init my-service\n\n# Create project in specific location\nchapkit init my-service --path ~/projects\n\n# Create project with monitoring stack\nchapkit init my-service --monitoring\n\n# From GitHub (development version)\nuvx --from git+https://github.com/dhis2-chap/chapkit chapkit init my-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#generated-project-structure","title":"Generated Project Structure","text":""},{"location":"guides/cli-scaffolding/#basic-project","title":"Basic Project","text":"<pre><code>my-service/\n\u251c\u2500\u2500 main.py              # ML service application\n\u251c\u2500\u2500 pyproject.toml       # Python dependencies\n\u251c\u2500\u2500 Dockerfile           # Multi-stage Docker build\n\u251c\u2500\u2500 compose.yml          # Docker Compose configuration\n\u251c\u2500\u2500 .gitignore           # Python gitignore\n\u2514\u2500\u2500 README.md            # Project documentation\n</code></pre>"},{"location":"guides/cli-scaffolding/#with-monitoring","title":"With Monitoring","text":"<p>When using <code>--monitoring</code>, additional files are generated:</p> <pre><code>my-service/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 compose.yml          # Includes Prometheus &amp; Grafana\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 monitoring/\n    \u251c\u2500\u2500 prometheus/\n    \u2502   \u2514\u2500\u2500 prometheus.yml\n    \u2514\u2500\u2500 grafana/\n        \u251c\u2500\u2500 provisioning/\n        \u2502   \u251c\u2500\u2500 datasources/prometheus.yml\n        \u2502   \u2514\u2500\u2500 dashboards/dashboard.yml\n        \u2514\u2500\u2500 dashboards/\n            \u2514\u2500\u2500 chapkit-service-metrics.json\n</code></pre>"},{"location":"guides/cli-scaffolding/#generated-files","title":"Generated Files","text":""},{"location":"guides/cli-scaffolding/#mainpy","title":"main.py","text":"<p>The generated <code>main.py</code> includes:</p> <ul> <li>Config Schema: Pydantic model for ML parameters</li> <li>Training Function: <code>on_train</code> with simple model example</li> <li>Prediction Function: <code>on_predict</code> for inference</li> <li>Service Info: Metadata (name, version, author, status)</li> <li>Artifact Hierarchy: Storage structure for models and predictions</li> <li>FastAPI App: Built using <code>MLServiceBuilder</code></li> </ul> <p>Example structure:</p> <pre><code>class MyServiceConfig(BaseConfig):\n    # Add your model parameters here\n    pass\n\nasync def on_train(config, data, geo=None):\n    # Training logic\n    model = {\"means\": data.select_dtypes(include=[\"number\"]).mean().to_dict()}\n    return model\n\nasync def on_predict(config, model, historic, future, geo=None):\n    # Prediction logic\n    return future\n\nrunner = FunctionalModelRunner(on_train=on_train, on_predict=on_predict)\napp = MLServiceBuilder(...).with_monitoring().build()\n</code></pre>"},{"location":"guides/cli-scaffolding/#pyprojecttoml","title":"pyproject.toml","text":"<p>Defines project metadata and dependencies:</p> <pre><code>[project]\nname = \"my-service\"\nversion = \"0.1.0\"\ndescription = \"ML service for my-service\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\"chapkit\"]\n\n[dependency-groups]\ndev = [\"uvicorn[standard]&gt;=0.30.0\"]\n</code></pre>"},{"location":"guides/cli-scaffolding/#dockerfile","title":"Dockerfile","text":"<p>Multi-stage Docker build with:</p> <ul> <li>Builder stage: UV-based dependency installation</li> <li>Runtime stage: Slim Python image with gunicorn/uvicorn</li> <li>Health checks and proper user setup</li> <li>Environment variables for configuration</li> </ul>"},{"location":"guides/cli-scaffolding/#composeyml","title":"compose.yml","text":"<p>Basic version:</p> <ul> <li>Single service (API) on port 8000</li> <li>Health checks</li> <li>Configurable workers and logging</li> </ul> <p>Monitoring version:</p> <ul> <li>API service (port 8000)</li> <li>Prometheus (port 9090)</li> <li>Grafana (port 3000, admin/admin)</li> <li>Pre-configured dashboards and datasources</li> </ul>"},{"location":"guides/cli-scaffolding/#customization","title":"Customization","text":""},{"location":"guides/cli-scaffolding/#update-model-logic","title":"Update Model Logic","text":"<p>Edit the <code>on_train</code> and <code>on_predict</code> functions in <code>main.py</code>:</p> <pre><code>async def on_train(config, data, geo=None):\n    # Your training logic here\n    from sklearn.ensemble import RandomForestRegressor\n    model = RandomForestRegressor()\n    model.fit(data[features], data[target])\n    return model\n</code></pre>"},{"location":"guides/cli-scaffolding/#add-configuration-parameters","title":"Add Configuration Parameters","text":"<p>Extend the config schema:</p> <pre><code>class MyServiceConfig(BaseConfig):\n    min_samples: int = 5\n    learning_rate: float = 0.001\n    features: list[str] = [\"feature_1\", \"feature_2\"]\n</code></pre>"},{"location":"guides/cli-scaffolding/#add-dependencies","title":"Add Dependencies","text":"<p>Use <code>uv</code> to add packages:</p> <pre><code>uv add scikit-learn pandas numpy\n</code></pre>"},{"location":"guides/cli-scaffolding/#customize-service-metadata","title":"Customize Service Metadata","text":"<p>Update the <code>MLServiceInfo</code>:</p> <pre><code>info = MLServiceInfo(\n    display_name=\"Production Model\",\n    version=\"2.0.0\",\n    summary=\"Production-ready ML service\",\n    description=\"Detailed description here\",\n    author=\"Your Team\",\n    author_assessed_status=AssessedStatus.green,\n    contact_email=\"team@example.com\",\n)\n</code></pre>"},{"location":"guides/cli-scaffolding/#development-workflow","title":"Development Workflow","text":""},{"location":"guides/cli-scaffolding/#local-development","title":"Local Development","text":"<pre><code># Install dependencies\nuv sync\n\n# Run development server\nuv run python main.py\n\n# Run tests (if added)\npytest\n\n# Lint code\nruff check .\n</code></pre>"},{"location":"guides/cli-scaffolding/#docker-development","title":"Docker Development","text":"<pre><code># Build and run\ndocker compose up --build\n\n# Run in background\ndocker compose up -d\n\n# View logs\ndocker compose logs -f\n\n# Stop services\ndocker compose down\n</code></pre>"},{"location":"guides/cli-scaffolding/#access-services","title":"Access Services","text":"<ul> <li>API: http://localhost:8000</li> <li>API Docs: http://localhost:8000/docs</li> <li>Prometheus (if monitoring enabled): http://localhost:9090</li> <li>Grafana (if monitoring enabled): http://localhost:3000</li> </ul>"},{"location":"guides/cli-scaffolding/#next-steps","title":"Next Steps","text":"<p>After scaffolding your project:</p> <ol> <li>Customize the model: Update <code>on_train</code> and <code>on_predict</code> functions</li> <li>Add dependencies: Use <code>uv add</code> to install required packages</li> <li>Update configuration: Add model-specific parameters to config schema</li> <li>Test locally: Run with <code>uv run python main.py</code></li> <li>Dockerize: Build and test with <code>docker compose up --build</code></li> <li>Add tests: Create tests for your training and prediction logic</li> <li>Deploy: Use the generated Dockerfile for deployment</li> </ol>"},{"location":"guides/cli-scaffolding/#related-documentation","title":"Related Documentation","text":"<ul> <li>ML Workflows - Learn about model training and prediction</li> <li>Configuration Management - Working with configs</li> <li>Artifact Storage - Managing models and predictions</li> <li>Task Execution - Scheduling background jobs</li> </ul>"},{"location":"guides/configuration-management/","title":"Configuration Management","text":"<p>Chapkit provides a type-safe configuration management system for storing and managing application settings, environment configurations, and ML model parameters with JSON storage and optional artifact linking.</p>"},{"location":"guides/configuration-management/#quick-start","title":"Quick Start","text":"<pre><code>from chapkit import BaseConfig\nfrom chapkit.api import ServiceBuilder, ServiceInfo\n\nclass AppConfig(BaseConfig):\n    \"\"\"Application configuration schema.\"\"\"\n    debug: bool\n    api_host: str\n    api_port: int\n\napp = (\n    ServiceBuilder(info=ServiceInfo(display_name=\"My Service\"))\n    .with_health()\n    .with_config(AppConfig)\n    .build()\n)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p> <p>Visit http://localhost:8000/docs to manage configurations via Swagger UI.</p>"},{"location":"guides/configuration-management/#architecture","title":"Architecture","text":""},{"location":"guides/configuration-management/#configuration-storage","title":"Configuration Storage","text":"<p>Configurations are stored as key-value pairs with JSON data:</p> <pre><code>Config\n  \u251c\u2500 name: \"production\" (unique identifier)\n  \u251c\u2500 data: {...}         (validated against schema)\n  \u251c\u2500 id: ULID            (auto-generated)\n  \u2514\u2500 created_at, updated_at, tags\n</code></pre>"},{"location":"guides/configuration-management/#type-safety-with-pydantic","title":"Type Safety with Pydantic","text":"<pre><code>class MLConfig(BaseConfig):\n    model_name: str\n    learning_rate: float = 0.001\n    epochs: int = 100\n    features: list[str]\n</code></pre> <p>Benefits: - Compile-time type checking - Runtime validation - Automatic API documentation - JSON schema generation - Extra fields allowed by default</p>"},{"location":"guides/configuration-management/#artifact-linking","title":"Artifact Linking","text":"<p>Link configs to trained models or experiment results:</p> <pre><code>Config(\"production_model\")\n  \u2514\u2500&gt; Trained Model Artifact (level 0)\n       \u251c\u2500&gt; Predictions 1 (level 1)\n       \u2514\u2500&gt; Predictions 2 (level 1)\n</code></pre>"},{"location":"guides/configuration-management/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/configuration-management/#baseconfig","title":"BaseConfig","text":"<p>Base class for all configuration schemas with flexible schema support.</p> <pre><code>from chapkit import BaseConfig\n\nclass DatabaseConfig(BaseConfig):\n    \"\"\"Database connection configuration.\"\"\"\n    host: str\n    port: int = 5432\n    username: str\n    password: str\n    database: str\n    ssl_enabled: bool = True\n</code></pre> <p>Features: - Inherits from <code>pydantic.BaseModel</code> - <code>extra=\"allow\"</code> - accepts arbitrary additional fields - JSON serializable - Validation on instantiation</p>"},{"location":"guides/configuration-management/#configin-configout","title":"ConfigIn / ConfigOut","text":"<p>Input and output schemas for API operations.</p> <pre><code>from chapkit import ConfigIn, ConfigOut\n\n# Create config\nconfig_in = ConfigIn[DatabaseConfig](\n    name=\"production_db\",\n    data=DatabaseConfig(\n        host=\"db.example.com\",\n        port=5432,\n        username=\"app_user\",\n        password=\"secret\",\n        database=\"prod\"\n    )\n)\n\n# Response schema\nconfig_out: ConfigOut[DatabaseConfig] = await manager.save(config_in)\n</code></pre>"},{"location":"guides/configuration-management/#configmanager","title":"ConfigManager","text":"<p>Business logic layer for configuration operations.</p> <pre><code>from chapkit import ConfigManager, ConfigRepository\n\nmanager = ConfigManager[AppConfig](repository, AppConfig)\n\n# Create/update\nconfig = await manager.save(ConfigIn(name=\"dev\", data=app_config))\n\n# Find by name\nconfig = await manager.find_by_name(\"dev\")\n\n# List all\nconfigs = await manager.find_all()\n\n# Delete\nawait manager.delete_by_id(config_id)\n</code></pre>"},{"location":"guides/configuration-management/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/configuration-management/#post-apiv1configs","title":"POST /api/v1/configs","text":"<p>Create new configuration.</p> <p>Request: <pre><code>{\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 8080,\n    \"max_connections\": 2000\n  }\n}\n</code></pre></p> <p>Response (201 Created): <pre><code>{\n  \"id\": \"01K72P5N5KCRM6MD3BRE4P07N8\",\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 8080,\n    \"max_connections\": 2000\n  },\n  \"created_at\": \"2025-10-24T12:00:00Z\",\n  \"updated_at\": \"2025-10-24T12:00:00Z\",\n  \"tags\": []\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#get-apiv1configs","title":"GET /api/v1/configs","text":"<p>List all configurations with pagination.</p> <p>Query Parameters: - <code>page</code>: Page number (default: 1) - <code>size</code>: Page size (default: 50)</p> <p>Response: <pre><code>{\n  \"items\": [...],\n  \"total\": 3,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#get-apiv1configsid","title":"GET /api/v1/configs/{id}","text":"<p>Get configuration by ID.</p>"},{"location":"guides/configuration-management/#put-apiv1configsid","title":"PUT /api/v1/configs/{id}","text":"<p>Update configuration.</p> <p>Request: <pre><code>{\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 9090,\n    \"max_connections\": 3000\n  }\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#delete-apiv1configsid","title":"DELETE /api/v1/configs/{id}","text":"<p>Delete configuration.</p> <p>Note: When deleted, all linked artifact trees are cascade deleted.</p>"},{"location":"guides/configuration-management/#artifact-linking-operations","title":"Artifact Linking Operations","text":"<p>Enable artifact operations when building the service:</p> <pre><code>app = (\n    ServiceBuilder(info=info)\n    .with_config(MLConfig, enable_artifact_operations=True)\n    .with_artifacts(hierarchy=hierarchy)\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#post-apiv1configsidlink-artifact","title":"POST /api/v1/configs/{id}/$link-artifact","text":"<p>Link a root artifact to a config.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01MODEL456...\"\n}\n</code></pre></p> <p>Response: 204 No Content</p> <p>Validation: - Artifact must exist - Artifact must be a root (parent_id is NULL) - Each artifact can only be linked to one config</p>"},{"location":"guides/configuration-management/#post-apiv1configsidunlink-artifact","title":"POST /api/v1/configs/{id}/$unlink-artifact","text":"<p>Unlink an artifact from a config.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01MODEL456...\"\n}\n</code></pre></p> <p>Response: 204 No Content</p>"},{"location":"guides/configuration-management/#get-apiv1configsidartifacts","title":"GET /api/v1/configs/{id}/$artifacts","text":"<p>Get all root artifacts linked to a config.</p> <p>Response: <pre><code>[\n  {\n    \"id\": \"01MODEL456...\",\n    \"parent_id\": null,\n    \"level\": 0,\n    \"data\": {...},\n    \"created_at\": \"2025-10-24T10:00:00Z\",\n    \"updated_at\": \"2025-10-24T10:00:00Z\"\n  }\n]\n</code></pre></p>"},{"location":"guides/configuration-management/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/configuration-management/#environment-configurations","title":"Environment Configurations","text":"<pre><code>class EnvironmentConfig(BaseConfig):\n    \"\"\"Environment-specific configuration.\"\"\"\n    debug: bool\n    api_host: str\n    api_port: int\n    database_url: str\n    log_level: str = \"INFO\"\n    max_connections: int = 100\n\n# Create configs for different environments\nprod_config = ConfigIn(\n    name=\"production\",\n    data=EnvironmentConfig(\n        debug=False,\n        api_host=\"0.0.0.0\",\n        api_port=8080,\n        database_url=\"postgresql://...\",\n        log_level=\"WARNING\",\n        max_connections=2000\n    )\n)\n\ndev_config = ConfigIn(\n    name=\"development\",\n    data=EnvironmentConfig(\n        debug=True,\n        api_host=\"127.0.0.1\",\n        api_port=8000,\n        database_url=\"sqlite:///./dev.db\",\n        log_level=\"DEBUG\",\n        max_connections=10\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#ml-model-configurations","title":"ML Model Configurations","text":"<pre><code>class MLModelConfig(BaseConfig):\n    \"\"\"Machine learning model configuration.\"\"\"\n    model_type: str\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    features: list[str]\n    hyperparameters: dict[str, float]\n\nconfig = ConfigIn(\n    name=\"weather_model_v2\",\n    data=MLModelConfig(\n        model_type=\"RandomForest\",\n        learning_rate=0.001,\n        batch_size=32,\n        epochs=100,\n        features=[\"temperature\", \"humidity\", \"pressure\"],\n        hyperparameters={\n            \"n_estimators\": 100,\n            \"max_depth\": 10,\n            \"min_samples_split\": 2\n        }\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#nested-configurations","title":"Nested Configurations","text":"<pre><code>class DatabaseSettings(BaseModel):\n    \"\"\"Database connection settings.\"\"\"\n    host: str\n    port: int\n    ssl: bool = True\n\nclass CacheSettings(BaseModel):\n    \"\"\"Cache configuration.\"\"\"\n    enabled: bool = True\n    ttl_seconds: int = 3600\n\nclass AppConfig(BaseConfig):\n    \"\"\"Application configuration with nested settings.\"\"\"\n    app_name: str\n    version: str\n    database: DatabaseSettings\n    cache: CacheSettings\n    debug: bool = False\n\nconfig = ConfigIn(\n    name=\"app_config\",\n    data=AppConfig(\n        app_name=\"My API\",\n        version=\"1.0.0\",\n        database=DatabaseSettings(\n            host=\"db.example.com\",\n            port=5432,\n            ssl=True\n        ),\n        cache=CacheSettings(\n            enabled=True,\n            ttl_seconds=7200\n        ),\n        debug=False\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#extra-fields-support","title":"Extra Fields Support","text":"<pre><code># BaseConfig allows extra fields\nconfig = ConfigIn(\n    name=\"flexible_config\",\n    data=AppConfig(\n        required_field=\"value\",\n        dynamic_field=\"extra_value\",  # Not in schema but allowed\n        another_field=123\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#database-seeding","title":"Database Seeding","text":"<p>Seed configurations on application startup:</p> <pre><code>from fastapi import FastAPI\nfrom servicekit import Database\nfrom chapkit import ConfigIn, ConfigManager, ConfigRepository\n\nSEED_CONFIGS = [\n    (\"production\", EnvironmentConfig(debug=False, ...)),\n    (\"staging\", EnvironmentConfig(debug=True, ...)),\n    (\"local\", EnvironmentConfig(debug=True, ...)),\n]\n\nasync def seed_configs(app: FastAPI) -&gt; None:\n    \"\"\"Seed database with predefined configurations.\"\"\"\n    database: Database = app.state.database\n\n    async with database.session() as session:\n        repo = ConfigRepository(session)\n        manager = ConfigManager[EnvironmentConfig](repo, EnvironmentConfig)\n\n        # Clear existing configs (optional)\n        await manager.delete_all()\n\n        # Seed new configs\n        await manager.save_all(\n            ConfigIn(name=name, data=data)\n            for name, data in SEED_CONFIGS\n        )\n\napp = (\n    ServiceBuilder(info=info)\n    .with_config(EnvironmentConfig)\n    .on_startup(seed_configs)\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#complete-workflow-example","title":"Complete Workflow Example","text":""},{"location":"guides/configuration-management/#1-define-configuration-schema","title":"1. Define Configuration Schema","text":"<pre><code>class WeatherModelConfig(BaseConfig):\n    \"\"\"Configuration for weather prediction model.\"\"\"\n    model_version: str\n    training_features: list[str]\n    prediction_horizon_days: int\n    update_frequency: str\n</code></pre>"},{"location":"guides/configuration-management/#2-build-service","title":"2. Build Service","text":"<pre><code>app = (\n    ServiceBuilder(info=ServiceInfo(display_name=\"Weather Model Service\"))\n    .with_health()\n    .with_config(WeatherModelConfig, enable_artifact_operations=True)\n    .with_artifacts(hierarchy=ArtifactHierarchy(\n        name=\"weather_models\",\n        level_labels={0: \"trained_model\", 1: \"predictions\"}\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#3-create-configuration","title":"3. Create Configuration","text":"<pre><code>CONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"weather_v1\",\n    \"data\": {\n      \"model_version\": \"1.0.0\",\n      \"training_features\": [\"temperature\", \"humidity\", \"pressure\"],\n      \"prediction_horizon_days\": 7,\n      \"update_frequency\": \"daily\"\n    }\n  }' | jq -r '.id')\n\necho \"Config ID: $CONFIG_ID\"\n</code></pre>"},{"location":"guides/configuration-management/#4-train-model-creates-artifact","title":"4. Train Model (Creates Artifact)","text":"<pre><code># Train model - creates artifact\nTRAIN_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {...}\n  }')\n\nMODEL_ARTIFACT_ID=$(echo $TRAIN_RESPONSE | jq -r '.model_artifact_id')\n</code></pre>"},{"location":"guides/configuration-management/#5-link-model-to-config","title":"5. Link Model to Config","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/configs/$CONFIG_ID/\\$link-artifact \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"artifact_id\": \"'$MODEL_ARTIFACT_ID'\"}'\n</code></pre>"},{"location":"guides/configuration-management/#6-query-linked-artifacts","title":"6. Query Linked Artifacts","text":"<pre><code>curl http://localhost:8000/api/v1/configs/$CONFIG_ID/\\$artifacts | jq\n</code></pre>"},{"location":"guides/configuration-management/#7-update-configuration","title":"7. Update Configuration","text":"<pre><code>curl -X PUT http://localhost:8000/api/v1/configs/$CONFIG_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"weather_v1\",\n    \"data\": {\n      \"model_version\": \"1.1.0\",\n      \"training_features\": [\"temperature\", \"humidity\", \"pressure\", \"wind_speed\"],\n      \"prediction_horizon_days\": 14,\n      \"update_frequency\": \"twice_daily\"\n    }\n  }' | jq\n</code></pre>"},{"location":"guides/configuration-management/#testing","title":"Testing","text":""},{"location":"guides/configuration-management/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chapkit import BaseConfig, ConfigIn, ConfigManager, ConfigRepository\n\nclass TestConfig(BaseConfig):\n    \"\"\"Test configuration schema.\"\"\"\n    setting: str\n    value: int\n\n@pytest.mark.asyncio\nasync def test_config_crud(session):\n    \"\"\"Test config CRUD operations.\"\"\"\n    repo = ConfigRepository(session)\n    manager = ConfigManager[TestConfig](repo, TestConfig)\n\n    # Create\n    config_in = ConfigIn(\n        name=\"test\",\n        data=TestConfig(setting=\"test_setting\", value=42)\n    )\n    config = await manager.save(config_in)\n\n    assert config.name == \"test\"\n    assert config.data.setting == \"test_setting\"\n    assert config.data.value == 42\n\n    # Find by name\n    found = await manager.find_by_name(\"test\")\n    assert found is not None\n    assert found.id == config.id\n\n    # Update\n    config_in.data.value = 100\n    updated = await manager.save(config_in)\n    assert updated.data.value == 100\n\n    # Delete\n    await manager.delete_by_id(config.id)\n    assert await manager.find_by_id(config.id) is None\n</code></pre>"},{"location":"guides/configuration-management/#integration-tests-with-artifact-linking","title":"Integration Tests with Artifact Linking","text":"<pre><code>@pytest.mark.asyncio\nasync def test_config_artifact_linking(session, artifact_manager, config_manager):\n    \"\"\"Test linking configs to artifacts.\"\"\"\n    # Create config\n    config = await config_manager.save(ConfigIn(\n        name=\"model_config\",\n        data=TestConfig(setting=\"ml\", value=1)\n    ))\n\n    # Create root artifact\n    artifact = await artifact_manager.save(ArtifactIn(\n        data={\"model\": \"trained\"}\n    ))\n\n    # Link artifact to config\n    await config_manager.link_artifact(config.id, artifact.id)\n\n    # Verify link\n    linked_artifacts = await config_manager.get_linked_artifacts(config.id)\n    assert len(linked_artifacts) == 1\n    assert linked_artifacts[0].id == artifact.id\n\n    # Unlink\n    await config_manager.unlink_artifact(artifact.id)\n    linked_artifacts = await config_manager.get_linked_artifacts(config.id)\n    assert len(linked_artifacts) == 0\n</code></pre>"},{"location":"guides/configuration-management/#curl-testing","title":"cURL Testing","text":"<pre><code># Create config\ncurl -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"test\", \"data\": {\"debug\": true, \"port\": 8000}}'\n\n# List configs\ncurl http://localhost:8000/api/v1/configs | jq\n\n# Get by ID\ncurl http://localhost:8000/api/v1/configs/01CONFIG123... | jq\n\n# Update\ncurl -X PUT http://localhost:8000/api/v1/configs/01CONFIG123... \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"test\", \"data\": {\"debug\": false, \"port\": 9000}}'\n\n# Delete\ncurl -X DELETE http://localhost:8000/api/v1/configs/01CONFIG123...\n</code></pre>"},{"location":"guides/configuration-management/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/configuration-management/#configuration-versioning","title":"Configuration Versioning","text":"<p>Use config names to track versions:</p> <pre><code># Version in name\nconfigs = [\n    ConfigIn(name=\"model_v1.0.0\", data=config_data_v1),\n    ConfigIn(name=\"model_v1.1.0\", data=config_data_v11),\n    ConfigIn(name=\"model_v2.0.0\", data=config_data_v2),\n]\n\n# Or in data\nclass VersionedConfig(BaseConfig):\n    version: str\n    settings: dict[str, object]\n\nconfig = ConfigIn(\n    name=\"production_model\",\n    data=VersionedConfig(\n        version=\"1.2.3\",\n        settings={...}\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#environment-variables","title":"Environment Variables","text":"<p>Load configurations from environment:</p> <pre><code>import os\nfrom pydantic_settings import BaseSettings\n\nclass EnvConfig(BaseSettings):\n    \"\"\"Configuration from environment variables.\"\"\"\n    database_url: str\n    api_key: str\n    debug: bool = False\n\n    class Config:\n        env_file = \".env\"\n\n# Load from environment\nenv_config = EnvConfig()\n\n# Store in database\nconfig_in = ConfigIn(\n    name=\"from_env\",\n    data=AppConfig(\n        database_url=env_config.database_url,\n        api_key=env_config.api_key,\n        debug=env_config.debug\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#secrets-management","title":"Secrets Management","text":"<p>Never store secrets in configs:</p> <pre><code># BAD: Secrets in database\nclass BadConfig(BaseConfig):\n    api_key: str  # Don't store in database!\n    password: str  # Don't store in database!\n\n# GOOD: Reference to secrets\nclass GoodConfig(BaseConfig):\n    secret_name: str  # Reference to secret manager\n    credential_id: str  # Reference to vault\n\n# Usage\nconfig = ConfigIn(\n    name=\"api_config\",\n    data=GoodConfig(\n        secret_name=\"production_api_key\",  # Load from AWS Secrets Manager\n        credential_id=\"vault:db/prod\"       # Load from HashiCorp Vault\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#config-validation","title":"Config Validation","text":"<p>Add custom validation:</p> <pre><code>from pydantic import field_validator\n\nclass ValidatedConfig(BaseConfig):\n    \"\"\"Configuration with custom validation.\"\"\"\n    port: int\n    workers: int\n    timeout_seconds: int\n\n    @field_validator(\"port\")\n    @classmethod\n    def validate_port(cls, v: int) -&gt; int:\n        \"\"\"Validate port is in valid range.\"\"\"\n        if not 1024 &lt;= v &lt;= 65535:\n            raise ValueError(\"Port must be between 1024 and 65535\")\n        return v\n\n    @field_validator(\"workers\")\n    @classmethod\n    def validate_workers(cls, v: int) -&gt; int:\n        \"\"\"Validate worker count.\"\"\"\n        if v &lt; 1 or v &gt; 32:\n            raise ValueError(\"Workers must be between 1 and 32\")\n        return v\n</code></pre>"},{"location":"guides/configuration-management/#backup-configurations","title":"Backup Configurations","text":"<pre><code># Export all configs\ncurl http://localhost:8000/api/v1/configs?size=1000 | jq &gt; configs_backup.json\n\n# Restore\ncat configs_backup.json | jq -c '.items[]' | while read config; do\n  curl -X POST http://localhost:8000/api/v1/configs \\\n    -H \"Content-Type: application/json\" \\\n    -d \"$config\"\ndone\n</code></pre>"},{"location":"guides/configuration-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/configuration-management/#validation-errors","title":"Validation Errors","text":"<p>Problem: Config creation fails with validation errors.</p> <p>Cause: Data doesn't match schema.</p> <p>Solution: <pre><code># Check schema\nprint(YourConfig.model_json_schema())\n\n# Validate data before saving\ntry:\n    validated = YourConfig(debug=True, port=\"invalid\")\nexcept ValidationError as e:\n    print(e.errors())\n</code></pre></p>"},{"location":"guides/configuration-management/#extra-fields-not-saved","title":"Extra Fields Not Saved","text":"<p>Problem: Additional fields disappear after saving.</p> <p>Cause: Only fields in schema are saved unless using <code>extra=\"allow\"</code>.</p> <p>Solution: <pre><code># Ensure BaseConfig is used (has extra=\"allow\")\nclass MyConfig(BaseConfig):  # Inherits extra=\"allow\"\n    required_field: str\n    # Extra fields automatically allowed\n</code></pre></p>"},{"location":"guides/configuration-management/#artifact-link-fails","title":"Artifact Link Fails","text":"<p>Problem: \"Artifact is not a root artifact\" error.</p> <p>Cause: Trying to link a child artifact (has parent_id).</p> <p>Solution: <pre><code># Check artifact\ncurl http://localhost:8000/api/v1/artifacts/$ARTIFACT_ID | jq '.parent_id'\n\n# Should be null for root artifacts\n# Only link root artifacts to configs\n</code></pre></p>"},{"location":"guides/configuration-management/#config-deletion-cascade","title":"Config Deletion Cascade","text":"<p>Problem: Deleting config also deletes artifacts.</p> <p>Cause: Cascade delete removes entire artifact tree.</p> <p>Solution: <pre><code># Unlink artifacts before deleting config\nartifacts = await manager.get_linked_artifacts(config_id)\nfor artifact in artifacts:\n    await manager.unlink_artifact(artifact.id)\n\n# Then delete config\nawait manager.delete_by_id(config_id)\n</code></pre></p>"},{"location":"guides/configuration-management/#complete-example","title":"Complete Example","text":"<p>See <code>examples/config_basic/</code> for a complete working example with: - Custom configuration schema - Database seeding - Environment configurations - Custom service metadata - Docker deployment</p>"},{"location":"guides/configuration-management/#next-steps","title":"Next Steps","text":"<ul> <li>Artifact Storage: Link configs to trained models and results</li> <li>ML Workflows: Use configs for training and prediction</li> <li>Database Migrations: Add custom config tables</li> <li>Monitoring: Track config usage and changes</li> </ul>"},{"location":"guides/database-migrations/","title":"Database Migrations","text":"<p>Chapkit uses Alembic for database schema migrations, integrated with servicekit's async SQLAlchemy infrastructure. This guide covers the migration workflow and how to extend chapkit with your own custom database models.</p>"},{"location":"guides/database-migrations/#quick-start","title":"Quick Start","text":"<pre><code># Generate a new migration\nmake migrate MSG='add user table'\n\n# Apply migrations\nmake upgrade\n\n# Rollback last migration\nmake downgrade\n</code></pre> <p>Migrations are automatically applied when your application starts via <code>Database.init()</code>.</p>"},{"location":"guides/database-migrations/#architecture-overview","title":"Architecture Overview","text":""},{"location":"guides/database-migrations/#how-it-works","title":"How It Works","text":"<ol> <li>Base Metadata Registry: All ORM models inherit from <code>servicekit.models.Base</code> or <code>servicekit.models.Entity</code></li> <li>Model Registration: Models with <code>__tablename__</code> are automatically registered with <code>Base.metadata</code></li> <li>Alembic Auto-detection: Alembic's <code>env.py</code> uses <code>Base.metadata</code> as <code>target_metadata</code> to discover all tables</li> <li>Migration Generation: Alembic compares ORM models to current database schema and generates SQL operations</li> </ol>"},{"location":"guides/database-migrations/#chapkits-tables","title":"Chapkit's Tables","text":"<p>Chapkit provides these domain models: - configs - Configuration key-value storage - config_artifacts - Junction table linking configs to artifacts - artifacts - Hierarchical artifact storage - tasks - Task execution infrastructure</p> <p>All inherit from <code>servicekit.models.Entity</code> which provides: <code>id</code> (ULID), <code>created_at</code>, <code>updated_at</code>, <code>tags</code>.</p>"},{"location":"guides/database-migrations/#basic-workflow","title":"Basic Workflow","text":""},{"location":"guides/database-migrations/#1-modify-your-models","title":"1. Modify Your Models","text":"<pre><code># src/myapp/models.py\nfrom servicekit.models import Entity\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass User(Entity):\n    \"\"\"User model with email and name.\"\"\"\n    __tablename__ = \"users\"\n\n    email: Mapped[str] = mapped_column(unique=True, index=True)\n    name: Mapped[str]\n</code></pre> <p>Important: Models must inherit from <code>servicekit.models.Entity</code> or <code>Base</code> to be detected by Alembic.</p>"},{"location":"guides/database-migrations/#2-generate-migration","title":"2. Generate Migration","text":"<pre><code>make migrate MSG='add user table'\n</code></pre> <p>This creates a timestamped migration file in <code>alembic/versions/</code>: <pre><code>alembic/versions/20251024_1430_a1b2c3d4e5f6_add_user_table.py\n</code></pre></p>"},{"location":"guides/database-migrations/#3-review-migration","title":"3. Review Migration","text":"<pre><code>def upgrade() -&gt; None:\n    \"\"\"Apply database schema changes.\"\"\"\n    op.create_table(\n        'users',\n        sa.Column('email', sa.String(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n    )\n    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)\n\ndef downgrade() -&gt; None:\n    \"\"\"Revert database schema changes.\"\"\"\n    op.drop_index(op.f('ix_users_email'), table_name='users')\n    op.drop_table('users')\n</code></pre>"},{"location":"guides/database-migrations/#4-apply-migration","title":"4. Apply Migration","text":"<pre><code># Apply manually\nmake upgrade\n\n# Or restart app (auto-applies on startup)\nfastapi dev main.py\n</code></pre>"},{"location":"guides/database-migrations/#5-verify-schema","title":"5. Verify Schema","text":"<pre><code># Check migration history\nuv run alembic history\n\n# Check current version\nuv run alembic current\n\n# View database schema (SQLite example)\nsqlite3 app.db \".schema users\"\n</code></pre>"},{"location":"guides/database-migrations/#using-your-own-alembic-setup","title":"Using Your Own Alembic Setup","text":"<p>For larger projects, you may want to maintain your own Alembic configuration separate from chapkit while still reusing chapkit's infrastructure.</p>"},{"location":"guides/database-migrations/#project-structure","title":"Project Structure","text":"<pre><code>myproject/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 myapp/\n\u2502       \u251c\u2500\u2500 models.py          # Your custom models\n\u2502       \u2514\u2500\u2500 alembic_helpers.py # Optional: your migration helpers\n\u251c\u2500\u2500 alembic/                   # Your alembic directory\n\u2502   \u251c\u2500\u2500 env.py                 # Your env.py imports servicekit.Base\n\u2502   \u2514\u2500\u2500 versions/              # Your migrations\n\u251c\u2500\u2500 alembic.ini                # Your alembic config\n\u2514\u2500\u2500 main.py\n</code></pre>"},{"location":"guides/database-migrations/#setup-steps","title":"Setup Steps","text":""},{"location":"guides/database-migrations/#1-initialize-your-alembic","title":"1. Initialize Your Alembic","text":"<pre><code>uv run alembic init alembic\n</code></pre>"},{"location":"guides/database-migrations/#2-configure-alembicini","title":"2. Configure alembic.ini","text":"<pre><code>[alembic]\nscript_location = alembic\nfile_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s\ntimezone = UTC\nsqlalchemy.url = sqlite+aiosqlite:///./app.db\n</code></pre>"},{"location":"guides/database-migrations/#3-update-envpy","title":"3. Update env.py","text":"<p>Key Change: Import <code>Base</code> from servicekit to include all models (both chapkit's and yours):</p> <pre><code>\"\"\"Alembic environment configuration for async migrations.\"\"\"\nimport asyncio\nimport sys\nfrom logging.config import fileConfig\nfrom pathlib import Path\n\nfrom alembic import context\nfrom servicekit import Base  # Import Base from servicekit\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\n\n# Add parent directory to path for model imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n# Import your custom models so they register with Base.metadata\nfrom myapp.models import User, Order  # Your models here\n\n# Alembic Config object\nconfig = context.config\n\n# Configure logging\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Target metadata from servicekit.Base (includes all models)\ntarget_metadata = Base.metadata\n\n\ndef run_migrations_offline() -&gt; None:\n    \"\"\"Run migrations in 'offline' mode.\"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection: Connection) -&gt; None:\n    \"\"\"Run migrations with the provided connection.\"\"\"\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_async_migrations() -&gt; None:\n    \"\"\"Run migrations using async engine.\"\"\"\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n\n    await connectable.dispose()\n\n\ndef run_migrations_online() -&gt; None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n    # Create event loop in thread to avoid conflicts with existing async code\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(run_async_migrations())\n    finally:\n        loop.close()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n</code></pre> <p>Critical Points: - Import <code>Base</code> from <code>servicekit</code> - Import all your custom models at the top - Optionally import chapkit's models if you want those tables too - Set <code>target_metadata = Base.metadata</code></p> <p>Note: Only imported models are included in migrations. If you don't import chapkit's models (Config, Artifact, Task), those tables won't be created. The example in <code>examples/custom_migrations/</code> demonstrates a standalone setup with only custom tables.</p>"},{"location":"guides/database-migrations/#4-create-your-models","title":"4. Create Your Models","text":"<pre><code># src/myapp/models.py\n\"\"\"Custom application models.\"\"\"\nfrom servicekit.models import Entity\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import Mapped, mapped_column\nfrom servicekit.types import ULIDType\nfrom ulid import ULID\n\nclass User(Entity):\n    \"\"\"User account model.\"\"\"\n    __tablename__ = \"users\"\n\n    email: Mapped[str] = mapped_column(unique=True, index=True)\n    name: Mapped[str]\n    is_active: Mapped[bool] = mapped_column(default=True)\n\nclass Order(Entity):\n    \"\"\"Order model with user relationship.\"\"\"\n    __tablename__ = \"orders\"\n\n    user_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"users.id\", ondelete=\"CASCADE\"),\n        nullable=False,\n        index=True\n    )\n    total_amount: Mapped[float]\n    status: Mapped[str] = mapped_column(default=\"pending\")\n</code></pre>"},{"location":"guides/database-migrations/#5-generate-migrations","title":"5. Generate Migrations","text":"<pre><code># Using alembic directly\nuv run alembic revision --autogenerate -m \"add user and order tables\"\n\n# Format the generated file\nuv run ruff format alembic/versions/\n</code></pre>"},{"location":"guides/database-migrations/#6-optional-reuse-chapkits-helper-pattern","title":"6. Optional: Reuse Chapkit's Helper Pattern","text":"<p>You can create your own migration helpers following chapkit's pattern:</p> <pre><code># src/myapp/alembic_helpers.py\n\"\"\"Migration helpers for myapp tables.\"\"\"\nfrom typing import Any\nimport sqlalchemy as sa\nimport servicekit.types\n\n\ndef create_users_table(op: Any) -&gt; None:\n    \"\"\"Create users table.\"\"\"\n    op.create_table(\n        'users',\n        sa.Column('email', sa.String(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='1'),\n        sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n    )\n    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)\n\n\ndef drop_users_table(op: Any) -&gt; None:\n    \"\"\"Drop users table.\"\"\"\n    op.drop_index(op.f('ix_users_email'), table_name='users')\n    op.drop_table('users')\n</code></pre> <p>Then use in migrations, optionally mixing with chapkit's helpers:</p> <pre><code>\"\"\"Add user and order tables.\"\"\"\nfrom alembic import op\nfrom myapp.alembic_helpers import create_users_table, drop_users_table\n\n# OPTIONAL: Import chapkit's helpers if you want those tables too\nfrom chapkit.alembic_helpers import (\n    create_configs_table,\n    create_artifacts_table,\n    drop_configs_table,\n    drop_artifacts_table,\n)\n\nrevision = 'a1b2c3d4e5f6'\ndown_revision = None\n\ndef upgrade() -&gt; None:\n    \"\"\"Apply database schema changes.\"\"\"\n    # Create chapkit tables (optional)\n    create_configs_table(op)\n    create_artifacts_table(op)\n\n    # Create your custom tables\n    create_users_table(op)\n\ndef downgrade() -&gt; None:\n    \"\"\"Revert database schema changes.\"\"\"\n    drop_users_table(op)\n    drop_artifacts_table(op)\n    drop_configs_table(op)\n</code></pre> <p>Benefits of Helpers: - Reusable across migrations and projects - Mix chapkit's helpers with your own - Consistent table definitions - Easier to maintain - Clear upgrade/downgrade operations - No need to regenerate migrations when adding chapkit tables</p>"},{"location":"guides/database-migrations/#model-import-requirements","title":"Model Import Requirements","text":"<p>Critical: Models must be imported before Alembic runs for auto-detection to work.</p>"},{"location":"guides/database-migrations/#where-to-import","title":"Where to Import","text":"<p>Option 1: In alembic/env.py (Recommended) <pre><code>from servicekit import Base\nfrom myapp.models import User, Order, Product  # Explicit imports\n</code></pre></p> <p>Option 2: In your app module <pre><code># src/myapp/__init__.py\nfrom .models import User, Order, Product\n\n__all__ = [\"User\", \"Order\", \"Product\"]\n</code></pre></p> <p>Then import in env.py: <pre><code>import myapp  # Imports __init__.py which imports models\n</code></pre></p>"},{"location":"guides/database-migrations/#common-mistake","title":"Common Mistake","text":"<pre><code># BAD: Models not imported\nfrom servicekit import Base\ntarget_metadata = Base.metadata  # Won't include your models!\n</code></pre> <pre><code># GOOD: Models imported before metadata is used\nfrom servicekit import Base\nfrom myapp.models import User, Order\ntarget_metadata = Base.metadata  # Now includes User and Order\n</code></pre>"},{"location":"guides/database-migrations/#mixing-chapkit-and-custom-migrations","title":"Mixing Chapkit and Custom Migrations","text":"<p>You can combine chapkit's tables with your custom tables in migrations.</p> <p>Recommended Approach: Reuse Helpers</p> <p>Import chapkit's helpers directly in your migrations:</p> <pre><code>from alembic import op\nfrom chapkit.alembic_helpers import (\n    create_configs_table,\n    create_artifacts_table,\n    create_tasks_table,\n)\nfrom myapp.alembic_helpers import create_users_table\n\ndef upgrade() -&gt; None:\n    # Create chapkit tables\n    create_configs_table(op)\n    create_artifacts_table(op)\n    create_tasks_table(op)\n\n    # Create your tables\n    create_users_table(op)\n</code></pre> <p>Benefits: - \u2705 No need to regenerate migrations - \u2705 Reuse tested helper functions - \u2705 Explicit control over table creation order - \u2705 Mix and match as needed - \u2705 Clear and maintainable</p> <p>Alternative: Single Alembic with All Models</p> <p>Import all models in <code>env.py</code> and use autogenerate:</p> <pre><code># Import chapkit models\nfrom chapkit.config.models import Config\nfrom chapkit.artifact.models import Artifact\n\n# Import your models\nfrom myapp.models import User, Order\n\n# All models registered in Base.metadata\ntarget_metadata = Base.metadata\n</code></pre> <p>Then: <code>uv run alembic revision --autogenerate -m \"create all tables\"</code></p> <p>Choose the helper approach when: - You want explicit control - You're following the helper pattern - You don't need autogenerate for these tables</p> <p>Choose autogenerate when: - Models change frequently - You prefer automatic detection - You want Alembic to track differences</p>"},{"location":"guides/database-migrations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/database-migrations/#models-not-detected","title":"Models Not Detected","text":"<p>Problem: <code>make migrate</code> doesn't generate changes for your new model.</p> <p>Cause: Model not imported or not inheriting from Base/Entity.</p> <p>Solution: <pre><code># 1. Verify model inherits from Entity or Base\nfrom servicekit.models import Entity\n\nclass User(Entity):  # Must inherit from Entity or Base\n    __tablename__ = \"users\"\n    # ...\n\n# 2. Import model in alembic/env.py\nfrom myapp.models import User  # Add this line\n\n# 3. Verify import works\npython -c \"from myapp.models import User; print(User.__tablename__)\"\n</code></pre></p>"},{"location":"guides/database-migrations/#migration-conflicts","title":"Migration Conflicts","text":"<p>Problem: \"Multiple heads\" or \"Can't determine base revision\".</p> <p>Cause: Branched revision history (two migrations with same parent).</p> <p>Solution: <pre><code># View all revisions\nuv run alembic branches\n\n# Merge branches\nuv run alembic merge -m \"merge branches\" head1 head2\n\n# Apply merged migration\nmake upgrade\n</code></pre></p>"},{"location":"guides/database-migrations/#import-errors-during-migration","title":"Import Errors During Migration","text":"<p>Problem: Migration fails with import errors.</p> <p>Cause: Model imports fail due to circular dependencies or missing packages.</p> <p>Solution: - Use relative imports in models - Avoid importing app-level code in models - Only import SQLAlchemy and servicekit in model files</p>"},{"location":"guides/database-migrations/#autogenerate-not-detecting-changes","title":"Autogenerate Not Detecting Changes","text":"<p>Problem: Changed a column but autogenerate ignores it.</p> <p>Cause: Alembic doesn't always detect all changes automatically.</p> <p>Solution: <pre><code># Create empty migration\nuv run alembic revision -m \"update user table\"\n\n# Manually add changes\ndef upgrade() -&gt; None:\n    op.alter_column('users', 'email', new_column_name='email_address')\n</code></pre></p>"},{"location":"guides/database-migrations/#database-locked-sqlite","title":"Database Locked (SQLite)","text":"<p>Problem: \"Database is locked\" during migration.</p> <p>Cause: Another process has the database open.</p> <p>Solution: <pre><code># Stop all running apps\n# Then apply migration\nmake upgrade\n</code></pre></p> <p>For production, use PostgreSQL instead of SQLite to avoid locking issues.</p>"},{"location":"guides/database-migrations/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/database-migrations/#backup-before-migrating","title":"Backup Before Migrating","text":"<pre><code># SQLite backup\ncp app.db app.db.backup\n\n# PostgreSQL backup\npg_dump -U user -d database &gt; backup.sql\n</code></pre>"},{"location":"guides/database-migrations/#test-migrations","title":"Test Migrations","text":"<pre><code># Apply migration\nmake upgrade\n\n# Verify schema\nsqlite3 app.db \".schema\"\n\n# Test rollback\nmake downgrade\n\n# Reapply\nmake upgrade\n</code></pre>"},{"location":"guides/database-migrations/#migration-in-cicd","title":"Migration in CI/CD","text":"<pre><code># In your deployment script\nuv run alembic upgrade head\n</code></pre>"},{"location":"guides/database-migrations/#multiple-environments","title":"Multiple Environments","text":"<p>Use environment variables for database URLs:</p> <pre><code># alembic/env.py\nimport os\n\nconfig.set_main_option(\n    \"sqlalchemy.url\",\n    os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n)\n</code></pre>"},{"location":"guides/database-migrations/#complete-example","title":"Complete Example","text":"<p>See <code>examples/custom_migrations/</code> for a working example with: - Custom User and Order models - Separate alembic setup - Reusable migration helpers - Integration with chapkit's infrastructure</p>"},{"location":"guides/database-migrations/#next-steps","title":"Next Steps","text":"<ul> <li>Database Guide: Learn about servicekit's Database and SqliteDatabase classes</li> <li>Models Guide: Deep dive into Entity and custom ORM patterns</li> <li>Testing: Test migrations in CI/CD pipelines</li> </ul>"},{"location":"guides/ml-workflows/","title":"ML Workflows","text":"<p>Chapkit provides a complete ML workflow system for training models and making predictions with artifact-based model storage, job scheduling, and hierarchical model lineage tracking.</p>"},{"location":"guides/ml-workflows/#quick-start","title":"Quick Start","text":""},{"location":"guides/ml-workflows/#functional-approach-recommended-for-simple-models","title":"Functional Approach (Recommended for Simple Models)","text":"<pre><code>from chapkit.artifact import ArtifactHierarchy\n\nfrom chapkit import BaseConfig\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo\nfrom chapkit.ml import FunctionalModelRunner\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nclass ModelConfig(BaseConfig):\n    pass\n\nasync def on_train(config: ModelConfig, data: pd.DataFrame, geo=None):\n    X = data[[\"feature1\", \"feature2\"]]\n    y = data[\"target\"]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\nasync def on_predict(config: ModelConfig, model, historic, future, geo=None):\n    X = future[[\"feature1\", \"feature2\"]]\n    future[\"sample_0\"] = model.predict(X)\n    return future\n\nrunner = FunctionalModelRunner(on_train=on_train, on_predict=on_predict)\n\napp = (\n    MLServiceBuilder(\n        info=MLServiceInfo(display_name=\"My ML Service\"),\n        config_schema=ModelConfig,\n        hierarchy=ArtifactHierarchy(name=\"ml\", level_labels={0: \"model\", 1: \"predictions\"}),\n        runner=runner,\n    )\n    .build()\n)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/ml-workflows/#class-based-approach-recommended-for-complex-models","title":"Class-Based Approach (Recommended for Complex Models)","text":"<pre><code>from chapkit.ml import BaseModelRunner\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomModelRunner(BaseModelRunner[ModelConfig]):\n    def __init__(self):\n        self.scaler = StandardScaler()\n\n    async def on_train(self, config: ModelConfig, data, geo=None):\n        X = data[[\"feature1\", \"feature2\"]]\n        y = data[\"target\"]\n\n        X_scaled = self.scaler.fit_transform(X)\n        model = LinearRegression()\n        model.fit(X_scaled, y)\n\n        return {\"model\": model, \"scaler\": self.scaler}\n\n    async def on_predict(self, config: ModelConfig, model, historic, future, geo=None):\n        X = future[[\"feature1\", \"feature2\"]]\n        X_scaled = model[\"scaler\"].transform(X)\n        future[\"sample_0\"] = model[\"model\"].predict(X_scaled)\n        return future\n\nrunner = CustomModelRunner()\n# Use same MLServiceBuilder setup as above\n</code></pre>"},{"location":"guides/ml-workflows/#shell-based-approach-language-agnostic","title":"Shell-Based Approach (Language-Agnostic)","text":"<pre><code>from chapkit.ml import ShellModelRunner\nimport sys\n\ntrain_command = (\n    f\"{sys.executable} scripts/train.py \"\n    \"--config {config_file} --data {data_file} --model {model_file}\"\n)\n\npredict_command = (\n    f\"{sys.executable} scripts/predict.py \"\n    \"--config {config_file} --model {model_file} \"\n    \"--future {future_file} --output {output_file}\"\n)\n\nrunner = ShellModelRunner(\n    train_command=train_command,\n    predict_command=predict_command,\n    model_format=\"pickle\"\n)\n# Use same MLServiceBuilder setup as above\n</code></pre>"},{"location":"guides/ml-workflows/#architecture","title":"Architecture","text":""},{"location":"guides/ml-workflows/#trainpredict-flow","title":"Train/Predict Flow","text":"<pre><code>1. TRAIN                           2. PREDICT\n   POST /api/v1/ml/$train             POST /api/v1/ml/$predict\n   \u251c\u2500&gt; Submit job                     \u251c\u2500&gt; Load trained model artifact\n   \u251c\u2500&gt; Load config                    \u251c\u2500&gt; Load config\n   \u251c\u2500&gt; Execute runner.on_train()      \u251c\u2500&gt; Execute runner.on_predict()\n   \u2514\u2500&gt; Store model in artifact        \u2514\u2500&gt; Store predictions in artifact\n       (level 0, parent_id=None)           (level 1, parent_id=model_id)\n</code></pre>"},{"location":"guides/ml-workflows/#artifact-hierarchy","title":"Artifact Hierarchy","text":"<pre><code>Config\n  \u2514\u2500&gt; Trained Model (level 0)\n       \u251c\u2500&gt; Predictions 1 (level 1)\n       \u251c\u2500&gt; Predictions 2 (level 1)\n       \u2514\u2500&gt; Predictions 3 (level 1)\n</code></pre> <p>Benefits: - Complete model lineage tracking - Multiple predictions from same model - Config linked to all model artifacts - Immutable model versioning</p>"},{"location":"guides/ml-workflows/#job-scheduling","title":"Job Scheduling","text":"<p>All train/predict operations are asynchronous: - Submit returns immediately with <code>job_id</code> and <code>artifact_id</code> - Monitor progress via Job API or SSE streaming - Results stored in artifacts when complete</p>"},{"location":"guides/ml-workflows/#model-runners","title":"Model Runners","text":""},{"location":"guides/ml-workflows/#basemodelrunner","title":"BaseModelRunner","text":"<p>Abstract base class for custom model runners with lifecycle hooks.</p> <pre><code>from chapkit.ml import BaseModelRunner\nfrom chapkit import BaseConfig\n\nclass MyConfig(BaseConfig):\n    \"\"\"Your config schema.\"\"\"\n    pass\n\nclass MyRunner(BaseModelRunner[MyConfig]):\n    async def on_init(self):\n        \"\"\"Called before train or predict (optional).\"\"\"\n        pass\n\n    async def on_cleanup(self):\n        \"\"\"Called after train or predict (optional).\"\"\"\n        pass\n\n    async def on_train(self, config: MyConfig, data, geo=None):\n        \"\"\"Train and return model (must be pickleable).\"\"\"\n        # config is typed as MyConfig - no casting needed!\n        # Your training logic\n        return trained_model\n\n    async def on_predict(self, config: MyConfig, model, historic, future, geo=None):\n        \"\"\"Make predictions and return DataFrame.\"\"\"\n        # config is typed as MyConfig - autocomplete works!\n        # Your prediction logic\n        return predictions_df\n</code></pre> <p>Key Points: - Model must be pickleable (stored in artifact) - Return value from <code>on_train</code> is passed to <code>on_predict</code> as <code>model</code> parameter - <code>historic</code> parameter is required (must be provided, can be empty DataFrame) - GeoJSON support via <code>geo</code> parameter</p>"},{"location":"guides/ml-workflows/#functionalmodelrunner","title":"FunctionalModelRunner","text":"<p>Wraps train/predict functions for functional-style ML workflows.</p> <pre><code>from chapkit.ml import FunctionalModelRunner\n\nasync def train_fn(config, data, geo=None):\n    # Training logic\n    return model\n\nasync def predict_fn(config, model, historic, future, geo=None):\n    # Prediction logic\n    return predictions\n\nrunner = FunctionalModelRunner(on_train=train_fn, on_predict=predict_fn)\n</code></pre> <p>Use Cases: - Simple models without state - Quick prototypes - Pure function workflows</p>"},{"location":"guides/ml-workflows/#shellmodelrunner","title":"ShellModelRunner","text":"<p>Executes external scripts for language-agnostic ML workflows.</p> <pre><code>from chapkit.ml import ShellModelRunner\n\nrunner = ShellModelRunner(\n    train_command=\"python train.py --config {config_file} --data {data_file} --model {model_file}\",\n    predict_command=\"python predict.py --config {config_file} --model {model_file} --future {future_file} --output {output_file}\",\n    model_format=\"pickle\"  # or \"joblib\", \"json\", etc.\n)\n</code></pre> <p>Variable Substitution: - <code>{config_file}</code> - JSON config file - <code>{data_file}</code> - Training data CSV - <code>{model_file}</code> - Model file (format specified) - <code>{future_file}</code> - Future data CSV - <code>{historic_file}</code> - Historic data CSV (required) - <code>{output_file}</code> - Predictions output CSV - <code>{geo_file}</code> - GeoJSON file (if provided)</p> <p>Script Requirements: - Training script: Read data/config, train model, save model to <code>{model_file}</code> - Prediction script: Read model/data/config, make predictions, save to <code>{output_file}</code> - Exit code 0 on success, non-zero on failure - Use stderr for logging</p> <p>Example Training Script (Python): <pre><code>#!/usr/bin/env python3\nimport argparse, json, pickle\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--config\", required=True)\nparser.add_argument(\"--data\", required=True)\nparser.add_argument(\"--model\", required=True)\nargs = parser.parse_args()\n\n# Load config\nwith open(args.config) as f:\n    config = json.load(f)\n\n# Load data\ndata = pd.read_csv(args.data)\n\n# Train\nX = data[[\"feature1\", \"feature2\"]]\ny = data[\"target\"]\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Save\nwith open(args.model, \"wb\") as f:\n    pickle.dump(model, f)\n</code></pre></p> <p>Use Cases: - Integration with R, Julia, or other languages - Legacy scripts without modification - Containerized ML pipelines - Team collaboration across languages</p>"},{"location":"guides/ml-workflows/#servicebuilder-setup","title":"ServiceBuilder Setup","text":""},{"location":"guides/ml-workflows/#mlservicebuilder-recommended","title":"MLServiceBuilder (Recommended)","text":"<p>Bundles health, config, artifacts, jobs, and ML in one builder.</p> <pre><code>from chapkit.artifact import ArtifactHierarchy\n\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo, AssessedStatus\n\ninfo = MLServiceInfo(\n    display_name=\"Disease Prediction Service\",\n    version=\"1.0.0\",\n    summary=\"ML service for disease prediction\",\n    description=\"Train and predict disease cases using weather data\",\n    author=\"ML Team\",\n    author_assessed_status=AssessedStatus.green,\n    contact_email=\"ml-team@example.com\",\n)\n\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",\n    level_labels={0: \"trained_model\", 1: \"predictions\"},\n)\n\napp = (\n    MLServiceBuilder(\n        info=info,\n        config_schema=ModelConfig,\n        hierarchy=hierarchy,\n        runner=runner,\n    )\n    .with_monitoring()  # Optional: Prometheus metrics\n    .build()\n)\n</code></pre> <p>MLServiceBuilder automatically includes: - Health check (<code>/health</code>) - Config CRUD (<code>/api/v1/configs</code>) - Artifact CRUD (<code>/api/v1/artifacts</code>) - Job scheduler (<code>/api/v1/jobs</code>) with concurrency control - ML endpoints (<code>/api/v1/ml/$train</code>, <code>/api/v1/ml/$predict</code>)</p>"},{"location":"guides/ml-workflows/#servicebuilder-manual-configuration","title":"ServiceBuilder (Manual Configuration)","text":"<p>For fine-grained control:</p> <pre><code>from chapkit.api import ServiceBuilder, ServiceInfo\n\napp = (\n    ServiceBuilder(info=ServiceInfo(display_name=\"Custom ML Service\"))\n    .with_health()\n    .with_config(ModelConfig)\n    .with_artifacts(hierarchy=hierarchy)\n    .with_jobs(max_concurrency=3)\n    .with_ml(runner=runner)\n    .build()\n)\n</code></pre> <p>Requirements: - <code>.with_config()</code> must be called before <code>.with_ml()</code> - <code>.with_artifacts()</code> must be called before <code>.with_ml()</code> - <code>.with_jobs()</code> must be called before <code>.with_ml()</code></p>"},{"location":"guides/ml-workflows/#configuration-options","title":"Configuration Options","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=YourConfig,\n    hierarchy=hierarchy,\n    runner=runner,\n    max_concurrency=5,       # Limit concurrent jobs (default: unlimited)\n    database_url=\"ml.db\",    # Persistent storage (default: in-memory)\n)\n</code></pre>"},{"location":"guides/ml-workflows/#api-reference","title":"API Reference","text":""},{"location":"guides/ml-workflows/#post-apiv1mltrain","title":"POST /api/v1/ml/$train","text":"<p>Train a model asynchronously.</p> <p>Request: <pre><code>{\n  \"config_id\": \"01JCONFIG...\",\n  \"data\": {\n    \"columns\": [\"feature1\", \"feature2\", \"target\"],\n    \"data\": [\n      [1.0, 2.0, 10.0],\n      [2.0, 3.0, 15.0],\n      [3.0, 4.0, 20.0]\n    ]\n  },\n  \"geo\": null\n}\n</code></pre></p> <p>Response (202 Accepted): <pre><code>{\n  \"job_id\": \"01JOB123...\",\n  \"model_artifact_id\": \"01MODEL456...\",\n  \"message\": \"Training job submitted. Job ID: 01JOB123...\"\n}\n</code></pre></p> <p>cURL Example: <pre><code># Create config first\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"my_config\", \"data\": {}}' | jq -r '.id')\n\n# Submit training job\ncurl -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {\n      \"columns\": [\"rainfall\", \"temperature\", \"cases\"],\n      \"data\": [[10.0, 25.0, 5.0], [15.0, 28.0, 8.0]]\n    }\n  }' | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#post-apiv1mlpredict","title":"POST /api/v1/ml/$predict","text":"<p>Make predictions using a trained model.</p> <p>Request: <pre><code>{\n  \"model_artifact_id\": \"01MODEL456...\",\n  \"historic\": {\n    \"columns\": [\"feature1\", \"feature2\"],\n    \"data\": []\n  },\n  \"future\": {\n    \"columns\": [\"feature1\", \"feature2\"],\n    \"data\": [\n      [1.5, 2.5],\n      [2.5, 3.5]\n    ]\n  },\n  \"geo\": null\n}\n</code></pre></p> <p>Response (202 Accepted): <pre><code>{\n  \"job_id\": \"01JOB789...\",\n  \"prediction_artifact_id\": \"01PRED012...\",\n  \"message\": \"Prediction job submitted. Job ID: 01JOB789...\"\n}\n</code></pre></p> <p>cURL Example: <pre><code># Use model from training\ncurl -X POST http://localhost:8000/api/v1/ml/\\$predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_artifact_id\": \"'$MODEL_ARTIFACT_ID'\",\n    \"historic\": {\n      \"columns\": [\"rainfall\", \"temperature\"],\n      \"data\": []\n    },\n    \"future\": {\n      \"columns\": [\"rainfall\", \"temperature\"],\n      \"data\": [[12.0, 26.0], [18.0, 29.0]]\n    }\n  }' | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#monitor-job-status","title":"Monitor Job Status","text":"<pre><code># Poll job status\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq\n\n# Stream status updates (SSE)\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n\n# Get results from artifact\nARTIFACT_ID=$(curl -s http://localhost:8000/api/v1/jobs/$JOB_ID | jq -r '.artifact_id')\ncurl http://localhost:8000/api/v1/artifacts/$ARTIFACT_ID | jq\n</code></pre>"},{"location":"guides/ml-workflows/#data-formats","title":"Data Formats","text":""},{"location":"guides/ml-workflows/#dataframe-schema","title":"DataFrame Schema","text":"<p>All tabular data uses the <code>DataFrame</code> schema:</p> <pre><code>{\n  \"columns\": [\"col1\", \"col2\", \"col3\"],\n  \"data\": [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0]\n  ],\n  \"index\": null,\n  \"column_types\": null\n}\n</code></pre> <p>Python Usage: <pre><code>from servicekit.data import DataFrame\n\n# Create from DataFrame\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndata_frame = DataFrame.from_pandas(df)\n\n# Convert to DataFrame\ndf = data_frame.to_pandas()\n</code></pre></p>"},{"location":"guides/ml-workflows/#geojson-support","title":"GeoJSON Support","text":"<p>Optional geospatial data via <code>geojson-pydantic</code>:</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [-122.4194, 37.7749]\n      },\n      \"properties\": {\n        \"name\": \"San Francisco\",\n        \"population\": 883305\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/ml-workflows/#artifact-structure","title":"Artifact Structure","text":""},{"location":"guides/ml-workflows/#trainedmodelartifactdata","title":"TrainedModelArtifactData","text":"<p>Stored at hierarchy level 0:</p> <pre><code>{\n  \"ml_type\": \"trained_model\",\n  \"config_id\": \"01CONFIG...\",\n  \"model\": \"&lt;pickled model object&gt;\",\n  \"model_type\": \"sklearn.linear_model.LinearRegression\",\n  \"model_size_bytes\": 1234,\n  \"started_at\": \"2025-10-14T10:00:00Z\",\n  \"completed_at\": \"2025-10-14T10:00:15Z\",\n  \"duration_seconds\": 15.23\n}\n</code></pre> <p>Fields: - <code>ml_type</code>: Always <code>\"trained_model\"</code> - <code>config_id</code>: Config used for training - <code>model</code>: Pickled model object (any Python object) - <code>model_type</code>: Fully qualified class name (e.g., <code>sklearn.linear_model.LinearRegression</code>) - <code>model_size_bytes</code>: Serialized pickle size - <code>started_at</code>, <code>completed_at</code>: ISO timestamps - <code>duration_seconds</code>: Training duration (rounded to 2 decimals)</p>"},{"location":"guides/ml-workflows/#predictionartifactdata","title":"PredictionArtifactData","text":"<p>Stored at hierarchy level 1 (linked to model):</p> <pre><code>{\n  \"ml_type\": \"prediction\",\n  \"config_id\": \"01CONFIG...\",\n  \"model_artifact_id\": \"01MODEL...\",\n  \"predictions\": {\n    \"columns\": [\"feature1\", \"feature2\", \"sample_0\"],\n    \"data\": [[1.5, 2.5, 12.3], [2.5, 3.5, 17.8]]\n  },\n  \"started_at\": \"2025-10-14T10:05:00Z\",\n  \"completed_at\": \"2025-10-14T10:05:02Z\",\n  \"duration_seconds\": 2.15\n}\n</code></pre> <p>Fields: - <code>ml_type</code>: Always <code>\"prediction\"</code> - <code>config_id</code>: Config used for prediction - <code>model_artifact_id</code>: Parent trained model artifact - <code>predictions</code>: Result DataFrame (DataFrame schema) - <code>started_at</code>, <code>completed_at</code>: ISO timestamps - <code>duration_seconds</code>: Prediction duration (rounded to 2 decimals)</p>"},{"location":"guides/ml-workflows/#complete-workflow-examples","title":"Complete Workflow Examples","text":""},{"location":"guides/ml-workflows/#basic-functional-workflow","title":"Basic Functional Workflow","text":"<pre><code># 1. Start service\nfastapi dev examples/ml_basic.py\n\n# 2. Create config\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"weather_model\", \"data\": {}}' | jq -r '.id')\n\n# 3. Train model\nTRAIN_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\", \"disease_cases\"],\n      \"data\": [\n        [10.0, 25.0, 5.0],\n        [15.0, 28.0, 8.0],\n        [8.0, 22.0, 3.0],\n        [20.0, 30.0, 12.0],\n        [12.0, 26.0, 6.0]\n      ]\n    }\n  }')\n\nJOB_ID=$(echo $TRAIN_RESPONSE | jq -r '.job_id')\nMODEL_ARTIFACT_ID=$(echo $TRAIN_RESPONSE | jq -r '.model_artifact_id')\n\necho \"Training Job ID: $JOB_ID\"\necho \"Model Artifact ID: $MODEL_ARTIFACT_ID\"\n\n# 4. Wait for training completion\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n\n# 5. View trained model\ncurl http://localhost:8000/api/v1/artifacts/$MODEL_ARTIFACT_ID | jq\n\n# 6. Make predictions\nPREDICT_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_artifact_id\": \"'$MODEL_ARTIFACT_ID'\",\n    \"historic\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\"],\n      \"data\": []\n    },\n    \"future\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\"],\n      \"data\": [\n        [11.0, 26.0],\n        [14.0, 27.0],\n        [9.0, 24.0]\n      ]\n    }\n  }')\n\nPRED_JOB_ID=$(echo $PREDICT_RESPONSE | jq -r '.job_id')\nPRED_ARTIFACT_ID=$(echo $PREDICT_RESPONSE | jq -r '.prediction_artifact_id')\n\n# 7. Wait for predictions\ncurl -N http://localhost:8000/api/v1/jobs/$PRED_JOB_ID/\\$stream\n\n# 8. View predictions\ncurl http://localhost:8000/api/v1/artifacts/$PRED_ARTIFACT_ID | jq '.data.predictions'\n</code></pre>"},{"location":"guides/ml-workflows/#class-based-with-preprocessing","title":"Class-Based with Preprocessing","text":"<pre><code># examples/ml_class.py demonstrates:\n# - StandardScaler for feature normalization\n# - State management (scaler shared between train/predict)\n# - Lifecycle hooks (on_init, on_cleanup)\n# - Model artifact containing multiple objects\n\nfrom chapkit.ml import BaseModelRunner\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nclass WeatherModelRunner(BaseModelRunner[WeatherConfig]):\n    def __init__(self):\n        self.feature_names = [\"rainfall\", \"mean_temperature\", \"humidity\"]\n        self.scaler = None\n\n    async def on_train(self, config: WeatherConfig, data, geo=None):\n        X = data[self.feature_names].fillna(0)\n        y = data[\"disease_cases\"].fillna(0)\n\n        # Normalize features\n        self.scaler = StandardScaler()\n        X_scaled = self.scaler.fit_transform(X)\n\n        # Train model\n        model = LinearRegression()\n        model.fit(X_scaled, y)\n\n        # Return dict with model and preprocessing artifacts\n        return {\n            \"model\": model,\n            \"scaler\": self.scaler,\n            \"feature_names\": self.feature_names,\n        }\n\n    async def on_predict(self, config: WeatherConfig, model, historic, future, geo=None):\n        # Extract artifacts\n        trained_model = model[\"model\"]\n        scaler = model[\"scaler\"]\n        feature_names = model[\"feature_names\"]\n\n        # Apply same preprocessing\n        X = future[feature_names].fillna(0)\n        X_scaled = scaler.transform(X)\n\n        # Predict\n        future[\"sample_0\"] = trained_model.predict(X_scaled)\n        return future\n</code></pre> <p>Benefits: - Consistent preprocessing between train/predict - Model artifacts include all necessary objects - Type safety and validation - Easy testing and debugging</p>"},{"location":"guides/ml-workflows/#shell-based-language-agnostic","title":"Shell-Based Language-Agnostic","text":"<pre><code># examples/ml_shell.py demonstrates:\n# - External R/Julia/Python scripts\n# - File-based data interchange\n# - No code modification required\n# - Container-friendly workflows\n\nfrom chapkit.ml import ShellModelRunner\nimport sys\n\nrunner = ShellModelRunner(\n    train_command=f\"{sys.executable} scripts/train_model.py --config {{config_file}} --data {{data_file}} --model {{model_file}}\",\n    predict_command=f\"{sys.executable} scripts/predict_model.py --config {{config_file}} --model {{model_file}} --future {{future_file}} --output {{output_file}}\",\n    model_format=\"pickle\"\n)\n</code></pre> <p>External Script Example (R): <pre><code>#!/usr/bin/env Rscript\nlibrary(jsonlite)\n\nargs &lt;- commandArgs(trailingOnly = TRUE)\nconfig_file &lt;- args[which(args == \"--config\") + 1]\ndata_file &lt;- args[which(args == \"--data\") + 1]\nmodel_file &lt;- args[which(args == \"--model\") + 1]\n\n# Load data\nconfig &lt;- fromJSON(config_file)\ndata &lt;- read.csv(data_file)\n\n# Train model\nmodel &lt;- lm(disease_cases ~ rainfall + mean_temperature, data = data)\n\n# Save model\nsaveRDS(model, model_file)\ncat(\"SUCCESS: Model trained\\n\")\n</code></pre></p>"},{"location":"guides/ml-workflows/#testing","title":"Testing","text":""},{"location":"guides/ml-workflows/#manual-testing","title":"Manual Testing","text":"<p>Terminal 1: <pre><code>fastapi dev examples/ml_basic.py\n</code></pre></p> <p>Terminal 2: <pre><code># Complete workflow test\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -d '{\"name\":\"test\",\"data\":{}}' | jq -r '.id')\n\nTRAIN=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train -d '{\n  \"config_id\":\"'$CONFIG_ID'\",\n  \"data\":{\"columns\":[\"a\",\"b\",\"y\"],\"data\":[[1,2,10],[2,3,15],[3,4,20]]}\n}')\n\nMODEL_ID=$(echo $TRAIN | jq -r '.model_artifact_id')\nJOB_ID=$(echo $TRAIN | jq -r '.job_id')\n\n# Wait for completion\nsleep 2\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.status'\n\n# Predict\nPRED=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$predict -d '{\n  \"model_artifact_id\":\"'$MODEL_ID'\",\n  \"historic\":{\"columns\":[\"a\",\"b\"],\"data\":[]},\n  \"future\":{\"columns\":[\"a\",\"b\"],\"data\":[[1.5,2.5],[2.5,3.5]]}\n}')\n\nPRED_ID=$(echo $PRED | jq -r '.prediction_artifact_id')\nsleep 2\n\n# View results\ncurl http://localhost:8000/api/v1/artifacts/$PRED_ID | jq '.data.predictions'\n</code></pre></p>"},{"location":"guides/ml-workflows/#automated-testing","title":"Automated Testing","text":"<pre><code>import time\nfrom fastapi.testclient import TestClient\n\ndef wait_for_job(client: TestClient, job_id: str, timeout: float = 5.0):\n    \"\"\"Poll until job completes.\"\"\"\n    start = time.time()\n    while time.time() - start &lt; timeout:\n        job = client.get(f\"/api/v1/jobs/{job_id}\").json()\n        if job[\"status\"] in [\"completed\", \"failed\", \"canceled\"]:\n            return job\n        time.sleep(0.1)\n    raise TimeoutError(f\"Job {job_id} timeout\")\n\n\ndef test_train_predict_workflow(client: TestClient):\n    \"\"\"Test complete ML workflow.\"\"\"\n    # Create config\n    config_resp = client.post(\"/api/v1/configs\", json={\n        \"name\": \"test_config\",\n        \"data\": {}\n    })\n    config_id = config_resp.json()[\"id\"]\n\n    # Train\n    train_resp = client.post(\"/api/v1/ml/$train\", json={\n        \"config_id\": config_id,\n        \"data\": {\n            \"columns\": [\"x1\", \"x2\", \"y\"],\n            \"data\": [[1, 2, 10], [2, 3, 15], [3, 4, 20]]\n        }\n    })\n    assert train_resp.status_code == 202\n\n    train_data = train_resp.json()\n    job_id = train_data[\"job_id\"]\n    model_id = train_data[\"model_artifact_id\"]\n\n    # Wait for training\n    job = wait_for_job(client, job_id)\n    assert job[\"status\"] == \"completed\"\n\n    # Verify model artifact\n    model_artifact = client.get(f\"/api/v1/artifacts/{model_id}\").json()\n    assert model_artifact[\"data\"][\"ml_type\"] == \"trained_model\"\n    assert model_artifact[\"level\"] == 0\n\n    # Predict\n    pred_resp = client.post(\"/api/v1/ml/$predict\", json={\n        \"model_artifact_id\": model_id,\n        \"historic\": {\n            \"columns\": [\"x1\", \"x2\"],\n            \"data\": []\n        },\n        \"future\": {\n            \"columns\": [\"x1\", \"x2\"],\n            \"data\": [[1.5, 2.5], [2.5, 3.5]]\n        }\n    })\n    assert pred_resp.status_code == 202\n\n    pred_data = pred_resp.json()\n    pred_job_id = pred_data[\"job_id\"]\n    pred_id = pred_data[\"prediction_artifact_id\"]\n\n    # Wait for prediction\n    pred_job = wait_for_job(client, pred_job_id)\n    assert pred_job[\"status\"] == \"completed\"\n\n    # Verify predictions\n    pred_artifact = client.get(f\"/api/v1/artifacts/{pred_id}\").json()\n    assert pred_artifact[\"data\"][\"ml_type\"] == \"prediction\"\n    assert pred_artifact[\"parent_id\"] == model_id\n    assert pred_artifact[\"level\"] == 1\n    assert \"sample_0\" in pred_artifact[\"data\"][\"predictions\"][\"columns\"]\n</code></pre>"},{"location":"guides/ml-workflows/#browser-testing-swagger-ui","title":"Browser Testing (Swagger UI)","text":"<ol> <li>Open http://localhost:8000/docs</li> <li>Create config via POST <code>/api/v1/configs</code></li> <li>Train via POST <code>/api/v1/ml/$train</code></li> <li>Monitor job via GET <code>/api/v1/jobs/{job_id}</code></li> <li>Predict via POST <code>/api/v1/ml/$predict</code></li> <li>View artifacts via GET <code>/api/v1/artifacts/{artifact_id}</code></li> </ol>"},{"location":"guides/ml-workflows/#production-deployment","title":"Production Deployment","text":""},{"location":"guides/ml-workflows/#concurrency-control","title":"Concurrency Control","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=config_schema,\n    hierarchy=hierarchy,\n    runner=runner,\n    max_concurrency=3,  # Limit concurrent training jobs\n)\n</code></pre> <p>Recommendations: - CPU-intensive models: Set to CPU core count (4-8) - GPU models: Set to GPU count (1-4) - Memory-intensive: Lower limits (2-3) - I/O-bound: Higher limits OK (10-20)</p>"},{"location":"guides/ml-workflows/#database-configuration","title":"Database Configuration","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=config_schema,\n    hierarchy=hierarchy,\n    runner=runner,\n    database_url=\"/data/ml.db\",  # Persistent storage\n)\n</code></pre> <p>Best Practices: - Mount persistent volume for <code>/data</code> - Regular backups (models + artifacts) - Monitor database size growth - Implement artifact retention policies</p>"},{"location":"guides/ml-workflows/#model-versioning","title":"Model Versioning","text":"<pre><code># Use config name for version tracking\nconfig = {\n    \"name\": \"weather_model_v1.2.3\",\n    \"data\": {\n        \"version\": \"1.2.3\",\n        \"features\": [\"rainfall\", \"temperature\"],\n        \"hyperparameters\": {\"alpha\": 0.01}\n    }\n}\n</code></pre> <p>Artifact Hierarchy for Versions: <pre><code>weather_model_v1.0.0 (config)\n  \u2514\u2500&gt; trained_model_1 (artifact level 0)\n       \u2514\u2500&gt; predictions_* (artifact level 1)\n\nweather_model_v1.1.0 (config)\n  \u2514\u2500&gt; trained_model_2 (artifact level 0)\n       \u2514\u2500&gt; predictions_* (artifact level 1)\n</code></pre></p>"},{"location":"guides/ml-workflows/#monitoring","title":"Monitoring","text":"<pre><code>app = (\n    MLServiceBuilder(info=info, config_schema=config, hierarchy=hierarchy, runner=runner)\n    .with_monitoring()  # Prometheus metrics at /metrics\n    .build()\n)\n</code></pre> <p>Available Metrics: - <code>ml_train_jobs_total</code> - Total training jobs submitted - <code>ml_predict_jobs_total</code> - Total prediction jobs submitted - Job scheduler metrics (see Job Scheduler guide)</p> <p>Custom Metrics: <pre><code>from prometheus_client import Histogram\n\nmodel_training_duration = Histogram(\n    'model_training_duration_seconds',\n    'Model training duration'\n)\n\n# Training durations already tracked in artifact metadata\n# Query via artifact API\n</code></pre></p>"},{"location":"guides/ml-workflows/#docker-deployment","title":"Docker Deployment","text":"<p>Dockerfile: <pre><code>FROM python:3.13-slim\n\nWORKDIR /app\nCOPY . /app\n\nRUN pip install --no-cache-dir -e .\n\n# Create non-root user\nRUN useradd -m -u 1000 mluser &amp;&amp; chown -R mluser:mluser /app\nUSER mluser\n\nEXPOSE 8000\n\nCMD [\"fastapi\", \"run\", \"ml_service.py\", \"--host\", \"0.0.0.0\"]\n</code></pre></p> <p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  ml-service:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ml-data:/data\n    environment:\n      - DATABASE_URL=/data/ml.db\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 8G\n\nvolumes:\n  ml-data:\n</code></pre></p>"},{"location":"guides/ml-workflows/#gpu-support","title":"GPU Support","text":"<pre><code>FROM nvidia/cuda:12.0-runtime-ubuntu22.04\nFROM python:3.13\n\n# Install ML libraries with GPU support\nRUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cu120\n\n# Your ML code\nCOPY . /app\n</code></pre> <p>docker-compose.yml: <pre><code>services:\n  ml-service:\n    build: .\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre></p>"},{"location":"guides/ml-workflows/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ml-workflows/#config-not-found-error","title":"\"Config not found\" Error","text":"<p>Problem: Training fails with \"Config {id} not found\"</p> <p>Cause: Invalid or deleted config ID</p> <p>Solution: <pre><code># List configs\ncurl http://localhost:8000/api/v1/configs | jq\n\n# Verify config exists\ncurl http://localhost:8000/api/v1/configs/$CONFIG_ID\n</code></pre></p>"},{"location":"guides/ml-workflows/#model-artifact-not-found-error","title":"\"Model artifact not found\" Error","text":"<p>Problem: Prediction fails with \"Model artifact {id} not found\"</p> <p>Cause: Invalid model artifact ID or training failed</p> <p>Solution: <pre><code># Check training job status\ncurl http://localhost:8000/api/v1/jobs/$TRAIN_JOB_ID | jq\n\n# If training failed, check error\ncurl http://localhost:8000/api/v1/jobs/$TRAIN_JOB_ID | jq '.error'\n\n# List artifacts\ncurl http://localhost:8000/api/v1/artifacts | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#training-job-fails-immediately","title":"Training Job Fails Immediately","text":"<p>Problem: Job status shows \"failed\" right after submission</p> <p>Causes: 1. Model not pickleable 2. Missing required columns in data 3. Insufficient training data 4. Config validation errors</p> <p>Solution: <pre><code># Check job error message\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.error, .error_traceback'\n\n# Common fixes:\n# - Ensure model is pickleable (no lambda functions, local classes)\n# - Verify DataFrame columns match feature expectations\n# - Check config schema validation\n</code></pre></p>"},{"location":"guides/ml-workflows/#prediction-returns-wrong-shape","title":"Prediction Returns Wrong Shape","text":"<p>Problem: Predictions DataFrame has incorrect columns</p> <p>Cause: <code>on_predict</code> must add prediction columns to input DataFrame</p> <p>Solution: <pre><code>async def on_predict(self, config, model, historic, future, geo=None):\n    X = future[[\"feature1\", \"feature2\"]]\n    predictions = model.predict(X)\n\n    # IMPORTANT: Add predictions to future DataFrame\n    future[\"sample_0\"] = predictions  # Required column name\n\n    return future  # Return modified DataFrame\n</code></pre></p>"},{"location":"guides/ml-workflows/#shell-runner-script-fails","title":"Shell Runner Script Fails","text":"<p>Problem: ShellModelRunner returns \"script failed with exit code 1\"</p> <p>Causes: 1. Script not executable 2. Wrong interpreter 3. Missing dependencies 4. File path issues</p> <p>Solution: <pre><code># Make script executable\nchmod +x scripts/train_model.py\n\n# Test script manually\npython scripts/train_model.py \\\n  --config /tmp/test_config.json \\\n  --data /tmp/test_data.csv \\\n  --model /tmp/test_model.pkl\n\n# Check script stderr output\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.error'\n</code></pre></p>"},{"location":"guides/ml-workflows/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Service consuming excessive memory</p> <p>Causes: 1. Large models in memory 2. Too many concurrent jobs 3. Artifact accumulation</p> <p>Solution: <pre><code># Limit concurrent jobs\nMLServiceBuilder(..., max_concurrency=2)\n\n# Implement artifact cleanup\nasync def cleanup_old_artifacts(app):\n    # Delete artifacts older than 30 days\n    cutoff = datetime.now() - timedelta(days=30)\n    # Implementation depends on your needs\n\napp.on_startup(cleanup_old_artifacts)\n</code></pre></p>"},{"location":"guides/ml-workflows/#model-size-too-large","title":"Model Size Too Large","text":"<p>Problem: \"Model size exceeds limit\" or slow artifact storage</p> <p>Cause: Large models (&gt;100MB) stored in SQLite</p> <p>Solution: <pre><code># Option 1: External model storage\nasync def on_train(self, config, data, geo=None):\n    model = train_large_model(data)\n\n    # Save to external storage (S3, etc.)\n    model_url = save_to_s3(model)\n\n    # Return metadata instead of model\n    return {\n        \"model_url\": model_url,\n        \"model_metadata\": {...}\n    }\n\n# Option 2: Use PostgreSQL instead of SQLite\nMLServiceBuilder(..., database_url=\"postgresql://...\")\n</code></pre></p>"},{"location":"guides/ml-workflows/#dataframe-validation-errors","title":"DataFrame Validation Errors","text":"<p>Problem: \"Invalid DataFrame schema\" during train/predict</p> <p>Cause: Incorrect data format in request</p> <p>Solution: <pre><code>// Correct format\n{\n  \"columns\": [\"col1\", \"col2\"],\n  \"data\": [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ]\n}\n\n// Wrong formats:\n// {\"col1\": [1, 3], \"col2\": [2, 4]}  (dict format - not supported)\n// [{\"col1\": 1, \"col2\": 2}]  (records format - not supported)\n</code></pre></p>"},{"location":"guides/ml-workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Job Monitoring: See Job Scheduler guide for SSE streaming</li> <li>Task Execution: Combine with Tasks for preprocessing pipelines</li> <li>Authentication: Secure ML endpoints with API keys</li> <li>Monitoring: Track model performance with Prometheus metrics</li> </ul> <p>For more examples: - <code>examples/ml_basic.py</code> - Functional runner with LinearRegression - <code>examples/ml_class.py</code> - Class-based runner with preprocessing - <code>examples/ml_shell.py</code> - Shell-based runner with external scripts - <code>tests/test_example_ml_basic.py</code> - Complete test suite</p>"},{"location":"guides/task-execution/","title":"Task Execution","text":"<p>Servicekit provides a task execution system for running Python functions and shell commands asynchronously with dependency injection, orphan detection, and artifact storage integration.</p>"},{"location":"guides/task-execution/#quick-start","title":"Quick Start","text":""},{"location":"guides/task-execution/#python-task-functions","title":"Python Task Functions","text":"<pre><code>from chapkit.task import TaskRegistry, TaskManager, TaskIn, TaskRepository, TaskRouter\nfrom servicekit.api import BaseServiceBuilder, ServiceInfo\nfrom servicekit import Database\nfrom chapkit.artifact import ArtifactManager\nfrom fastapi import Depends\n\n# Register Python task\n@TaskRegistry.register(\"greet_user\")\nasync def greet_user(name: str = \"World\") -&gt; dict[str, str]:\n    \"\"\"Simple task that returns a greeting.\"\"\"\n    return {\"message\": f\"Hello, {name}!\"}\n\n# Task with dependency injection\n@TaskRegistry.register(\"process_data\")\nasync def process_data(database: Database, artifact_manager: ArtifactManager) -&gt; dict[str, object]:\n    \"\"\"Dependencies are automatically injected.\"\"\"\n    # Use injected database and artifact_manager\n    return {\"status\": \"complete\", \"database_url\": str(database.url)}\n\n# Build service\napp = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Task Service\"))\n    .with_health()\n    .with_database(\"sqlite+aiosqlite:///./data.db\")\n    .with_jobs(max_concurrency=5)\n    .build()\n)\n\n# Add task router\ntask_router = TaskRouter.create(\n    prefix=\"/api/v1/tasks\",\n    tags=[\"Tasks\"],\n    manager_factory=get_task_manager,  # Dependency factory\n    entity_in_type=TaskIn,\n    entity_out_type=TaskOut,\n)\napp.include_router(task_router)\n</code></pre>"},{"location":"guides/task-execution/#shell-commands","title":"Shell Commands","text":"<pre><code>import sys\n\n# Execute shell commands\nawait manager.save(TaskIn(\n    command=f\"{sys.executable} --version\",\n    enabled=True\n))\n\nawait manager.save(TaskIn(\n    command=\"echo 'Hello from shell!'\",\n    enabled=True\n))\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/task-execution/#architecture","title":"Architecture","text":""},{"location":"guides/task-execution/#task-types","title":"Task Types","text":"<p>Python Functions: - Registered with <code>@TaskRegistry.register(name)</code> - Automatic dependency injection - Return dict with results - Async or sync functions</p> <p>Shell Commands: - Any executable command - Stdout/stderr captured - Exit code handling - Environment variable support</p>"},{"location":"guides/task-execution/#execution-flow","title":"Execution Flow","text":"<pre><code>1. Task Submitted\n   POST /api/v1/tasks\n\n2. Task Stored\n   \u251c\u2500&gt; Database record created\n   \u251c\u2500&gt; Enabled = True/False\n   \u2514\u2500&gt; Metadata stored\n\n3. Task Execution ($execute endpoint)\n   \u251c\u2500&gt; Job created in scheduler\n   \u251c\u2500&gt; Dependencies injected (Python tasks)\n   \u251c\u2500&gt; Process spawned (shell tasks)\n   \u2514\u2500&gt; Results returned\n\n4. Results Storage\n   \u251c\u2500&gt; Return value stored\n   \u251c\u2500&gt; Stdout/stderr captured\n   \u2514\u2500&gt; Optional artifact linking\n</code></pre>"},{"location":"guides/task-execution/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/task-execution/#taskregistry","title":"TaskRegistry","text":"<p>Global registry for Python task functions.</p> <pre><code>from chapkit.task import TaskRegistry\n\n@TaskRegistry.register(\"my_task\")\nasync def my_task(param: str) -&gt; dict[str, object]:\n    \"\"\"Task docstring.\"\"\"\n    return {\"result\": param.upper()}\n\n# Verify registration\nassert TaskRegistry.is_registered(\"my_task\")\n\n# Get task function\ntask_fn = TaskRegistry.get(\"my_task\")\n</code></pre> <p>Rules: - Task names must be unique - Functions must return dict or None - Both async and sync functions supported - Parameters can have defaults</p>"},{"location":"guides/task-execution/#dependency-injection","title":"Dependency Injection","text":"<p>Tasks can request dependencies as function parameters:</p> <pre><code>@TaskRegistry.register(\"with_dependencies\")\nasync def with_dependencies(\n    database: Database,\n    artifact_manager: ArtifactManager,\n    scheduler: JobScheduler,\n    custom_param: str = \"default\"\n) -&gt; dict[str, object]:\n    \"\"\"Dependencies automatically injected at runtime.\"\"\"\n    # Use injected dependencies\n    artifact = await artifact_manager.save(ArtifactIn(data={\"result\": custom_param}))\n    return {\"artifact_id\": str(artifact.id)}\n</code></pre> <p>Available Injectables: - <code>database</code>: Database instance - <code>artifact_manager</code>: ArtifactManager instance - <code>scheduler</code>: JobScheduler instance</p> <p>Note: Task parameters with defaults are treated as task arguments, not injected dependencies.</p>"},{"location":"guides/task-execution/#task-model","title":"Task Model","text":"<pre><code>from chapkit.task import TaskIn\n\ntask = TaskIn(\n    command=\"greet_user\",      # Function name or shell command\n    enabled=True,               # Whether task can be executed\n    params={\"name\": \"Alice\"},  # Parameters for Python tasks\n)\n</code></pre> <p>Fields: - <code>id</code>: ULID (auto-generated) - <code>command</code>: Task function name or shell command - <code>enabled</code>: Boolean flag for execution control - <code>params</code>: Optional dict of parameters (Python tasks only) - <code>created_at</code>, <code>updated_at</code>: Timestamps</p>"},{"location":"guides/task-execution/#taskmanager","title":"TaskManager","text":"<p>Business logic layer for task operations.</p> <pre><code>from chapkit.task import TaskManager\n\nmanager = TaskManager(repository, scheduler, database, artifact_manager)\n\n# Create task\ntask = await manager.save(TaskIn(command=\"greet_user\", enabled=True))\n\n# Execute task\nresult = await manager.execute_task(task.id, params={\"name\": \"Bob\"})\n\n# List all tasks\ntasks = await manager.find_all()\n\n# Find orphaned tasks (no job assigned)\norphans = await manager.find_orphaned_tasks()\n</code></pre>"},{"location":"guides/task-execution/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/task-execution/#post-apiv1tasks","title":"POST /api/v1/tasks","text":"<p>Create new task.</p> <p>Request: <pre><code>{\n  \"command\": \"greet_user\",\n  \"enabled\": true,\n  \"params\": {\n    \"name\": \"Alice\"\n  }\n}\n</code></pre></p> <p>Response (201 Created): <pre><code>{\n  \"id\": \"01TASK123...\",\n  \"command\": \"greet_user\",\n  \"enabled\": true,\n  \"params\": {\n    \"name\": \"Alice\"\n  },\n  \"created_at\": \"2025-10-18T12:00:00Z\",\n  \"updated_at\": \"2025-10-18T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"guides/task-execution/#get-apiv1tasks","title":"GET /api/v1/tasks","text":"<p>List all tasks with pagination.</p>"},{"location":"guides/task-execution/#get-apiv1tasksid","title":"GET /api/v1/tasks/{id}","text":"<p>Get task by ID.</p>"},{"location":"guides/task-execution/#put-apiv1tasksid","title":"PUT /api/v1/tasks/{id}","text":"<p>Update task.</p> <p>Request: <pre><code>{\n  \"enabled\": false,\n  \"params\": {\n    \"name\": \"Updated\"\n  }\n}\n</code></pre></p>"},{"location":"guides/task-execution/#delete-apiv1tasksid","title":"DELETE /api/v1/tasks/{id}","text":"<p>Delete task.</p>"},{"location":"guides/task-execution/#post-apiv1tasksidexecute","title":"POST /api/v1/tasks/{id}/$execute","text":"<p>Execute task asynchronously.</p> <p>Request: <pre><code>{\n  \"params\": {\n    \"name\": \"Bob\"\n  }\n}\n</code></pre></p> <p>Response (202 Accepted): <pre><code>{\n  \"job_id\": \"01JOB456...\",\n  \"message\": \"Task execution started\"\n}\n</code></pre></p>"},{"location":"guides/task-execution/#get-apiv1tasksorphaned","title":"GET /api/v1/tasks/$orphaned","text":"<p>Find orphaned tasks (tasks without assigned jobs).</p> <p>Response: <pre><code>{\n  \"orphaned_tasks\": [\n    {\n      \"id\": \"01TASK789...\",\n      \"command\": \"greet_user\",\n      \"enabled\": true\n    }\n  ],\n  \"count\": 1\n}\n</code></pre></p>"},{"location":"guides/task-execution/#python-task-patterns","title":"Python Task Patterns","text":""},{"location":"guides/task-execution/#simple-task","title":"Simple Task","text":"<pre><code>@TaskRegistry.register(\"hello\")\nasync def hello() -&gt; dict[str, str]:\n    \"\"\"Simple hello world task.\"\"\"\n    return {\"message\": \"Hello!\"}\n</code></pre>"},{"location":"guides/task-execution/#task-with-parameters","title":"Task with Parameters","text":"<pre><code>@TaskRegistry.register(\"add\")\nasync def add(a: int, b: int) -&gt; dict[str, int]:\n    \"\"\"Add two numbers.\"\"\"\n    return {\"result\": a + b}\n\n# Execute with params\nawait manager.execute_task(task_id, params={\"a\": 5, \"b\": 3})\n</code></pre>"},{"location":"guides/task-execution/#task-with-dependency-injection","title":"Task with Dependency Injection","text":"<pre><code>@TaskRegistry.register(\"store_result\")\nasync def store_result(\n    artifact_manager: ArtifactManager,\n    data: dict\n) -&gt; dict[str, object]:\n    \"\"\"Store result in artifact.\"\"\"\n    artifact = await artifact_manager.save(ArtifactIn(data=data))\n    return {\"artifact_id\": str(artifact.id)}\n\n# Execute with params\nawait manager.execute_task(task_id, params={\"data\": {\"key\": \"value\"}})\n</code></pre>"},{"location":"guides/task-execution/#database-query-task","title":"Database Query Task","text":"<pre><code>@TaskRegistry.register(\"count_users\")\nasync def count_users(database: Database) -&gt; dict[str, int]:\n    \"\"\"Count users in database.\"\"\"\n    async with database.session() as session:\n        from sqlalchemy import select, func\n        from myapp.models import User\n\n        stmt = select(func.count(User.id))\n        result = await session.execute(stmt)\n        count = result.scalar()\n\n    return {\"user_count\": count}\n</code></pre>"},{"location":"guides/task-execution/#file-processing-task","title":"File Processing Task","text":"<pre><code>@TaskRegistry.register(\"process_csv\")\nasync def process_csv(filepath: str) -&gt; dict[str, object]:\n    \"\"\"Process CSV file.\"\"\"\n    import pandas as pd\n\n    df = pd.read_csv(filepath)\n    summary = {\n        \"rows\": len(df),\n        \"columns\": list(df.columns),\n        \"summary\": df.describe().to_dict()\n    }\n\n    return summary\n</code></pre>"},{"location":"guides/task-execution/#shell-command-patterns","title":"Shell Command Patterns","text":""},{"location":"guides/task-execution/#simple-commands","title":"Simple Commands","text":"<pre><code># Python version\nawait manager.save(TaskIn(\n    command=f\"{sys.executable} --version\",\n    enabled=True\n))\n\n# Echo command\nawait manager.save(TaskIn(\n    command=\"echo 'Processing complete'\",\n    enabled=True\n))\n\n# List files\nawait manager.save(TaskIn(\n    command=\"ls -la /data\",\n    enabled=True\n))\n</code></pre>"},{"location":"guides/task-execution/#script-execution","title":"Script Execution","text":"<pre><code># Execute Python script\nawait manager.save(TaskIn(\n    command=f\"{sys.executable} scripts/process_data.py --input data.csv --output results.json\",\n    enabled=True\n))\n\n# Execute shell script\nawait manager.save(TaskIn(\n    command=\"bash scripts/backup.sh /data /backups\",\n    enabled=True\n))\n</code></pre>"},{"location":"guides/task-execution/#data-pipeline","title":"Data Pipeline","text":"<pre><code># Multi-step data processing\ncommands = [\n    \"wget https://example.com/data.csv -O /tmp/data.csv\",\n    f\"{sys.executable} scripts/clean_data.py /tmp/data.csv /tmp/clean.csv\",\n    f\"{sys.executable} scripts/analyze.py /tmp/clean.csv /tmp/results.json\"\n]\n\nfor cmd in commands:\n    task = await manager.save(TaskIn(command=cmd, enabled=True))\n    await manager.execute_task(task.id)\n</code></pre>"},{"location":"guides/task-execution/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code># Start service\nfastapi dev examples/task_execution_api.py\n\n# Create Python task\nTASK_ID=$(curl -s -X POST http://localhost:8000/api/v1/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"command\": \"greet_user\",\n    \"enabled\": true,\n    \"params\": {\"name\": \"Alice\"}\n  }' | jq -r '.id')\n\n# Execute task\nEXEC_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/tasks/$TASK_ID/\\$execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": {\"name\": \"Bob\"}}')\n\nJOB_ID=$(echo $EXEC_RESPONSE | jq -r '.job_id')\n\n# Monitor job status\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq\n\n# Stream job updates (SSE)\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n\n# Check for orphaned tasks\ncurl http://localhost:8000/api/v1/tasks/\\$orphaned | jq\n</code></pre>"},{"location":"guides/task-execution/#testing","title":"Testing","text":""},{"location":"guides/task-execution/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chapkit.task import TaskRegistry, TaskManager, TaskIn\n\n@TaskRegistry.register(\"test_task\")\nasync def test_task(value: str) -&gt; dict[str, str]:\n    \"\"\"Test task.\"\"\"\n    return {\"result\": value.upper()}\n\n@pytest.mark.asyncio\nasync def test_task_execution(task_manager):\n    \"\"\"Test task execution.\"\"\"\n    # Create task\n    task = await task_manager.save(TaskIn(\n        command=\"test_task\",\n        enabled=True,\n        params={\"value\": \"hello\"}\n    ))\n\n    # Execute\n    result = await task_manager.execute_task(task.id)\n\n    # Verify\n    assert result[\"result\"] == \"HELLO\"\n</code></pre>"},{"location":"guides/task-execution/#integration-tests","title":"Integration Tests","text":"<pre><code>from fastapi.testclient import TestClient\n\ndef test_task_workflow(client: TestClient):\n    \"\"\"Test complete task workflow.\"\"\"\n    # Create task\n    response = client.post(\"/api/v1/tasks\", json={\n        \"command\": \"greet_user\",\n        \"enabled\": True\n    })\n    assert response.status_code == 201\n    task_id = response.json()[\"id\"]\n\n    # Execute task\n    exec_response = client.post(f\"/api/v1/tasks/{task_id}/$execute\", json={\n        \"params\": {\"name\": \"Test\"}\n    })\n    assert exec_response.status_code == 202\n    job_id = exec_response.json()[\"job_id\"]\n\n    # Wait for completion\n    import time\n    time.sleep(1)\n\n    # Check job\n    job_response = client.get(f\"/api/v1/jobs/{job_id}\")\n    assert job_response.json()[\"status\"] == \"completed\"\n</code></pre>"},{"location":"guides/task-execution/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/task-execution/#concurrency-control","title":"Concurrency Control","text":"<p>Limit concurrent task execution:</p> <pre><code>app = (\n    BaseServiceBuilder(info=ServiceInfo(display_name=\"Task Service\"))\n    .with_jobs(max_concurrency=5)  # Max 5 concurrent tasks\n    .build()\n)\n</code></pre>"},{"location":"guides/task-execution/#error-handling","title":"Error Handling","text":"<p>Tasks should handle errors gracefully:</p> <pre><code>@TaskRegistry.register(\"safe_task\")\nasync def safe_task(risky_param: str) -&gt; dict[str, object]:\n    \"\"\"Task with error handling.\"\"\"\n    try:\n        # Risky operation\n        result = process_risky_operation(risky_param)\n        return {\"status\": \"success\", \"result\": result}\n    except Exception as e:\n        return {\"status\": \"error\", \"error\": str(e)}\n</code></pre>"},{"location":"guides/task-execution/#long-running-tasks","title":"Long-Running Tasks","text":"<p>For long-running tasks, provide progress updates:</p> <pre><code>@TaskRegistry.register(\"long_task\")\nasync def long_task() -&gt; dict[str, object]:\n    \"\"\"Long-running task.\"\"\"\n    import time\n\n    steps = 10\n    for i in range(steps):\n        # Do work\n        time.sleep(1)\n        # Log progress (captured in job logs)\n        print(f\"Progress: {i+1}/{steps}\")\n\n    return {\"status\": \"complete\"}\n</code></pre>"},{"location":"guides/task-execution/#orphan-detection","title":"Orphan Detection","text":"<p>Regularly check for orphaned tasks:</p> <pre><code># Cron job to detect orphans\n*/15 * * * * curl http://localhost:8000/api/v1/tasks/\\$orphaned\n</code></pre>"},{"location":"guides/task-execution/#shell-command-security","title":"Shell Command Security","text":"<p>Important: Validate shell commands to prevent injection:</p> <pre><code>import shlex\n\ndef validate_command(command: str) -&gt; bool:\n    \"\"\"Validate shell command for safety.\"\"\"\n    # Parse command safely\n    try:\n        args = shlex.split(command)\n        # Check against whitelist\n        allowed_executables = [\"python\", \"bash\", \"ls\", \"echo\"]\n        if args[0] not in allowed_executables:\n            return False\n        return True\n    except ValueError:\n        return False\n\n# Use in task creation\nif validate_command(task_command):\n    task = await manager.save(TaskIn(command=task_command, enabled=True))\n</code></pre>"},{"location":"guides/task-execution/#complete-example","title":"Complete Example","text":"<p>See <code>examples/task_execution_api.py</code> for a complete working example with Python tasks, shell commands, and dependency injection.</p>"},{"location":"guides/task-execution/#next-steps","title":"Next Steps","text":"<ul> <li>Job Scheduler: Learn about job monitoring and concurrency control</li> <li>Artifact Storage: Store task results in artifacts</li> <li>Authentication: Secure task endpoints with API keys</li> <li>Monitoring: Track task execution with Prometheus metrics</li> </ul>"}]}