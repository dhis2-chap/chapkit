{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Chapkit","text":"<p>Build production-ready ML services with train/predict workflows, artifact storage, config management, and job scheduling - all in a few lines of code.</p>"},{"location":"#quick-start-ml-service","title":"Quick Start: ML Service","text":"<pre><code>from geojson_pydantic import FeatureCollection\n\nfrom chapkit import BaseConfig\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo\nfrom chapkit.artifact import ArtifactHierarchy\nfrom chapkit.data import DataFrame\nfrom chapkit.ml import FunctionalModelRunner\n\n\nclass MyMLConfig(BaseConfig):\n    \"\"\"Configuration for your ML model.\"\"\"\n\n    prediction_periods: int = 3\n\n\nasync def train_model(\n    config: MyMLConfig,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; dict:\n    \"\"\"Train your model - returns trained model object.\"\"\"\n    df = data.to_pandas()\n    # Your training logic here - example using sklearn:\n    # from sklearn.linear_model import LinearRegression\n    # model = LinearRegression()\n    # model.fit(df[[\"feature1\", \"feature2\"]], df[\"target\"])\n    return {\"trained\": True}\n\n\nasync def predict(\n    config: MyMLConfig,\n    model: dict,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions using the trained model.\"\"\"\n    future_df = future.to_pandas()\n    # Your prediction logic here\n    future_df[\"sample_0\"] = 0.0  # Replace with actual predictions\n    return DataFrame.from_pandas(future_df)\n\n\napp = (\n    MLServiceBuilder(\n        info=MLServiceInfo(id=\"disease-prediction-service\", display_name=\"Disease Prediction Service\"),\n        config_schema=MyMLConfig,\n        hierarchy=ArtifactHierarchy(\n            name=\"ml\",\n            level_labels={0: \"ml_training_workspace\", 1: \"ml_prediction\"},\n        ),\n        runner=FunctionalModelRunner(on_train=train_model, on_predict=predict),\n    )\n    .with_monitoring()  # Optional: Add Prometheus metrics\n    .build()\n)\n</code></pre> <p>What you get: - <code>POST /api/v1/ml/train</code> - Train models with versioning - <code>POST /api/v1/ml/predict</code> - Make predictions - <code>GET /api/v1/configs</code> - Manage model configurations - <code>GET /api/v1/artifacts</code> - Browse trained models and predictions - <code>GET /api/v1/jobs</code> - Monitor training/prediction jobs - <code>GET /health</code> - Health checks - <code>GET /metrics</code> - Prometheus metrics (with <code>.with_monitoring()</code>)</p> <p>Run with: <code>fastapi dev your_file.py</code> \u2192 Service ready at <code>http://localhost:8000</code></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv add chapkit\n</code></pre> <p>Chapkit automatically installs servicekit as a dependency.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Repository</li> <li>Issues</li> <li>Servicekit - Core framework foundation (docs)</li> </ul>"},{"location":"#license","title":"License","text":"<p>AGPL-3.0-or-later</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for all chapkit modules, classes, and functions.</p>"},{"location":"api-reference/#artifact-module","title":"Artifact Module","text":"<p>Hierarchical storage system for models, data, and experiment tracking.</p>"},{"location":"api-reference/#models","title":"Models","text":""},{"location":"api-reference/#chapkit.artifact.models","title":"<code>models</code>","text":"<p>Artifact ORM model for hierarchical data storage.</p>"},{"location":"api-reference/#chapkit.artifact.models-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.models.Artifact","title":"<code>Artifact</code>","text":"<p>               Bases: <code>Entity</code></p> <p>ORM model for hierarchical artifacts with parent-child relationships.</p> Source code in <code>src/chapkit/artifact/models.py</code> <pre><code>class Artifact(Entity):\n    \"\"\"ORM model for hierarchical artifacts with parent-child relationships.\"\"\"\n\n    __tablename__ = \"artifacts\"\n\n    parent_id: Mapped[ULID | None] = mapped_column(\n        ULIDType,\n        ForeignKey(\"artifacts.id\", ondelete=\"SET NULL\"),\n        nullable=True,\n        index=True,\n    )\n\n    parent: Mapped[Artifact | None] = relationship(\n        remote_side=\"Artifact.id\",\n        back_populates=\"children\",\n    )\n\n    children: Mapped[list[Artifact]] = relationship(\n        back_populates=\"parent\",\n    )\n\n    data: Mapped[Any] = mapped_column(PickleType(protocol=4), nullable=False)\n    level: Mapped[int] = mapped_column(default=0, nullable=False, index=True)\n</code></pre>"},{"location":"api-reference/#schemas","title":"Schemas","text":""},{"location":"api-reference/#chapkit.artifact.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for hierarchical artifacts with tree structures.</p>"},{"location":"api-reference/#chapkit.artifact.schemas-attributes","title":"Attributes","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactData","title":"<code>ArtifactData = Annotated[MLTrainingWorkspaceArtifactData | MLPredictionArtifactData | MLPredictionWorkspaceArtifactData | GenericArtifactData, Field(discriminator='type')]</code>  <code>module-attribute</code>","text":"<p>Discriminated union type for all artifact data types.</p>"},{"location":"api-reference/#chapkit.artifact.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactIn","title":"<code>ArtifactIn</code>","text":"<p>               Bases: <code>EntityIn</code></p> <p>Input schema for creating or updating artifacts.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactIn(EntityIn):\n    \"\"\"Input schema for creating or updating artifacts.\"\"\"\n\n    data: Any\n    parent_id: ULID | None = None\n    level: int | None = None\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactOut","title":"<code>ArtifactOut</code>","text":"<p>               Bases: <code>EntityOut</code></p> <p>Output schema for artifact entities.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactOut(EntityOut):\n    \"\"\"Output schema for artifact entities.\"\"\"\n\n    data: JsonSafe\n    parent_id: ULID | None = None\n    level: int\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode","title":"<code>ArtifactTreeNode</code>","text":"<p>               Bases: <code>ArtifactOut</code></p> <p>Artifact node with tree structure metadata.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactTreeNode(ArtifactOut):\n    \"\"\"Artifact node with tree structure metadata.\"\"\"\n\n    level_label: str | None = None\n    hierarchy: str | None = None\n    children: list[\"ArtifactTreeNode\"] | None = None\n\n    @classmethod\n    def from_artifact(cls, artifact: ArtifactOut) -&gt; Self:\n        \"\"\"Create a tree node from an artifact output schema.\"\"\"\n        return cls.model_validate(artifact.model_dump())\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactTreeNode.from_artifact","title":"<code>from_artifact(artifact)</code>  <code>classmethod</code>","text":"<p>Create a tree node from an artifact output schema.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>@classmethod\ndef from_artifact(cls, artifact: ArtifactOut) -&gt; Self:\n    \"\"\"Create a tree node from an artifact output schema.\"\"\"\n    return cls.model_validate(artifact.model_dump())\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy","title":"<code>ArtifactHierarchy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for artifact hierarchy with level labels.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class ArtifactHierarchy(BaseModel):\n    \"\"\"Configuration for artifact hierarchy with level labels.\"\"\"\n\n    name: str = Field(..., description=\"Human readable name of this hierarchy\")\n    level_labels: Mapping[int, str] = Field(\n        default_factory=dict,\n        description=\"Mapping of numeric levels to labels (0 -&gt; 'train', etc.)\",\n    )\n\n    model_config = {\"frozen\": True}\n\n    hierarchy_key: ClassVar[str] = \"hierarchy\"\n    depth_key: ClassVar[str] = \"level_depth\"\n    label_key: ClassVar[str] = \"level_label\"\n\n    def label_for(self, level: int) -&gt; str:\n        \"\"\"Get the label for a given level or return default.\"\"\"\n        return self.level_labels.get(level, f\"level_{level}\")\n\n    def describe(self, level: int) -&gt; dict[str, Any]:\n        \"\"\"Get hierarchy metadata dict for a given level.\"\"\"\n        return {\n            self.hierarchy_key: self.name,\n            self.depth_key: level,\n            self.label_key: self.label_for(level),\n        }\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy.label_for","title":"<code>label_for(level)</code>","text":"<p>Get the label for a given level or return default.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>def label_for(self, level: int) -&gt; str:\n    \"\"\"Get the label for a given level or return default.\"\"\"\n    return self.level_labels.get(level, f\"level_{level}\")\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.ArtifactHierarchy.describe","title":"<code>describe(level)</code>","text":"<p>Get hierarchy metadata dict for a given level.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>def describe(self, level: int) -&gt; dict[str, Any]:\n    \"\"\"Get hierarchy metadata dict for a given level.\"\"\"\n    return {\n        self.hierarchy_key: self.name,\n        self.depth_key: level,\n        self.label_key: self.label_for(level),\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.BaseArtifactData","title":"<code>BaseArtifactData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all artifact data types with typed metadata.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class BaseArtifactData[MetadataT: BaseModel](BaseModel):\n    \"\"\"Base class for all artifact data types with typed metadata.\"\"\"\n\n    type: str = Field(description=\"Discriminator field for artifact type\")\n    metadata: MetadataT = Field(description=\"Strongly-typed JSON-serializable metadata\")\n    content: JsonSafe = Field(description=\"Content as Python object (bytes, DataFrame, models, etc.)\")\n    content_type: str | None = Field(default=None, description=\"MIME type for download responses\")\n    content_size: int | None = Field(default=None, description=\"Size of content in bytes\")\n\n    model_config = {\"extra\": \"forbid\"}\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.MLMetadata","title":"<code>MLMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for ML artifacts (training and prediction).</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLMetadata(BaseModel):\n    \"\"\"Metadata for ML artifacts (training and prediction).\"\"\"\n\n    status: Literal[\"success\", \"failed\"] = Field(description=\"Job execution status\")\n    config_id: str = Field(description=\"ID of the config used for this operation\")\n    started_at: str = Field(description=\"ISO 8601 timestamp when operation started\")\n    completed_at: str = Field(description=\"ISO 8601 timestamp when operation completed\")\n    duration_seconds: float = Field(description=\"Operation duration in seconds\")\n    exit_code: int | None = Field(default=None, description=\"Execution exit code (if applicable)\")\n    stdout: str | None = Field(default=None, description=\"Standard output from execution (if applicable)\")\n    stderr: str | None = Field(default=None, description=\"Standard error from execution (if applicable)\")\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.GenericMetadata","title":"<code>GenericMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Free-form metadata for generic artifacts.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class GenericMetadata(BaseModel):\n    \"\"\"Free-form metadata for generic artifacts.\"\"\"\n\n    model_config = {\"extra\": \"allow\"}\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.MLTrainingWorkspaceArtifactData","title":"<code>MLTrainingWorkspaceArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[MLMetadata]</code></p> <p>Schema for ML training workspace artifact data.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLTrainingWorkspaceArtifactData(BaseArtifactData[MLMetadata]):\n    \"\"\"Schema for ML training workspace artifact data.\"\"\"\n\n    type: Literal[\"ml_training_workspace\"] = Field(default=\"ml_training_workspace\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: MLMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.MLPredictionArtifactData","title":"<code>MLPredictionArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[MLMetadata]</code></p> <p>Schema for ML prediction artifact data with results.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLPredictionArtifactData(BaseArtifactData[MLMetadata]):\n    \"\"\"Schema for ML prediction artifact data with results.\"\"\"\n\n    type: Literal[\"ml_prediction\"] = Field(default=\"ml_prediction\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: MLMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.MLPredictionWorkspaceArtifactData","title":"<code>MLPredictionWorkspaceArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[MLMetadata]</code></p> <p>Schema for ML prediction workspace artifact data (debug/inspection).</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLPredictionWorkspaceArtifactData(BaseArtifactData[MLMetadata]):\n    \"\"\"Schema for ML prediction workspace artifact data (debug/inspection).\"\"\"\n\n    type: Literal[\"ml_prediction_workspace\"] = Field(default=\"ml_prediction_workspace\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: MLMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas.GenericArtifactData","title":"<code>GenericArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[GenericMetadata]</code></p> <p>Schema for generic artifact data with free-form metadata.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class GenericArtifactData(BaseArtifactData[GenericMetadata]):\n    \"\"\"Schema for generic artifact data with free-form metadata.\"\"\"\n\n    type: Literal[\"generic\"] = Field(default=\"generic\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: GenericMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.schemas-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.schemas.validate_artifact_data","title":"<code>validate_artifact_data(data)</code>","text":"<p>Validate artifact data against appropriate schema based on type field.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>def validate_artifact_data(data: dict[str, Any]) -&gt; BaseArtifactData:\n    \"\"\"Validate artifact data against appropriate schema based on type field.\"\"\"\n    artifact_type = data.get(\"type\", \"generic\")\n\n    schema_map: dict[str, type[BaseArtifactData]] = {\n        \"ml_training_workspace\": MLTrainingWorkspaceArtifactData,\n        \"ml_prediction\": MLPredictionArtifactData,\n        \"ml_prediction_workspace\": MLPredictionWorkspaceArtifactData,\n        \"generic\": GenericArtifactData,\n    }\n\n    schema = schema_map.get(artifact_type, GenericArtifactData)\n    return schema.model_validate(data)\n</code></pre>"},{"location":"api-reference/#repository","title":"Repository","text":""},{"location":"api-reference/#chapkit.artifact.repository","title":"<code>repository</code>","text":"<p>Artifact repository for hierarchical data access with tree traversal.</p>"},{"location":"api-reference/#chapkit.artifact.repository-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository","title":"<code>ArtifactRepository</code>","text":"<p>               Bases: <code>BaseRepository[Artifact, ULID]</code></p> <p>Repository for Artifact entities with tree traversal operations.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>class ArtifactRepository(BaseRepository[Artifact, ULID]):\n    \"\"\"Repository for Artifact entities with tree traversal operations.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initialize artifact repository with database session.\"\"\"\n        super().__init__(session, Artifact)\n\n    async def find_by_id(self, id: ULID) -&gt; Artifact | None:\n        \"\"\"Find an artifact by ID with children eagerly loaded.\"\"\"\n        return await self.s.get(self.model, id, options=[selectinload(self.model.children)])\n\n    async def find_subtree(self, start_id: ULID) -&gt; Iterable[Artifact]:\n        \"\"\"Find all artifacts in the subtree rooted at the given ID using recursive CTE.\"\"\"\n        cte = select(self.model.id).where(self.model.id == start_id).cte(name=\"descendants\", recursive=True)\n        cte = cte.union_all(select(self.model.id).where(self.model.parent_id == cte.c.id))\n\n        subtree_ids = (await self.s.scalars(select(cte.c.id))).all()\n        rows = (await self.s.scalars(select(self.model).where(self.model.id.in_(subtree_ids)))).all()\n        return rows\n\n    async def get_root_artifact(self, artifact_id: ULID) -&gt; Artifact | None:\n        \"\"\"Find the root artifact by traversing up the parent chain.\"\"\"\n        artifact = await self.s.get(self.model, artifact_id)\n        if artifact is None:\n            return None\n\n        while artifact.parent_id is not None:\n            parent = await self.s.get(self.model, artifact.parent_id)\n            if parent is None:\n                break\n            artifact = parent\n\n        return artifact\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize artifact repository with database session.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initialize artifact repository with database session.\"\"\"\n    super().__init__(session, Artifact)\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.find_by_id","title":"<code>find_by_id(id)</code>  <code>async</code>","text":"<p>Find an artifact by ID with children eagerly loaded.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def find_by_id(self, id: ULID) -&gt; Artifact | None:\n    \"\"\"Find an artifact by ID with children eagerly loaded.\"\"\"\n    return await self.s.get(self.model, id, options=[selectinload(self.model.children)])\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.find_subtree","title":"<code>find_subtree(start_id)</code>  <code>async</code>","text":"<p>Find all artifacts in the subtree rooted at the given ID using recursive CTE.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def find_subtree(self, start_id: ULID) -&gt; Iterable[Artifact]:\n    \"\"\"Find all artifacts in the subtree rooted at the given ID using recursive CTE.\"\"\"\n    cte = select(self.model.id).where(self.model.id == start_id).cte(name=\"descendants\", recursive=True)\n    cte = cte.union_all(select(self.model.id).where(self.model.parent_id == cte.c.id))\n\n    subtree_ids = (await self.s.scalars(select(cte.c.id))).all()\n    rows = (await self.s.scalars(select(self.model).where(self.model.id.in_(subtree_ids)))).all()\n    return rows\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.repository.ArtifactRepository.get_root_artifact","title":"<code>get_root_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Find the root artifact by traversing up the parent chain.</p> Source code in <code>src/chapkit/artifact/repository.py</code> <pre><code>async def get_root_artifact(self, artifact_id: ULID) -&gt; Artifact | None:\n    \"\"\"Find the root artifact by traversing up the parent chain.\"\"\"\n    artifact = await self.s.get(self.model, artifact_id)\n    if artifact is None:\n        return None\n\n    while artifact.parent_id is not None:\n        parent = await self.s.get(self.model, artifact.parent_id)\n        if parent is None:\n            break\n        artifact = parent\n\n    return artifact\n</code></pre>"},{"location":"api-reference/#manager","title":"Manager","text":""},{"location":"api-reference/#chapkit.artifact.manager","title":"<code>manager</code>","text":"<p>Artifact manager for hierarchical data with parent-child relationships.</p>"},{"location":"api-reference/#chapkit.artifact.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager","title":"<code>ArtifactManager</code>","text":"<p>               Bases: <code>BaseManager[Artifact, ArtifactIn, ArtifactOut, ULID]</code></p> <p>Manager for Artifact entities with hierarchical tree operations.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>class ArtifactManager(BaseManager[Artifact, ArtifactIn, ArtifactOut, ULID]):\n    \"\"\"Manager for Artifact entities with hierarchical tree operations.\"\"\"\n\n    def __init__(\n        self,\n        repo: ArtifactRepository,\n        hierarchy: ArtifactHierarchy | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize artifact manager with repository and optional hierarchy.\"\"\"\n        super().__init__(repo, Artifact, ArtifactOut)\n        self.repository: ArtifactRepository = repo\n        self.hierarchy = hierarchy\n\n    # Public API ------------------------------------------------------\n\n    async def find_subtree(self, start_id: ULID) -&gt; list[ArtifactTreeNode]:\n        \"\"\"Find all artifacts in the subtree rooted at the given ID.\"\"\"\n        artifacts = await self.repository.find_subtree(start_id)\n        return [self._to_tree_node(artifact) for artifact in artifacts]\n\n    async def expand_artifact(self, artifact_id: ULID) -&gt; ArtifactTreeNode | None:\n        \"\"\"Expand a single artifact with hierarchy metadata but without children.\"\"\"\n        artifact = await self.repository.find_by_id(artifact_id)\n        if artifact is None:\n            return None\n\n        node = self._to_tree_node(artifact)\n        node.children = None\n\n        return node\n\n    async def build_tree(self, start_id: ULID) -&gt; ArtifactTreeNode | None:\n        \"\"\"Build a hierarchical tree structure rooted at the given artifact ID.\"\"\"\n        artifacts = await self.find_subtree(start_id)\n        if not artifacts:\n            return None\n\n        node_map: dict[ULID, ArtifactTreeNode] = {}\n        for node in artifacts:\n            node.children = []\n            node_map[node.id] = node\n\n        for node in artifacts:\n            if node.parent_id is None:\n                continue\n            parent = node_map.get(node.parent_id)\n            if parent is None:\n                continue\n            if parent.children is None:\n                parent.children = []\n            parent.children.append(node)\n\n        # Keep children as [] for leaf nodes (semantic: \"loaded but empty\")\n        # Only expand_artifact sets children=None (semantic: \"not loaded\")\n\n        root = node_map.get(start_id)\n\n        return root\n\n    # Lifecycle overrides --------------------------------------------\n\n    def _should_assign_field(self, field: str, value: object) -&gt; bool:\n        \"\"\"Prevent assigning None to level field during updates.\"\"\"\n        if field == \"level\" and value is None:\n            return False\n        return super()._should_assign_field(field, value)\n\n    async def pre_save(self, entity: Artifact, data: ArtifactIn) -&gt; None:\n        \"\"\"Compute and set artifact level before saving.\"\"\"\n        entity.level = await self._compute_level(entity.parent_id)\n\n    async def pre_update(self, entity: Artifact, data: ArtifactIn, old_values: dict[str, object]) -&gt; None:\n        \"\"\"Recalculate artifact level and cascade updates to descendants if parent changed.\"\"\"\n        previous_level = old_values.get(\"level\", entity.level)\n        entity.level = await self._compute_level(entity.parent_id)\n        parent_changed = old_values.get(\"parent_id\") != entity.parent_id\n        if parent_changed or previous_level != entity.level:\n            await self._recalculate_descendants(entity)\n\n    # Helper utilities ------------------------------------------------\n\n    async def _compute_level(self, parent_id: ULID | None) -&gt; int:\n        \"\"\"Compute the level of an artifact based on its parent.\"\"\"\n        if parent_id is None:\n            return 0\n        parent = await self.repository.find_by_id(parent_id)\n        if parent is None:\n            return 0  # pragma: no cover\n        return parent.level + 1\n\n    async def _recalculate_descendants(self, entity: Artifact) -&gt; None:\n        \"\"\"Recalculate levels for all descendants of an artifact.\"\"\"\n        subtree = await self.repository.find_subtree(entity.id)\n        by_parent: dict[ULID | None, list[Artifact]] = {}\n        for node in subtree:\n            by_parent.setdefault(node.parent_id, []).append(node)\n\n        queue: deque[Artifact] = deque([entity])\n        while queue:\n            current = queue.popleft()\n            for child in by_parent.get(current.id, []):\n                child.level = current.level + 1\n                queue.append(child)\n\n    def _to_tree_node(self, entity: Artifact) -&gt; ArtifactTreeNode:\n        \"\"\"Convert artifact entity to tree node with hierarchy metadata.\"\"\"\n        base = super()._to_output_schema(entity)\n        node = ArtifactTreeNode.from_artifact(base)\n        if self.hierarchy is not None:\n            meta = self.hierarchy.describe(node.level)\n            hierarchy_value = meta.get(self.hierarchy.hierarchy_key)\n            if hierarchy_value is not None:\n                node.hierarchy = str(hierarchy_value)\n            label_value = meta.get(self.hierarchy.label_key)\n            if label_value is not None:\n                node.level_label = str(label_value)\n\n        return node\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.__init__","title":"<code>__init__(repo, hierarchy=None)</code>","text":"<p>Initialize artifact manager with repository and optional hierarchy.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>def __init__(\n    self,\n    repo: ArtifactRepository,\n    hierarchy: ArtifactHierarchy | None = None,\n) -&gt; None:\n    \"\"\"Initialize artifact manager with repository and optional hierarchy.\"\"\"\n    super().__init__(repo, Artifact, ArtifactOut)\n    self.repository: ArtifactRepository = repo\n    self.hierarchy = hierarchy\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.find_subtree","title":"<code>find_subtree(start_id)</code>  <code>async</code>","text":"<p>Find all artifacts in the subtree rooted at the given ID.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def find_subtree(self, start_id: ULID) -&gt; list[ArtifactTreeNode]:\n    \"\"\"Find all artifacts in the subtree rooted at the given ID.\"\"\"\n    artifacts = await self.repository.find_subtree(start_id)\n    return [self._to_tree_node(artifact) for artifact in artifacts]\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.expand_artifact","title":"<code>expand_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Expand a single artifact with hierarchy metadata but without children.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def expand_artifact(self, artifact_id: ULID) -&gt; ArtifactTreeNode | None:\n    \"\"\"Expand a single artifact with hierarchy metadata but without children.\"\"\"\n    artifact = await self.repository.find_by_id(artifact_id)\n    if artifact is None:\n        return None\n\n    node = self._to_tree_node(artifact)\n    node.children = None\n\n    return node\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.build_tree","title":"<code>build_tree(start_id)</code>  <code>async</code>","text":"<p>Build a hierarchical tree structure rooted at the given artifact ID.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def build_tree(self, start_id: ULID) -&gt; ArtifactTreeNode | None:\n    \"\"\"Build a hierarchical tree structure rooted at the given artifact ID.\"\"\"\n    artifacts = await self.find_subtree(start_id)\n    if not artifacts:\n        return None\n\n    node_map: dict[ULID, ArtifactTreeNode] = {}\n    for node in artifacts:\n        node.children = []\n        node_map[node.id] = node\n\n    for node in artifacts:\n        if node.parent_id is None:\n            continue\n        parent = node_map.get(node.parent_id)\n        if parent is None:\n            continue\n        if parent.children is None:\n            parent.children = []\n        parent.children.append(node)\n\n    # Keep children as [] for leaf nodes (semantic: \"loaded but empty\")\n    # Only expand_artifact sets children=None (semantic: \"not loaded\")\n\n    root = node_map.get(start_id)\n\n    return root\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.pre_save","title":"<code>pre_save(entity, data)</code>  <code>async</code>","text":"<p>Compute and set artifact level before saving.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def pre_save(self, entity: Artifact, data: ArtifactIn) -&gt; None:\n    \"\"\"Compute and set artifact level before saving.\"\"\"\n    entity.level = await self._compute_level(entity.parent_id)\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.manager.ArtifactManager.pre_update","title":"<code>pre_update(entity, data, old_values)</code>  <code>async</code>","text":"<p>Recalculate artifact level and cascade updates to descendants if parent changed.</p> Source code in <code>src/chapkit/artifact/manager.py</code> <pre><code>async def pre_update(self, entity: Artifact, data: ArtifactIn, old_values: dict[str, object]) -&gt; None:\n    \"\"\"Recalculate artifact level and cascade updates to descendants if parent changed.\"\"\"\n    previous_level = old_values.get(\"level\", entity.level)\n    entity.level = await self._compute_level(entity.parent_id)\n    parent_changed = old_values.get(\"parent_id\") != entity.parent_id\n    if parent_changed or previous_level != entity.level:\n        await self._recalculate_descendants(entity)\n</code></pre>"},{"location":"api-reference/#router","title":"Router","text":""},{"location":"api-reference/#chapkit.artifact.router","title":"<code>router</code>","text":"<p>Artifact CRUD router with hierarchical tree operations.</p>"},{"location":"api-reference/#chapkit.artifact.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter","title":"<code>ArtifactRouter</code>","text":"<p>               Bases: <code>CrudRouter[ArtifactIn, ArtifactOut]</code></p> <p>CRUD router for Artifact entities with tree operations.</p> Source code in <code>src/chapkit/artifact/router.py</code> <pre><code>class ArtifactRouter(CrudRouter[ArtifactIn, ArtifactOut]):\n    \"\"\"CRUD router for Artifact entities with tree operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        manager_factory: Any,\n        entity_in_type: type[ArtifactIn],\n        entity_out_type: type[ArtifactOut],\n        permissions: CrudPermissions | None = None,\n        enable_config_access: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize artifact router with entity types and manager factory.\"\"\"\n        # Store enable_config_access to conditionally register config endpoint\n        self.enable_config_access = enable_config_access\n\n        super().__init__(\n            prefix=prefix,\n            tags=list(tags),\n            entity_in_type=entity_in_type,\n            entity_out_type=entity_out_type,\n            manager_factory=manager_factory,\n            permissions=permissions,\n            **kwargs,\n        )\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register artifact CRUD routes and tree operations.\"\"\"\n        super()._register_routes()\n\n        manager_factory = self.manager_factory\n\n        async def expand_artifact(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; ArtifactTreeNode:\n            ulid_id = self._parse_ulid(entity_id)\n\n            expanded = await manager.expand_artifact(ulid_id)\n            if expanded is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n            return expanded\n\n        async def build_tree(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; ArtifactTreeNode:\n            ulid_id = self._parse_ulid(entity_id)\n\n            tree = await manager.build_tree(ulid_id)\n            if tree is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n            return tree\n\n        self.register_entity_operation(\n            \"expand\",\n            expand_artifact,\n            response_model=ArtifactTreeNode,\n            summary=\"Expand artifact\",\n            description=\"Get artifact with hierarchy metadata but without children\",\n        )\n\n        self.register_entity_operation(\n            \"tree\",\n            build_tree,\n            response_model=ArtifactTreeNode,\n            summary=\"Build artifact tree\",\n            description=\"Build hierarchical tree structure rooted at the given artifact\",\n        )\n\n        # Conditionally register config access endpoint\n        if self.enable_config_access:\n            from ..api.dependencies import get_config_manager\n            from ..config.manager import ConfigManager\n\n            async def get_config(\n                entity_id: str,\n                artifact_manager: ArtifactManager = Depends(manager_factory),\n                config_manager: ConfigManager[BaseConfig] = Depends(get_config_manager),\n            ) -&gt; ConfigOut[BaseConfig]:\n                \"\"\"Get the config linked to this artifact.\"\"\"\n                ulid_id = self._parse_ulid(entity_id)\n\n                # Get config by traversing to root artifact\n                config = await config_manager.get_config_for_artifact(\n                    artifact_id=ulid_id, artifact_repo=artifact_manager.repository\n                )\n\n                if config is None:\n                    raise HTTPException(\n                        status_code=status.HTTP_404_NOT_FOUND,\n                        detail=f\"No config linked to artifact {entity_id}\",\n                    )\n\n                return config\n\n            self.register_entity_operation(\n                \"config\",\n                get_config,\n                response_model=ConfigOut[BaseConfig],\n                summary=\"Get artifact config\",\n                description=\"Get configuration linked to this artifact by traversing to root\",\n            )\n\n        # Download endpoint\n        async def download_artifact(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; Response:\n            \"\"\"Download artifact content as binary file.\"\"\"\n            ulid_id = self._parse_ulid(entity_id)\n\n            artifact = await manager.find_by_id(ulid_id)\n            if artifact is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n\n            if not isinstance(artifact.data, dict):\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=\"Artifact has no downloadable content\",\n                )\n\n            content = artifact.data.get(\"content\")\n            if content is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=\"Artifact has no content\",\n                )\n\n            content_type = artifact.data.get(\"content_type\", \"application/octet-stream\")\n\n            # Serialize content to bytes based on type\n            if isinstance(content, bytes):\n                # Most common case: ZIP files, PNG images, etc.\n                binary = content\n            elif isinstance(content, DataFrame):\n                # Serialize DataFrame based on content_type\n                if content_type == \"text/csv\":\n                    csv_string = content.to_csv()\n                    binary = csv_string.encode() if csv_string else b\"\"\n                else:\n                    # Default to JSON for all other types\n                    binary = content.to_json().encode()\n            elif isinstance(content, dict):\n                # DataFrame serialized to dict in database - reconstruct and serialize\n                if content_type == \"application/vnd.chapkit.dataframe+json\":\n                    df = DataFrame.model_validate(content)\n                    binary = df.to_json().encode()\n                else:\n                    # Generic dict content - serialize to JSON\n                    import json\n\n                    binary = json.dumps(content).encode()\n            else:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=f\"Cannot serialize content of type {type(content).__name__}\",\n                )\n\n            # Determine filename extension\n            extension_map = {\n                \"application/zip\": \"zip\",\n                \"text/csv\": \"csv\",\n                \"application/json\": \"json\",\n                \"application/vnd.chapkit.dataframe+json\": \"json\",\n                \"image/png\": \"png\",\n            }\n            ext = extension_map.get(content_type, \"bin\")\n\n            return Response(\n                content=binary,\n                media_type=content_type,\n                headers={\"Content-Disposition\": f\"attachment; filename=artifact_{entity_id}.{ext}\"},\n            )\n\n        # Metadata endpoint\n        async def get_artifact_metadata(\n            entity_id: str,\n            manager: ArtifactManager = Depends(manager_factory),\n        ) -&gt; dict[str, Any]:\n            \"\"\"Get only JSON-serializable metadata, excluding binary content.\"\"\"\n            ulid_id = self._parse_ulid(entity_id)\n\n            artifact = await manager.find_by_id(ulid_id)\n            if artifact is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Artifact with id {entity_id} not found\",\n                )\n\n            if not isinstance(artifact.data, dict):\n                return {}\n\n            return artifact.data.get(\"metadata\", {})\n\n        self.register_entity_operation(\n            \"download\",\n            download_artifact,\n            response_model=None,  # Raw Response, don't serialize\n            summary=\"Download artifact content\",\n            description=\"Download artifact content as binary file (ZIP, CSV, etc.)\",\n        )\n\n        self.register_entity_operation(\n            \"metadata\",\n            get_artifact_metadata,\n            response_model=dict[str, Any],\n            summary=\"Get artifact metadata\",\n            description=\"Get JSON-serializable metadata without binary content\",\n        )\n</code></pre>"},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.artifact.router.ArtifactRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, entity_in_type, entity_out_type, permissions=None, enable_config_access=False, **kwargs)</code>","text":"<p>Initialize artifact router with entity types and manager factory.</p> Source code in <code>src/chapkit/artifact/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    manager_factory: Any,\n    entity_in_type: type[ArtifactIn],\n    entity_out_type: type[ArtifactOut],\n    permissions: CrudPermissions | None = None,\n    enable_config_access: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize artifact router with entity types and manager factory.\"\"\"\n    # Store enable_config_access to conditionally register config endpoint\n    self.enable_config_access = enable_config_access\n\n    super().__init__(\n        prefix=prefix,\n        tags=list(tags),\n        entity_in_type=entity_in_type,\n        entity_out_type=entity_out_type,\n        manager_factory=manager_factory,\n        permissions=permissions,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-reference/#task-module","title":"Task Module","text":"<p>Registry-based task execution system for Python functions with dependency injection.</p>"},{"location":"api-reference/#registry","title":"Registry","text":""},{"location":"api-reference/#chapkit.task.registry","title":"<code>registry</code>","text":"<p>Global registry for Python task functions with metadata support.</p>"},{"location":"api-reference/#chapkit.task.registry-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.registry.TaskMetadata","title":"<code>TaskMetadata</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata for a registered task.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>class TaskMetadata(TypedDict):\n    \"\"\"Metadata for a registered task.\"\"\"\n\n    func: Callable[..., Any]\n    tags: list[str]\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry","title":"<code>TaskRegistry</code>","text":"<p>Global registry for Python task functions with tags and metadata.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>class TaskRegistry:\n    \"\"\"Global registry for Python task functions with tags and metadata.\"\"\"\n\n    _registry: dict[str, TaskMetadata] = {}\n\n    @classmethod\n    def register(\n        cls,\n        name: str,\n        tags: list[str] | None = None,\n    ) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n        \"\"\"Decorator to register a task function with optional tags.\"\"\"\n        # Validate URL-safe name\n        if not re.match(r\"^[a-zA-Z0-9_-]+$\", name):\n            raise ValueError(f\"Task name '{name}' must be URL-safe (alphanumeric, underscore, hyphen only)\")\n\n        def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n            if name in cls._registry:\n                raise ValueError(f\"Task '{name}' already registered\")\n            cls._registry[name] = {\n                \"func\": func,\n                \"tags\": tags or [],\n            }\n            return func\n\n        return decorator\n\n    @classmethod\n    def register_function(\n        cls,\n        name: str,\n        func: Callable[..., Any],\n        tags: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Imperatively register a task function with optional tags.\"\"\"\n        # Validate URL-safe name\n        if not re.match(r\"^[a-zA-Z0-9_-]+$\", name):\n            raise ValueError(f\"Task name '{name}' must be URL-safe (alphanumeric, underscore, hyphen only)\")\n\n        if name in cls._registry:\n            raise ValueError(f\"Task '{name}' already registered\")\n        cls._registry[name] = {\n            \"func\": func,\n            \"tags\": tags or [],\n        }\n\n    @classmethod\n    def get(cls, name: str) -&gt; Callable[..., Any]:\n        \"\"\"Retrieve a registered task function.\"\"\"\n        if name not in cls._registry:\n            raise KeyError(f\"Task '{name}' not found in registry\")\n        return cls._registry[name][\"func\"]\n\n    @classmethod\n    def has(cls, name: str) -&gt; bool:\n        \"\"\"Check if a task is registered.\"\"\"\n        return name in cls._registry\n\n    @classmethod\n    def get_tags(cls, name: str) -&gt; list[str]:\n        \"\"\"Get tags for a registered task.\"\"\"\n        if name not in cls._registry:\n            raise KeyError(f\"Task '{name}' not found in registry\")\n        return cls._registry[name][\"tags\"]\n\n    @classmethod\n    def get_info(cls, name: str) -&gt; \"TaskInfo\":\n        \"\"\"Get metadata for a registered task.\"\"\"\n        from .schemas import ParameterInfo, TaskInfo\n\n        if name not in cls._registry:\n            raise KeyError(f\"Task '{name}' not found in registry\")\n\n        metadata = cls._registry[name]\n        func = metadata[\"func\"]\n        sig = inspect.signature(func)\n\n        # Extract parameter info\n        parameters = []\n        for param_name, param in sig.parameters.items():\n            if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n                continue\n            parameters.append(\n                ParameterInfo(\n                    name=param_name,\n                    annotation=str(param.annotation) if param.annotation != param.empty else None,\n                    default=str(param.default) if param.default != param.empty else None,\n                    required=param.default == param.empty,\n                )\n            )\n\n        return TaskInfo(\n            name=name,\n            docstring=inspect.getdoc(func),\n            signature=str(sig),\n            parameters=parameters,\n            tags=metadata[\"tags\"],\n        )\n\n    @classmethod\n    def list_all(cls) -&gt; list[str]:\n        \"\"\"List all registered task names.\"\"\"\n        return sorted(cls._registry.keys())\n\n    @classmethod\n    def list_all_info(cls) -&gt; list[\"TaskInfo\"]:\n        \"\"\"List metadata for all registered tasks.\"\"\"\n        return [cls.get_info(name) for name in cls.list_all()]\n\n    @classmethod\n    def list_by_tags(cls, tags: list[str]) -&gt; list[str]:\n        \"\"\"List task names that have ALL specified tags.\"\"\"\n        if not tags:\n            return cls.list_all()\n\n        matching_tasks = []\n        for name, metadata in cls._registry.items():\n            task_tags = set(metadata[\"tags\"])\n            if all(tag in task_tags for tag in tags):\n                matching_tasks.append(name)\n\n        return sorted(matching_tasks)\n\n    @classmethod\n    def clear(cls) -&gt; None:\n        \"\"\"Clear all registered tasks (useful for testing and hot-reload).\"\"\"\n        cls._registry.clear()\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.register","title":"<code>register(name, tags=None)</code>  <code>classmethod</code>","text":"<p>Decorator to register a task function with optional tags.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef register(\n    cls,\n    name: str,\n    tags: list[str] | None = None,\n) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Decorator to register a task function with optional tags.\"\"\"\n    # Validate URL-safe name\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", name):\n        raise ValueError(f\"Task name '{name}' must be URL-safe (alphanumeric, underscore, hyphen only)\")\n\n    def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n        if name in cls._registry:\n            raise ValueError(f\"Task '{name}' already registered\")\n        cls._registry[name] = {\n            \"func\": func,\n            \"tags\": tags or [],\n        }\n        return func\n\n    return decorator\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.register_function","title":"<code>register_function(name, func, tags=None)</code>  <code>classmethod</code>","text":"<p>Imperatively register a task function with optional tags.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef register_function(\n    cls,\n    name: str,\n    func: Callable[..., Any],\n    tags: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Imperatively register a task function with optional tags.\"\"\"\n    # Validate URL-safe name\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", name):\n        raise ValueError(f\"Task name '{name}' must be URL-safe (alphanumeric, underscore, hyphen only)\")\n\n    if name in cls._registry:\n        raise ValueError(f\"Task '{name}' already registered\")\n    cls._registry[name] = {\n        \"func\": func,\n        \"tags\": tags or [],\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Retrieve a registered task function.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; Callable[..., Any]:\n    \"\"\"Retrieve a registered task function.\"\"\"\n    if name not in cls._registry:\n        raise KeyError(f\"Task '{name}' not found in registry\")\n    return cls._registry[name][\"func\"]\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.has","title":"<code>has(name)</code>  <code>classmethod</code>","text":"<p>Check if a task is registered.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef has(cls, name: str) -&gt; bool:\n    \"\"\"Check if a task is registered.\"\"\"\n    return name in cls._registry\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.get_tags","title":"<code>get_tags(name)</code>  <code>classmethod</code>","text":"<p>Get tags for a registered task.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef get_tags(cls, name: str) -&gt; list[str]:\n    \"\"\"Get tags for a registered task.\"\"\"\n    if name not in cls._registry:\n        raise KeyError(f\"Task '{name}' not found in registry\")\n    return cls._registry[name][\"tags\"]\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.get_info","title":"<code>get_info(name)</code>  <code>classmethod</code>","text":"<p>Get metadata for a registered task.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef get_info(cls, name: str) -&gt; \"TaskInfo\":\n    \"\"\"Get metadata for a registered task.\"\"\"\n    from .schemas import ParameterInfo, TaskInfo\n\n    if name not in cls._registry:\n        raise KeyError(f\"Task '{name}' not found in registry\")\n\n    metadata = cls._registry[name]\n    func = metadata[\"func\"]\n    sig = inspect.signature(func)\n\n    # Extract parameter info\n    parameters = []\n    for param_name, param in sig.parameters.items():\n        if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n            continue\n        parameters.append(\n            ParameterInfo(\n                name=param_name,\n                annotation=str(param.annotation) if param.annotation != param.empty else None,\n                default=str(param.default) if param.default != param.empty else None,\n                required=param.default == param.empty,\n            )\n        )\n\n    return TaskInfo(\n        name=name,\n        docstring=inspect.getdoc(func),\n        signature=str(sig),\n        parameters=parameters,\n        tags=metadata[\"tags\"],\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.list_all","title":"<code>list_all()</code>  <code>classmethod</code>","text":"<p>List all registered task names.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef list_all(cls) -&gt; list[str]:\n    \"\"\"List all registered task names.\"\"\"\n    return sorted(cls._registry.keys())\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.list_all_info","title":"<code>list_all_info()</code>  <code>classmethod</code>","text":"<p>List metadata for all registered tasks.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef list_all_info(cls) -&gt; list[\"TaskInfo\"]:\n    \"\"\"List metadata for all registered tasks.\"\"\"\n    return [cls.get_info(name) for name in cls.list_all()]\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.list_by_tags","title":"<code>list_by_tags(tags)</code>  <code>classmethod</code>","text":"<p>List task names that have ALL specified tags.</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef list_by_tags(cls, tags: list[str]) -&gt; list[str]:\n    \"\"\"List task names that have ALL specified tags.\"\"\"\n    if not tags:\n        return cls.list_all()\n\n    matching_tasks = []\n    for name, metadata in cls._registry.items():\n        task_tags = set(metadata[\"tags\"])\n        if all(tag in task_tags for tag in tags):\n            matching_tasks.append(name)\n\n    return sorted(matching_tasks)\n</code></pre>"},{"location":"api-reference/#chapkit.task.registry.TaskRegistry.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clear all registered tasks (useful for testing and hot-reload).</p> Source code in <code>src/chapkit/task/registry.py</code> <pre><code>@classmethod\ndef clear(cls) -&gt; None:\n    \"\"\"Clear all registered tasks (useful for testing and hot-reload).\"\"\"\n    cls._registry.clear()\n</code></pre>"},{"location":"api-reference/#executor","title":"Executor","text":""},{"location":"api-reference/#chapkit.task.executor","title":"<code>executor</code>","text":"<p>Task executor for registry-based execution with dependency injection.</p>"},{"location":"api-reference/#chapkit.task.executor-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.executor.TaskExecutor","title":"<code>TaskExecutor</code>","text":"<p>Executes registered task functions with dependency injection.</p> Source code in <code>src/chapkit/task/executor.py</code> <pre><code>class TaskExecutor:\n    \"\"\"Executes registered task functions with dependency injection.\"\"\"\n\n    def __init__(\n        self,\n        database: Database,\n        scheduler: ChapkitScheduler | None = None,\n        artifact_manager: ArtifactManager | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize task executor with framework dependencies.\"\"\"\n        self.database = database\n        self.scheduler = scheduler\n        self.artifact_manager = artifact_manager\n\n    async def execute(self, name: str, params: dict[str, Any] | None = None) -&gt; Any:\n        \"\"\"Execute registered function by name with runtime parameters and return result.\"\"\"\n        # Verify function exists\n        if not TaskRegistry.has(name):\n            raise ValueError(f\"Task '{name}' not found in registry\")\n\n        # Get function from registry\n        func = TaskRegistry.get(name)\n\n        # Create a database session for potential injection\n        async with self.database.session() as session:\n            # Inject framework dependencies based on function signature\n            final_params = self._inject_parameters(func, params or {}, session)\n\n            # Handle sync/async functions\n            if inspect.iscoroutinefunction(func):\n                result = await func(**final_params)\n            else:\n                result = await asyncio.to_thread(func, **final_params)\n\n        return result\n\n    def _is_injectable_type(self, param_type: type | None) -&gt; bool:\n        \"\"\"Check if a parameter type should be injected by the framework.\"\"\"\n        if param_type is None:\n            return False\n\n        # Handle Optional[Type] -&gt; extract the non-None type\n        origin = get_origin(param_type)\n        if origin is types.UnionType or origin is Union:\n            args = getattr(param_type, \"__args__\", ())\n            non_none_types = [arg for arg in args if arg is not type(None)]\n            if len(non_none_types) == 1:\n                param_type = non_none_types[0]\n\n        return param_type in INJECTABLE_TYPES\n\n    def _build_injection_map(self, session: AsyncSession | None) -&gt; dict[type, Any]:\n        \"\"\"Build map of injectable types to their instances.\"\"\"\n        injection_map: dict[type, Any] = {\n            AsyncSession: session,\n            Database: self.database,\n        }\n        # Add optional dependencies if available\n        if self.scheduler is not None:\n            injection_map[ChapkitScheduler] = self.scheduler\n        if self.artifact_manager is not None:\n            injection_map[ArtifactManager] = self.artifact_manager\n        return injection_map\n\n    def _inject_parameters(\n        self,\n        func: Any,\n        user_params: dict[str, Any],\n        session: AsyncSession | None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Merge user parameters with framework injections based on function signature.\"\"\"\n        sig = inspect.signature(func)\n        type_hints = get_type_hints(func)\n\n        # Build injection map\n        injection_map = self._build_injection_map(session)\n\n        # Start with user parameters\n        final_params = dict(user_params)\n\n        # Inspect each parameter in function signature\n        for param_name, param in sig.parameters.items():\n            # Skip self, *args, **kwargs\n            if param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD):\n                continue\n\n            # Get type hint for this parameter\n            param_type = type_hints.get(param_name)\n\n            # Check if this type should be injected\n            if self._is_injectable_type(param_type):\n                # Get the actual type (handle Optional)\n                actual_type = param_type\n                origin = get_origin(param_type)\n                if origin is types.UnionType or origin is Union:\n                    args = getattr(param_type, \"__args__\", ())\n                    non_none_types = [arg for arg in args if arg is not type(None)]\n                    if non_none_types:\n                        actual_type = non_none_types[0]\n\n                # Inject if we have an instance of this type\n                if actual_type in injection_map:\n                    injectable_value = injection_map[actual_type]\n                    # For required parameters, inject even if None\n                    # For optional parameters, only inject if not None\n                    if param.default is param.empty:\n                        # Required parameter - inject whatever we have (even None)\n                        final_params[param_name] = injectable_value\n                    elif injectable_value is not None:\n                        # Optional parameter - only inject if we have a value\n                        final_params[param_name] = injectable_value\n                continue\n\n            # Not injectable - must come from user parameters\n            if param_name not in final_params:\n                # Check if parameter has a default value\n                if param.default is not param.empty:\n                    continue  # Will use default\n\n                # Required parameter missing\n                raise ValueError(\n                    f\"Missing required parameter '{param_name}' for task '{func.__name__}'. \"\n                    f\"Parameter is not injectable and not provided in params.\"\n                )\n\n        return final_params\n</code></pre>"},{"location":"api-reference/#chapkit.task.executor.TaskExecutor-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.executor.TaskExecutor.__init__","title":"<code>__init__(database, scheduler=None, artifact_manager=None)</code>","text":"<p>Initialize task executor with framework dependencies.</p> Source code in <code>src/chapkit/task/executor.py</code> <pre><code>def __init__(\n    self,\n    database: Database,\n    scheduler: ChapkitScheduler | None = None,\n    artifact_manager: ArtifactManager | None = None,\n) -&gt; None:\n    \"\"\"Initialize task executor with framework dependencies.\"\"\"\n    self.database = database\n    self.scheduler = scheduler\n    self.artifact_manager = artifact_manager\n</code></pre>"},{"location":"api-reference/#chapkit.task.executor.TaskExecutor.execute","title":"<code>execute(name, params=None)</code>  <code>async</code>","text":"<p>Execute registered function by name with runtime parameters and return result.</p> Source code in <code>src/chapkit/task/executor.py</code> <pre><code>async def execute(self, name: str, params: dict[str, Any] | None = None) -&gt; Any:\n    \"\"\"Execute registered function by name with runtime parameters and return result.\"\"\"\n    # Verify function exists\n    if not TaskRegistry.has(name):\n        raise ValueError(f\"Task '{name}' not found in registry\")\n\n    # Get function from registry\n    func = TaskRegistry.get(name)\n\n    # Create a database session for potential injection\n    async with self.database.session() as session:\n        # Inject framework dependencies based on function signature\n        final_params = self._inject_parameters(func, params or {}, session)\n\n        # Handle sync/async functions\n        if inspect.iscoroutinefunction(func):\n            result = await func(**final_params)\n        else:\n            result = await asyncio.to_thread(func, **final_params)\n\n    return result\n</code></pre>"},{"location":"api-reference/#router_1","title":"Router","text":""},{"location":"api-reference/#chapkit.task.router","title":"<code>router</code>","text":"<p>Task router for registry-based execution.</p>"},{"location":"api-reference/#chapkit.task.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.router.TaskRouter","title":"<code>TaskRouter</code>","text":"<p>Router for task execution (registry-based, no CRUD).</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>class TaskRouter:\n    \"\"\"Router for task execution (registry-based, no CRUD).\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        executor_factory: Any,\n    ) -&gt; None:\n        \"\"\"Initialize task router with executor factory.\"\"\"\n        self.prefix = prefix\n        self.tags = tags\n        self.executor_factory = executor_factory\n        self.router = APIRouter(prefix=prefix, tags=list(tags))\n        self._register_routes()\n\n    @classmethod\n    def create(\n        cls,\n        prefix: str,\n        tags: Sequence[str],\n        executor_factory: Any,\n    ) -&gt; TaskRouter:\n        \"\"\"Create a task router with executor factory.\"\"\"\n        return cls(prefix=prefix, tags=tags, executor_factory=executor_factory)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register task routes.\"\"\"\n        executor_factory = self.executor_factory\n\n        @self.router.get(\"\", response_model=list[TaskInfo])\n        async def list_tasks() -&gt; list[TaskInfo]:\n            \"\"\"List all registered tasks.\"\"\"\n            return TaskRegistry.list_all_info()\n\n        @self.router.get(\"/{name}\", response_model=TaskInfo)\n        async def get_task(name: str) -&gt; TaskInfo:\n            \"\"\"Get task metadata by name.\"\"\"\n            try:\n                return TaskRegistry.get_info(name)\n            except KeyError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=str(e),\n                ) from e\n\n        @self.router.post(\"/{name}/$execute\", response_model=TaskExecuteResponse)\n        async def execute_task(\n            name: str,\n            request: TaskExecuteRequest = TaskExecuteRequest(),\n            executor: TaskExecutor = Depends(executor_factory),\n        ) -&gt; TaskExecuteResponse:\n            \"\"\"Execute task by name with runtime parameters and return result.\"\"\"\n            import traceback\n\n            # Check if task exists\n            if not TaskRegistry.has(name):\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f\"Task '{name}' not found in registry\",\n                )\n\n            params = request.params or {}\n\n            # Execute task and handle errors\n            try:\n                result = await executor.execute(name, params)\n                return TaskExecuteResponse(\n                    task_name=name,\n                    params=params,\n                    result=result,\n                    error=None,\n                )\n            except Exception as e:\n                # Return error in response (don't raise exception)\n                return TaskExecuteResponse(\n                    task_name=name,\n                    params=params,\n                    result=None,\n                    error={\n                        \"type\": type(e).__name__,\n                        \"message\": str(e),\n                        \"traceback\": traceback.format_exc(),\n                    },\n                )\n</code></pre>"},{"location":"api-reference/#chapkit.task.router.TaskRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.task.router.TaskRouter.__init__","title":"<code>__init__(prefix, tags, executor_factory)</code>","text":"<p>Initialize task router with executor factory.</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    executor_factory: Any,\n) -&gt; None:\n    \"\"\"Initialize task router with executor factory.\"\"\"\n    self.prefix = prefix\n    self.tags = tags\n    self.executor_factory = executor_factory\n    self.router = APIRouter(prefix=prefix, tags=list(tags))\n    self._register_routes()\n</code></pre>"},{"location":"api-reference/#chapkit.task.router.TaskRouter.create","title":"<code>create(prefix, tags, executor_factory)</code>  <code>classmethod</code>","text":"<p>Create a task router with executor factory.</p> Source code in <code>src/chapkit/task/router.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    prefix: str,\n    tags: Sequence[str],\n    executor_factory: Any,\n) -&gt; TaskRouter:\n    \"\"\"Create a task router with executor factory.\"\"\"\n    return cls(prefix=prefix, tags=tags, executor_factory=executor_factory)\n</code></pre>"},{"location":"api-reference/#schemas_1","title":"Schemas","text":""},{"location":"api-reference/#chapkit.task.schemas","title":"<code>schemas</code>","text":"<p>Task schemas for registry-based execution.</p>"},{"location":"api-reference/#chapkit.task.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.task.schemas.ParameterInfo","title":"<code>ParameterInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Function parameter metadata.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class ParameterInfo(BaseModel):\n    \"\"\"Function parameter metadata.\"\"\"\n\n    name: str = Field(description=\"Parameter name\")\n    annotation: str | None = Field(default=None, description=\"Type annotation as string\")\n    default: str | None = Field(default=None, description=\"Default value as string\")\n    required: bool = Field(description=\"Whether parameter is required\")\n</code></pre>"},{"location":"api-reference/#chapkit.task.schemas.TaskInfo","title":"<code>TaskInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Task metadata from registry.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class TaskInfo(BaseModel):\n    \"\"\"Task metadata from registry.\"\"\"\n\n    name: str = Field(description=\"Task name (URL-safe)\")\n    docstring: str | None = Field(default=None, description=\"Function docstring\")\n    signature: str = Field(description=\"Function signature\")\n    parameters: list[ParameterInfo] = Field(default_factory=list, description=\"Function parameters\")\n    tags: list[str] = Field(default_factory=list, description=\"Task tags for filtering\")\n</code></pre>"},{"location":"api-reference/#chapkit.task.schemas.TaskExecuteRequest","title":"<code>TaskExecuteRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to execute a task.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class TaskExecuteRequest(BaseModel):\n    \"\"\"Request to execute a task.\"\"\"\n\n    params: dict[str, Any] | None = Field(default=None, description=\"Runtime parameters for task execution\")\n</code></pre>"},{"location":"api-reference/#chapkit.task.schemas.TaskExecuteResponse","title":"<code>TaskExecuteResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response from task execution.</p> Source code in <code>src/chapkit/task/schemas.py</code> <pre><code>class TaskExecuteResponse(BaseModel):\n    \"\"\"Response from task execution.\"\"\"\n\n    task_name: str = Field(description=\"Name of the executed task\")\n    params: dict[str, Any] = Field(default_factory=dict, description=\"Parameters used for execution\")\n    result: Any = Field(description=\"Task execution result\")\n    error: dict[str, str] | None = Field(default=None, description=\"Error information if execution failed\")\n</code></pre>"},{"location":"api-reference/#data-module","title":"Data Module","text":"<p>Universal DataFrame interchange format for tabular data across pandas, polars, xarray, and other libraries.</p>"},{"location":"api-reference/#dataframe","title":"DataFrame","text":""},{"location":"api-reference/#chapkit.data.DataFrame","title":"<code>DataFrame</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Universal interchange format for tabular data from pandas, polars, xarray, and other libraries.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>class DataFrame(BaseModel):\n    \"\"\"Universal interchange format for tabular data from pandas, polars, xarray, and other libraries.\"\"\"\n\n    columns: list[str]\n    data: list[list[Any]]\n\n    @classmethod\n    def from_pandas(cls, df: Any) -&gt; Self:\n        \"\"\"Create schema from pandas DataFrame.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for from_pandas(). Install with: uv add pandas\") from None\n\n        if not isinstance(df, pd.DataFrame):\n            raise TypeError(f\"Expected pandas DataFrame, got {type(df)}\")\n\n        return cls(\n            columns=df.columns.tolist(),\n            data=df.values.tolist(),\n        )\n\n    @classmethod\n    def from_polars(cls, df: Any) -&gt; Self:\n        \"\"\"Create schema from Polars DataFrame.\"\"\"\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\"polars is required for from_polars(). Install with: uv add polars\") from None\n\n        if not isinstance(df, pl.DataFrame):\n            raise TypeError(f\"Expected Polars DataFrame, got {type(df)}\")\n\n        return cls(\n            columns=df.columns,\n            data=[list(row) for row in df.rows()],\n        )\n\n    @classmethod\n    def from_xarray(cls, da: Any) -&gt; Self:\n        \"\"\"Create schema from xarray DataArray (2D only).\"\"\"\n        try:\n            import xarray as xr\n        except ImportError:\n            raise ImportError(\"xarray is required for from_xarray(). Install with: uv add xarray\") from None\n\n        if not isinstance(da, xr.DataArray):\n            raise TypeError(f\"Expected xarray DataArray, got {type(da)}\")\n\n        if len(da.dims) != 2:\n            raise ValueError(f\"Only 2D DataArrays supported, got {len(da.dims)} dimensions\")\n\n        # Convert to pandas then use from_pandas\n        pdf = da.to_pandas()\n        return cls.from_pandas(pdf)\n\n    @classmethod\n    def from_dict(cls, data: dict[str, list[Any]]) -&gt; Self:\n        \"\"\"Create schema from dictionary of columns.\"\"\"\n        if not data:\n            return cls(columns=[], data=[])\n\n        columns = list(data.keys())\n        num_rows = len(next(iter(data.values())))\n\n        if not all(len(vals) == num_rows for vals in data.values()):\n            raise ValueError(\"All columns must have the same length\")\n\n        rows = [[data[col][i] for col in columns] for i in range(num_rows)]\n\n        return cls(columns=columns, data=rows)\n\n    @classmethod\n    def from_records(cls, records: list[dict[str, Any]]) -&gt; Self:\n        \"\"\"Create schema from list of records (row-oriented).\"\"\"\n        if not records:\n            return cls(columns=[], data=[])\n\n        columns = list(records[0].keys())\n        data = [[record[col] for col in columns] for record in records]\n\n        return cls(columns=columns, data=data)\n\n    @classmethod\n    def from_csv(\n        cls,\n        path: str | Path | None = None,\n        *,\n        csv_string: str | None = None,\n        delimiter: str = \",\",\n        has_header: bool = True,\n        encoding: str = \"utf-8\",\n        infer_types: bool = True,\n    ) -&gt; Self:\n        \"\"\"Create DataFrame from CSV file or string.\"\"\"\n        # Validate mutually exclusive parameters\n        if path is None and csv_string is None:\n            raise ValueError(\"Either path or csv_string must be provided\")\n        if path is not None and csv_string is not None:\n            raise ValueError(\"path and csv_string are mutually exclusive\")\n\n        # Read CSV data\n        if path is not None:\n            path_obj = Path(path)\n            if not path_obj.exists():\n                raise FileNotFoundError(f\"File not found: {path}\")\n            with path_obj.open(\"r\", encoding=encoding, newline=\"\") as f:\n                reader = csv.reader(f, delimiter=delimiter)\n                rows = list(reader)\n        else:\n            # csv_string is not None\n            string_io = io.StringIO(csv_string)\n            reader = csv.reader(string_io, delimiter=delimiter)\n            rows = list(reader)\n\n        # Handle empty CSV\n        if not rows:\n            return cls(columns=[], data=[])\n\n        # Extract columns and data\n        if has_header:\n            columns = rows[0]\n            data = rows[1:]\n        else:\n            # Generate column names\n            num_cols = len(rows[0]) if rows else 0\n            columns = [f\"col_{i}\" for i in range(num_cols)]\n            data = rows\n\n        # Apply type inference if enabled\n        if infer_types and data:\n            num_cols = len(columns)\n            converted_columns: list[list[Any]] = []\n\n            for col_idx in range(num_cols):\n                col_values = [row[col_idx] for row in data]\n                target_type = _infer_column_type(col_values)\n                converted_values = _convert_column(col_values, target_type)\n                converted_columns.append(converted_values)\n\n            data = [\n                [converted_columns[col_idx][row_idx] for col_idx in range(num_cols)] for row_idx in range(len(data))\n            ]\n\n        return cls(columns=columns, data=data)\n\n    def to_pandas(self) -&gt; Any:\n        \"\"\"Convert schema to pandas DataFrame.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"pandas is required for to_pandas(). Install with: uv add pandas\") from None\n\n        return pd.DataFrame(self.data, columns=self.columns)\n\n    def to_polars(self) -&gt; Any:\n        \"\"\"Convert schema to Polars DataFrame.\"\"\"\n        try:\n            import polars as pl\n        except ImportError:\n            raise ImportError(\"polars is required for to_polars(). Install with: uv add polars\") from None\n\n        return pl.DataFrame(self.data, schema=self.columns, orient=\"row\")\n\n    def to_dict(self, orient: Literal[\"dict\", \"list\", \"records\"] = \"dict\") -&gt; Any:\n        \"\"\"Convert schema to dictionary with specified orient (dict, list, or records).\"\"\"\n        if orient == \"dict\":\n            return {col: {i: self.data[i][j] for i in range(len(self.data))} for j, col in enumerate(self.columns)}\n        elif orient == \"list\":\n            return {col: [row[j] for row in self.data] for j, col in enumerate(self.columns)}\n        elif orient == \"records\":\n            return [{col: row[j] for j, col in enumerate(self.columns)} for row in self.data]\n        else:\n            raise ValueError(f\"Invalid orient: {orient}\")\n\n    def to_csv(\n        self,\n        path: str | Path | None = None,\n        *,\n        delimiter: str = \",\",\n        include_header: bool = True,\n        encoding: str = \"utf-8\",\n    ) -&gt; str | None:\n        \"\"\"Export DataFrame to CSV file or string.\"\"\"\n        # Write to string buffer or file\n        if path is None:\n            # Return as string\n            output = io.StringIO()\n            writer = csv.writer(output, delimiter=delimiter)\n\n            if include_header:\n                writer.writerow(self.columns)\n\n            writer.writerows(self.data)\n\n            return output.getvalue()\n        else:\n            # Write to file\n            path_obj = Path(path)\n            with path_obj.open(\"w\", encoding=encoding, newline=\"\") as f:\n                writer = csv.writer(f, delimiter=delimiter)\n\n                if include_header:\n                    writer.writerow(self.columns)\n\n                writer.writerows(self.data)\n\n            return None\n\n    # Convenience aliases\n    from_dataframe = from_pandas\n    to_dataframe = to_pandas\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Return tuple representing dimensionality of the DataFrame.\"\"\"\n        return (len(self.data), len(self.columns))\n\n    @property\n    def empty(self) -&gt; bool:\n        \"\"\"Indicator whether DataFrame is empty.\"\"\"\n        return len(self.data) == 0 or len(self.columns) == 0\n\n    @property\n    def size(self) -&gt; int:\n        \"\"\"Return int representing number of elements in this object.\"\"\"\n        return len(self.data) * len(self.columns)\n\n    @property\n    def ndim(self) -&gt; int:\n        \"\"\"Return int representing number of axes/array dimensions.\"\"\"\n        return 2\n\n    def head(self, n: int = 5) -&gt; Self:\n        \"\"\"Return first n rows.\"\"\"\n        if n &gt;= 0:\n            selected_data = self.data[:n]\n        else:\n            selected_data = self.data[:n] if n != 0 else self.data\n        return self.__class__(columns=self.columns, data=selected_data)\n\n    def tail(self, n: int = 5) -&gt; Self:\n        \"\"\"Return last n rows.\"\"\"\n        if n &gt;= 0:\n            selected_data = self.data[-n:] if n &gt; 0 else []\n        else:\n            selected_data = self.data[abs(n) :]\n        return self.__class__(columns=self.columns, data=selected_data)\n\n    def sample(\n        self,\n        n: int | None = None,\n        frac: float | None = None,\n        *,\n        random_state: int | None = None,\n    ) -&gt; Self:\n        \"\"\"Return random sample of rows.\"\"\"\n        # Validate parameters\n        if n is None and frac is None:\n            raise ValueError(\"Either n or frac must be provided\")\n        if n is not None and frac is not None:\n            raise ValueError(\"n and frac are mutually exclusive\")\n\n        # Set random seed if provided\n        if random_state is not None:\n            random.seed(random_state)\n\n        # Calculate sample size\n        total_rows = len(self.data)\n        if frac is not None:\n            if frac &gt; 1.0:\n                raise ValueError(\"frac must be &lt;= 1.0\")\n            sample_size = int(total_rows * frac)\n        else:\n            sample_size = min(n, total_rows) if n is not None else 0\n\n        # Sample indices\n        if sample_size &gt;= total_rows:\n            sampled_indices = list(range(total_rows))\n            random.shuffle(sampled_indices)\n        else:\n            sampled_indices = random.sample(range(total_rows), sample_size)\n\n        # Extract sampled rows\n        sampled_data = [self.data[i] for i in sampled_indices]\n\n        return self.__class__(columns=self.columns, data=sampled_data)\n\n    def select(self, columns: list[str]) -&gt; Self:\n        \"\"\"Return DataFrame with only specified columns.\"\"\"\n        # Validate all columns exist\n        for col in columns:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Get column indices\n        indices = [self.columns.index(col) for col in columns]\n\n        # Extract data for selected columns\n        new_data = [[row[i] for i in indices] for row in self.data]\n\n        return self.__class__(columns=columns, data=new_data)\n\n    def drop(self, columns: list[str]) -&gt; Self:\n        \"\"\"Return DataFrame without specified columns.\"\"\"\n        # Validate all columns exist\n        for col in columns:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Get columns to keep\n        keep_cols = [c for c in self.columns if c not in columns]\n\n        # Get indices for columns to keep\n        indices = [self.columns.index(col) for col in keep_cols]\n\n        # Extract data for kept columns\n        new_data = [[row[i] for i in indices] for row in self.data]\n\n        return self.__class__(columns=keep_cols, data=new_data)\n\n    def rename(self, mapper: dict[str, str]) -&gt; Self:\n        \"\"\"Return DataFrame with renamed columns.\"\"\"\n        # Validate all old column names exist\n        for old_name in mapper:\n            if old_name not in self.columns:\n                raise KeyError(f\"Column '{old_name}' not found in DataFrame\")\n\n        # Create new column list\n        new_cols = [mapper.get(col, col) for col in self.columns]\n\n        # Check for duplicates\n        if len(new_cols) != len(set(new_cols)):\n            raise ValueError(\"Renaming would create duplicate column names\")\n\n        return self.__class__(columns=new_cols, data=self.data)\n\n    def rename_columns(self, mapper: dict[str, str]) -&gt; Self:\n        \"\"\"Return DataFrame with renamed columns (alias for rename).\"\"\"\n        return self.rename(mapper)\n\n    def validate_structure(self) -&gt; None:\n        \"\"\"Validate DataFrame structure.\"\"\"\n        # Check for empty column names\n        for i, col in enumerate(self.columns):\n            if col == \"\":\n                raise ValueError(f\"Column at index {i} is empty\")\n\n        # Check for duplicate column names\n        if len(self.columns) != len(set(self.columns)):\n            duplicates = [col for col in self.columns if self.columns.count(col) &gt; 1]\n            raise ValueError(f\"Duplicate column names found: {set(duplicates)}\")\n\n        # Check all rows have same length as columns\n        num_cols = len(self.columns)\n        for i, row in enumerate(self.data):\n            if len(row) != num_cols:\n                raise ValueError(f\"Row {i} has {len(row)} values, expected {num_cols}\")\n\n    def infer_types(self) -&gt; dict[str, str]:\n        \"\"\"Infer column data types.\"\"\"\n        result: dict[str, str] = {}\n\n        for col_idx, col_name in enumerate(self.columns):\n            # Extract all values for this column\n            values = [row[col_idx] for row in self.data]\n\n            # Filter out None values for type checking\n            non_null_values = [v for v in values if v is not None]\n\n            if not non_null_values:\n                result[col_name] = \"null\"\n                continue\n\n            # Check types\n            types_found = set()\n            for val in non_null_values:\n                if isinstance(val, bool):\n                    types_found.add(\"bool\")\n                elif isinstance(val, int):\n                    types_found.add(\"int\")\n                elif isinstance(val, float):\n                    types_found.add(\"float\")\n                elif isinstance(val, str):\n                    types_found.add(\"str\")\n                else:\n                    types_found.add(\"other\")\n\n            # Determine final type\n            if len(types_found) &gt; 1:\n                # Special case: int and float can be treated as float\n                if types_found == {\"int\", \"float\"}:\n                    result[col_name] = \"float\"\n                else:\n                    result[col_name] = \"mixed\"\n            elif \"bool\" in types_found:\n                result[col_name] = \"bool\"\n            elif \"int\" in types_found:\n                result[col_name] = \"int\"\n            elif \"float\" in types_found:\n                result[col_name] = \"float\"\n            elif \"str\" in types_found:\n                result[col_name] = \"str\"\n            else:\n                result[col_name] = \"mixed\"\n\n        return result\n\n    def has_nulls(self) -&gt; dict[str, bool]:\n        \"\"\"Check for null values in each column.\"\"\"\n        result: dict[str, bool] = {}\n\n        for col_idx, col_name in enumerate(self.columns):\n            # Check if any value in this column is None\n            has_null = any(row[col_idx] is None for row in self.data)\n            result[col_name] = has_null\n\n        return result\n\n    # Iteration and length\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of rows.\"\"\"\n        return len(self.data)\n\n    def __iter__(self) -&gt; Any:\n        \"\"\"Iterate over rows as dictionaries.\"\"\"\n        for row in self.data:\n            yield dict(zip(self.columns, row))\n\n    # JSON support\n\n    @classmethod\n    def from_json(cls, json_string: str) -&gt; Self:\n        \"\"\"Create DataFrame from JSON string (array of objects).\"\"\"\n        records = json.loads(json_string)\n        if not isinstance(records, list):\n            raise ValueError(\"JSON must be an array of objects\")\n        return cls.from_records(records)\n\n    def to_json(self, orient: Literal[\"records\", \"columns\"] = \"records\") -&gt; str:\n        \"\"\"Export DataFrame as JSON string.\"\"\"\n        # Map \"columns\" to \"list\" for to_dict()\n        dict_orient: Literal[\"dict\", \"list\", \"records\"] = \"list\" if orient == \"columns\" else orient\n        return json.dumps(self.to_dict(orient=dict_orient))\n\n    # Column access\n\n    def get_column(self, column: str) -&gt; list[Any]:\n        \"\"\"Get all values for a column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n        idx = self.columns.index(column)\n        return [row[idx] for row in self.data]\n\n    def __getitem__(self, key: str | list[str]) -&gt; list[Any] | Self:\n        \"\"\"Support df['col'] and df[['col1', 'col2']].\"\"\"\n        if isinstance(key, str):\n            return self.get_column(key)\n        return self.select(key)\n\n    # Analytics methods\n\n    def unique(self, column: str) -&gt; list[Any]:\n        \"\"\"Get unique values from a column (preserves order).\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        seen = set()\n        result = []\n        for row in self.data:\n            val = row[col_idx]\n            if val not in seen:\n                seen.add(val)\n                result.append(val)\n        return result\n\n    def value_counts(self, column: str) -&gt; dict[Any, int]:\n        \"\"\"Count occurrences of each unique value in column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        counts: dict[Any, int] = {}\n        for row in self.data:\n            val = row[col_idx]\n            counts[val] = counts.get(val, 0) + 1\n        return counts\n\n    def sort(self, by: str, ascending: bool = True) -&gt; Self:\n        \"\"\"Sort DataFrame by column.\"\"\"\n        if by not in self.columns:\n            raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n        col_idx = self.columns.index(by)\n\n        # Sort with None values at the end\n        def sort_key(row: list[Any]) -&gt; tuple[int, Any]:\n            val = row[col_idx]\n            if val is None:\n                # Use a tuple to ensure None sorts last\n                return (1, None) if ascending else (0, None)\n            return (0, val) if ascending else (1, val)\n\n        sorted_data = sorted(self.data, key=sort_key, reverse=not ascending)\n        return self.__class__(columns=self.columns, data=sorted_data)\n\n    # Row filtering and transformation\n\n    def filter(self, predicate: Any) -&gt; Self:\n        \"\"\"Filter rows using a predicate function.\"\"\"\n        filtered_data = []\n        for row in self.data:\n            row_dict = dict(zip(self.columns, row))\n            if predicate(row_dict):\n                filtered_data.append(row)\n        return self.__class__(columns=self.columns, data=filtered_data)\n\n    def apply(self, func: Any, column: str) -&gt; Self:\n        \"\"\"Apply function to column values.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        new_data = []\n        for row in self.data:\n            new_row = row.copy()\n            new_row[col_idx] = func(row[col_idx])\n            new_data.append(new_row)\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def add_column(self, name: str, values: list[Any]) -&gt; Self:\n        \"\"\"Add new column to DataFrame.\"\"\"\n        if name in self.columns:\n            raise ValueError(f\"Column '{name}' already exists\")\n\n        if len(values) != len(self.data):\n            raise ValueError(f\"Values length ({len(values)}) must match row count ({len(self.data)})\")\n\n        new_columns = self.columns + [name]\n        new_data = [row + [values[i]] for i, row in enumerate(self.data)]\n\n        return self.__class__(columns=new_columns, data=new_data)\n\n    def drop_rows(self, indices: list[int]) -&gt; Self:\n        \"\"\"Drop rows by index.\"\"\"\n        indices_set = set(indices)\n        new_data = [row for i, row in enumerate(self.data) if i not in indices_set]\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def drop_duplicates(self, subset: list[str] | None = None) -&gt; Self:\n        \"\"\"Remove duplicate rows.\"\"\"\n        # Validate subset columns\n        if subset is not None:\n            for col in subset:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n            col_indices = [self.columns.index(col) for col in subset]\n        else:\n            col_indices = list(range(len(self.columns)))\n\n        # Track seen values\n        seen = set()\n        new_data = []\n\n        for row in self.data:\n            # Create tuple of relevant column values\n            key = tuple(row[i] for i in col_indices)\n\n            if key not in seen:\n                seen.add(key)\n                new_data.append(row)\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def fillna(self, value: Any | dict[str, Any]) -&gt; Self:\n        \"\"\"Replace None values.\"\"\"\n        if isinstance(value, dict):\n            # Validate column names\n            for col in value:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n            # Create mapping of column index to fill value\n            fill_map = {self.columns.index(col): val for col, val in value.items()}\n\n            # Fill values\n            new_data = []\n            for row in self.data:\n                new_row = [fill_map[i] if i in fill_map and val is None else val for i, val in enumerate(row)]\n                new_data.append(new_row)\n        else:\n            # Single fill value for all None\n            new_data = [[value if val is None else val for val in row] for row in self.data]\n\n        return self.__class__(columns=self.columns, data=new_data)\n\n    def concat(self, other: Self) -&gt; Self:\n        \"\"\"Concatenate DataFrames vertically (stack rows).\"\"\"\n        if self.columns != other.columns:\n            raise ValueError(f\"Column mismatch: {self.columns} != {other.columns}\")\n\n        combined_data = self.data + other.data\n        return self.__class__(columns=self.columns, data=combined_data)\n\n    def melt(\n        self,\n        id_vars: list[str] | None = None,\n        value_vars: list[str] | None = None,\n        var_name: str = \"variable\",\n        value_name: str = \"value\",\n    ) -&gt; Self:\n        \"\"\"Unpivot DataFrame from wide to long format.\"\"\"\n        # Handle empty DataFrame\n        if not self.columns or not self.data:\n            return self.__class__(columns=[var_name, value_name], data=[])\n\n        # Default id_vars to empty list if not specified\n        if id_vars is None:\n            id_vars = []\n\n        # Validate id_vars exist\n        for col in id_vars:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Default value_vars to all non-id columns\n        if value_vars is None:\n            value_vars = [col for col in self.columns if col not in id_vars]\n        else:\n            # Validate value_vars exist\n            for col in value_vars:\n                if col not in self.columns:\n                    raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # If no value_vars to melt, return empty result\n        if not value_vars:\n            # Return just id columns if all columns are id_vars\n            if id_vars:\n                return self.select(id_vars)\n            return self.__class__(columns=[var_name, value_name], data=[])\n\n        # Check for column name conflicts\n        new_columns = id_vars + [var_name, value_name]\n        if len(new_columns) != len(set(new_columns)):\n            raise ValueError(\n                f\"Duplicate column names in result: {new_columns}. \"\n                f\"Choose different var_name or value_name to avoid conflicts.\"\n            )\n\n        # Get indices for id and value columns\n        id_indices = [self.columns.index(col) for col in id_vars]\n        value_indices = [(self.columns.index(col), col) for col in value_vars]\n\n        # Build melted data\n        melted_data: list[list[Any]] = []\n\n        for row in self.data:\n            # Extract id values for this row\n            id_values = [row[idx] for idx in id_indices]\n\n            # Create one new row for each value_var\n            for val_idx, var_col_name in value_indices:\n                new_row = id_values + [var_col_name, row[val_idx]]\n                melted_data.append(new_row)\n\n        return self.__class__(columns=new_columns, data=melted_data)\n\n    def pivot(self, index: str, columns: str, values: str) -&gt; Self:\n        \"\"\"Pivot DataFrame from long to wide format.\"\"\"\n        # Validate columns exist\n        for col_name, param in [(index, \"index\"), (columns, \"columns\"), (values, \"values\")]:\n            if col_name not in self.columns:\n                raise KeyError(f\"Column '{col_name}' not found in DataFrame (parameter: {param})\")\n\n        # Get column indices\n        index_idx = self.columns.index(index)\n        columns_idx = self.columns.index(columns)\n        values_idx = self.columns.index(values)\n\n        # Build pivot structure: dict[index_value, dict[column_value, value]]\n        pivot_dict: dict[Any, dict[Any, Any]] = {}\n        column_values_set: set[Any] = set()\n\n        for row in self.data:\n            idx_val = row[index_idx]\n            col_val = row[columns_idx]\n            val = row[values_idx]\n\n            # Track column values for final column list\n            column_values_set.add(col_val)\n\n            # Initialize nested dict if needed\n            if idx_val not in pivot_dict:\n                pivot_dict[idx_val] = {}\n\n            # Check for duplicates\n            if col_val in pivot_dict[idx_val]:\n                raise ValueError(\n                    f\"Duplicate entries found for index='{idx_val}' and columns='{col_val}'. \"\n                    f\"Cannot reshape with duplicate index/column combinations. \"\n                    f\"Consider using aggregation or removing duplicates first.\"\n                )\n\n            pivot_dict[idx_val] = {**pivot_dict[idx_val], col_val: val}\n\n        # Sort column values for consistent ordering\n        column_values = sorted(column_values_set, key=lambda x: (x is None, x))\n\n        # Build result columns: [index_column, col1, col2, ...]\n        result_columns = [index] + column_values\n\n        # Build result data\n        result_data: list[list[Any]] = []\n        for idx_val in sorted(pivot_dict.keys(), key=lambda x: (x is None, x)):\n            row_dict = pivot_dict[idx_val]\n            # Build row: [index_value, value_for_col1, value_for_col2, ...]\n            row = [idx_val] + [row_dict.get(col_val, None) for col_val in column_values]\n            result_data.append(row)\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    def merge(\n        self,\n        other: Self,\n        on: str | list[str] | None = None,\n        how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\",\n        left_on: str | list[str] | None = None,\n        right_on: str | list[str] | None = None,\n        suffixes: tuple[str, str] = (\"_x\", \"_y\"),\n    ) -&gt; Self:\n        \"\"\"Merge DataFrames using database-style join.\"\"\"\n        # Determine join keys\n        if on is not None:\n            if left_on is not None or right_on is not None:\n                raise ValueError(\"Cannot specify both 'on' and 'left_on'/'right_on'\")\n            left_keys = [on] if isinstance(on, str) else on\n            right_keys = left_keys\n        elif left_on is not None and right_on is not None:\n            left_keys = [left_on] if isinstance(left_on, str) else left_on\n            right_keys = [right_on] if isinstance(right_on, str) else right_on\n            if len(left_keys) != len(right_keys):\n                raise ValueError(\"left_on and right_on must have same length\")\n        else:\n            raise ValueError(\"Must specify either 'on' or both 'left_on' and 'right_on'\")\n\n        # Validate join keys exist\n        for key in left_keys:\n            if key not in self.columns:\n                raise KeyError(f\"Join key '{key}' not found in left DataFrame\")\n        for key in right_keys:\n            if key not in other.columns:\n                raise KeyError(f\"Join key '{key}' not found in right DataFrame\")\n\n        # Get indices for join keys\n        left_key_indices = [self.columns.index(k) for k in left_keys]\n        right_key_indices = [other.columns.index(k) for k in right_keys]\n\n        # Build lookup dict for right DataFrame: key_tuple -&gt; list[row_indices]\n        right_lookup: dict[tuple[Any, ...], list[int]] = {}\n        for row_idx, row in enumerate(other.data):\n            key_tuple = tuple(row[idx] for idx in right_key_indices)\n            if key_tuple not in right_lookup:\n                right_lookup[key_tuple] = []\n            right_lookup[key_tuple].append(row_idx)\n\n        # Determine result columns\n        left_suffix, right_suffix = suffixes\n\n        # Start with left DataFrame columns\n        result_columns = self.columns.copy()\n\n        # Add right DataFrame columns (excluding join keys if using 'on')\n        for col in other.columns:\n            if on is not None and col in left_keys:\n                # Skip join key columns from right when using 'on'\n                continue\n\n            if col in result_columns:\n                # Handle collision with suffix\n                result_columns.append(f\"{col}{right_suffix}\")\n                # Also need to rename left column\n                left_col_idx = result_columns.index(col)\n                result_columns[left_col_idx] = f\"{col}{left_suffix}\"\n            else:\n                result_columns.append(col)\n\n        # Get indices of right columns to include\n        right_col_indices = []\n        for col in other.columns:\n            if on is not None and col in right_keys:\n                continue\n            right_col_indices.append(other.columns.index(col))\n\n        # Perform join\n        result_data: list[list[Any]] = []\n        matched_right_indices: set[int] = set()\n\n        for left_row in self.data:\n            # Extract key from left row\n            left_key_tuple = tuple(left_row[idx] for idx in left_key_indices)\n\n            # Find matching rows in right DataFrame\n            right_matches = right_lookup.get(left_key_tuple, [])\n\n            if right_matches:\n                # Join matched rows\n                for right_idx in right_matches:\n                    matched_right_indices.add(right_idx)\n                    right_row = other.data[right_idx]\n\n                    # Build result row: left columns + right columns (excluding join keys)\n                    result_row = left_row.copy()\n                    for col_idx in right_col_indices:\n                        result_row.append(right_row[col_idx])\n\n                    result_data.append(result_row)\n            else:\n                # No match\n                if how in (\"left\", \"outer\"):\n                    # Include left row with None for right columns\n                    result_row = left_row.copy()\n                    result_row.extend([None] * len(right_col_indices))\n                    result_data.append(result_row)\n\n        # Handle right/outer joins - add unmatched right rows\n        if how in (\"right\", \"outer\"):\n            for right_idx, right_row in enumerate(other.data):\n                if right_idx not in matched_right_indices:\n                    # Build row with None for left columns\n                    result_row = [None] * len(self.columns)\n\n                    # Fill in join key values if using 'on'\n                    if on is not None:\n                        for left_idx, right_idx_key in zip(left_key_indices, right_key_indices):\n                            result_row[left_idx] = right_row[right_idx_key]\n\n                    # Add right columns\n                    for col_idx in right_col_indices:\n                        result_row.append(right_row[col_idx])\n\n                    result_data.append(result_row)\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    def transpose(self) -&gt; Self:\n        \"\"\"Transpose DataFrame by swapping rows and columns.\"\"\"\n        if not self.data:\n            # Empty DataFrame - return with swapped structure\n            return self.__class__(columns=[], data=[])\n\n        # First column becomes the new column names\n        # Remaining columns become data rows\n        if not self.columns:\n            return self.__class__(columns=[], data=[])\n\n        # Extract first column values as new column names\n        # Convert to strings to ensure valid column names\n        new_columns = [str(row[0]) for row in self.data]\n\n        # Transpose the remaining columns\n        num_original_cols = len(self.columns)\n        if num_original_cols == 1:\n            # Only one column (the index) - result is just column names as rows\n            single_col_data = [[col] for col in self.columns]\n            return self.__class__(columns=new_columns if new_columns else [\"0\"], data=single_col_data)\n\n        # Build transposed data\n        # Each original column (except first) becomes a row\n        # Each original row becomes a column\n        result_data: list[list[Any]] = []\n\n        for col_idx in range(1, num_original_cols):\n            # Original column name becomes first value in new row\n            row = [self.columns[col_idx]]\n            # Add values from each original row for this column\n            for orig_row in self.data:\n                row.append(orig_row[col_idx])\n            result_data.append(row)\n\n        # New columns: first is placeholder for original column names, rest are from first column\n        result_columns = [\"index\"] + new_columns\n\n        return self.__class__(columns=result_columns, data=result_data)\n\n    # Statistical methods\n\n    def describe(self) -&gt; Self:\n        \"\"\"Generate statistical summary for numeric columns.\"\"\"\n        import statistics\n\n        stats_rows: list[list[Any]] = []\n        stat_names = [\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n\n        for col_idx in range(len(self.columns)):\n            # Extract numeric values (filter None and non-numeric)\n            values = []\n            for row in self.data:\n                val = row[col_idx]\n                if val is not None and isinstance(val, (int, float)) and not isinstance(val, bool):\n                    values.append(float(val))\n\n            if not values:\n                # Non-numeric column - fill with None\n                stats_rows.append([None] * len(stat_names))\n                continue\n\n            # Calculate statistics\n            count = len(values)\n            mean = statistics.mean(values)\n            std = statistics.stdev(values) if count &gt; 1 else 0.0\n            min_val = min(values)\n            max_val = max(values)\n\n            # Quantiles\n            sorted_vals = sorted(values)\n            try:\n                q25 = statistics.quantiles(sorted_vals, n=4)[0] if count &gt; 1 else sorted_vals[0]\n                q50 = statistics.median(sorted_vals)\n                q75 = statistics.quantiles(sorted_vals, n=4)[2] if count &gt; 1 else sorted_vals[0]\n            except statistics.StatisticsError:\n                q25 = q50 = q75 = sorted_vals[0] if sorted_vals else 0.0\n\n            stats_rows.append([count, mean, std, min_val, q25, q50, q75, max_val])\n\n        # Transpose to make stats the rows and columns the columns\n        transposed_data = [\n            [stats_rows[col_idx][stat_idx] for col_idx in range(len(self.columns))]\n            for stat_idx in range(len(stat_names))\n        ]\n\n        return self.__class__(columns=self.columns, data=transposed_data).add_column(\"stat\", stat_names)\n\n    def groupby(self, by: str) -&gt; \"GroupBy\":\n        \"\"\"Group DataFrame by column values.\"\"\"\n        if by not in self.columns:\n            raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n        return GroupBy(self, by)\n\n    # Utility methods\n\n    def equals(self, other: Any) -&gt; bool:\n        \"\"\"Check if two DataFrames are identical.\"\"\"\n        if not isinstance(other, DataFrame):\n            return False\n        return self.columns == other.columns and self.data == other.data\n\n    def deepcopy(self) -&gt; Self:\n        \"\"\"Create a deep copy of the DataFrame.\"\"\"\n        import copy\n\n        return self.__class__(columns=self.columns.copy(), data=copy.deepcopy(self.data))\n\n    def isna(self) -&gt; Self:\n        \"\"\"Return DataFrame of booleans showing None locations.\"\"\"\n        null_data = [[val is None for val in row] for row in self.data]\n        return self.__class__(columns=self.columns, data=null_data)\n\n    def notna(self) -&gt; Self:\n        \"\"\"Return DataFrame of booleans showing non-None locations.\"\"\"\n        not_null_data = [[val is not None for val in row] for row in self.data]\n        return self.__class__(columns=self.columns, data=not_null_data)\n\n    def dropna(self, axis: Literal[0, 1] = 0, how: Literal[\"any\", \"all\"] = \"any\") -&gt; Self:\n        \"\"\"Drop rows or columns with None values.\"\"\"\n        if axis == 0:\n            # Drop rows\n            if how == \"any\":\n                # Drop rows with any None\n                new_data = [row for row in self.data if not any(val is None for val in row)]\n            else:\n                # Drop rows with all None\n                new_data = [row for row in self.data if not all(val is None for val in row)]\n            return self.__class__(columns=self.columns, data=new_data)\n        else:\n            # Drop columns (axis=1)\n            cols_to_keep = []\n            indices_to_keep = []\n\n            for col_idx, col_name in enumerate(self.columns):\n                col_values = [row[col_idx] for row in self.data]\n\n                if how == \"any\":\n                    # Keep column if no None values\n                    if not any(val is None for val in col_values):\n                        cols_to_keep.append(col_name)\n                        indices_to_keep.append(col_idx)\n                else:\n                    # Keep column if not all None\n                    if not all(val is None for val in col_values):\n                        cols_to_keep.append(col_name)\n                        indices_to_keep.append(col_idx)\n\n            # Extract data for kept columns\n            new_data = [[row[i] for i in indices_to_keep] for row in self.data]\n            return self.__class__(columns=cols_to_keep, data=new_data)\n\n    def nunique(self, column: str) -&gt; int:\n        \"\"\"Count number of unique values in column.\"\"\"\n        if column not in self.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.columns.index(column)\n        unique_values = set()\n        for row in self.data:\n            val = row[col_idx]\n            # Count None as a unique value\n            unique_values.add(val)\n        return len(unique_values)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame-attributes","title":"Attributes","text":""},{"location":"api-reference/#chapkit.data.DataFrame.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Return tuple representing dimensionality of the DataFrame.</p>"},{"location":"api-reference/#chapkit.data.DataFrame.empty","title":"<code>empty</code>  <code>property</code>","text":"<p>Indicator whether DataFrame is empty.</p>"},{"location":"api-reference/#chapkit.data.DataFrame.size","title":"<code>size</code>  <code>property</code>","text":"<p>Return int representing number of elements in this object.</p>"},{"location":"api-reference/#chapkit.data.DataFrame.ndim","title":"<code>ndim</code>  <code>property</code>","text":"<p>Return int representing number of axes/array dimensions.</p>"},{"location":"api-reference/#chapkit.data.DataFrame-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.data.DataFrame.from_pandas","title":"<code>from_pandas(df)</code>  <code>classmethod</code>","text":"<p>Create schema from pandas DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_pandas(cls, df: Any) -&gt; Self:\n    \"\"\"Create schema from pandas DataFrame.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for from_pandas(). Install with: uv add pandas\") from None\n\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f\"Expected pandas DataFrame, got {type(df)}\")\n\n    return cls(\n        columns=df.columns.tolist(),\n        data=df.values.tolist(),\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_polars","title":"<code>from_polars(df)</code>  <code>classmethod</code>","text":"<p>Create schema from Polars DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_polars(cls, df: Any) -&gt; Self:\n    \"\"\"Create schema from Polars DataFrame.\"\"\"\n    try:\n        import polars as pl\n    except ImportError:\n        raise ImportError(\"polars is required for from_polars(). Install with: uv add polars\") from None\n\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"Expected Polars DataFrame, got {type(df)}\")\n\n    return cls(\n        columns=df.columns,\n        data=[list(row) for row in df.rows()],\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_xarray","title":"<code>from_xarray(da)</code>  <code>classmethod</code>","text":"<p>Create schema from xarray DataArray (2D only).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_xarray(cls, da: Any) -&gt; Self:\n    \"\"\"Create schema from xarray DataArray (2D only).\"\"\"\n    try:\n        import xarray as xr\n    except ImportError:\n        raise ImportError(\"xarray is required for from_xarray(). Install with: uv add xarray\") from None\n\n    if not isinstance(da, xr.DataArray):\n        raise TypeError(f\"Expected xarray DataArray, got {type(da)}\")\n\n    if len(da.dims) != 2:\n        raise ValueError(f\"Only 2D DataArrays supported, got {len(da.dims)} dimensions\")\n\n    # Convert to pandas then use from_pandas\n    pdf = da.to_pandas()\n    return cls.from_pandas(pdf)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create schema from dictionary of columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, list[Any]]) -&gt; Self:\n    \"\"\"Create schema from dictionary of columns.\"\"\"\n    if not data:\n        return cls(columns=[], data=[])\n\n    columns = list(data.keys())\n    num_rows = len(next(iter(data.values())))\n\n    if not all(len(vals) == num_rows for vals in data.values()):\n        raise ValueError(\"All columns must have the same length\")\n\n    rows = [[data[col][i] for col in columns] for i in range(num_rows)]\n\n    return cls(columns=columns, data=rows)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_records","title":"<code>from_records(records)</code>  <code>classmethod</code>","text":"<p>Create schema from list of records (row-oriented).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_records(cls, records: list[dict[str, Any]]) -&gt; Self:\n    \"\"\"Create schema from list of records (row-oriented).\"\"\"\n    if not records:\n        return cls(columns=[], data=[])\n\n    columns = list(records[0].keys())\n    data = [[record[col] for col in columns] for record in records]\n\n    return cls(columns=columns, data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_csv","title":"<code>from_csv(path=None, *, csv_string=None, delimiter=',', has_header=True, encoding='utf-8', infer_types=True)</code>  <code>classmethod</code>","text":"<p>Create DataFrame from CSV file or string.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    path: str | Path | None = None,\n    *,\n    csv_string: str | None = None,\n    delimiter: str = \",\",\n    has_header: bool = True,\n    encoding: str = \"utf-8\",\n    infer_types: bool = True,\n) -&gt; Self:\n    \"\"\"Create DataFrame from CSV file or string.\"\"\"\n    # Validate mutually exclusive parameters\n    if path is None and csv_string is None:\n        raise ValueError(\"Either path or csv_string must be provided\")\n    if path is not None and csv_string is not None:\n        raise ValueError(\"path and csv_string are mutually exclusive\")\n\n    # Read CSV data\n    if path is not None:\n        path_obj = Path(path)\n        if not path_obj.exists():\n            raise FileNotFoundError(f\"File not found: {path}\")\n        with path_obj.open(\"r\", encoding=encoding, newline=\"\") as f:\n            reader = csv.reader(f, delimiter=delimiter)\n            rows = list(reader)\n    else:\n        # csv_string is not None\n        string_io = io.StringIO(csv_string)\n        reader = csv.reader(string_io, delimiter=delimiter)\n        rows = list(reader)\n\n    # Handle empty CSV\n    if not rows:\n        return cls(columns=[], data=[])\n\n    # Extract columns and data\n    if has_header:\n        columns = rows[0]\n        data = rows[1:]\n    else:\n        # Generate column names\n        num_cols = len(rows[0]) if rows else 0\n        columns = [f\"col_{i}\" for i in range(num_cols)]\n        data = rows\n\n    # Apply type inference if enabled\n    if infer_types and data:\n        num_cols = len(columns)\n        converted_columns: list[list[Any]] = []\n\n        for col_idx in range(num_cols):\n            col_values = [row[col_idx] for row in data]\n            target_type = _infer_column_type(col_values)\n            converted_values = _convert_column(col_values, target_type)\n            converted_columns.append(converted_values)\n\n        data = [\n            [converted_columns[col_idx][row_idx] for col_idx in range(num_cols)] for row_idx in range(len(data))\n        ]\n\n    return cls(columns=columns, data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Convert schema to pandas DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; Any:\n    \"\"\"Convert schema to pandas DataFrame.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_pandas(). Install with: uv add pandas\") from None\n\n    return pd.DataFrame(self.data, columns=self.columns)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.to_polars","title":"<code>to_polars()</code>","text":"<p>Convert schema to Polars DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def to_polars(self) -&gt; Any:\n    \"\"\"Convert schema to Polars DataFrame.\"\"\"\n    try:\n        import polars as pl\n    except ImportError:\n        raise ImportError(\"polars is required for to_polars(). Install with: uv add polars\") from None\n\n    return pl.DataFrame(self.data, schema=self.columns, orient=\"row\")\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.to_dict","title":"<code>to_dict(orient='dict')</code>","text":"<p>Convert schema to dictionary with specified orient (dict, list, or records).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def to_dict(self, orient: Literal[\"dict\", \"list\", \"records\"] = \"dict\") -&gt; Any:\n    \"\"\"Convert schema to dictionary with specified orient (dict, list, or records).\"\"\"\n    if orient == \"dict\":\n        return {col: {i: self.data[i][j] for i in range(len(self.data))} for j, col in enumerate(self.columns)}\n    elif orient == \"list\":\n        return {col: [row[j] for row in self.data] for j, col in enumerate(self.columns)}\n    elif orient == \"records\":\n        return [{col: row[j] for j, col in enumerate(self.columns)} for row in self.data]\n    else:\n        raise ValueError(f\"Invalid orient: {orient}\")\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.to_csv","title":"<code>to_csv(path=None, *, delimiter=',', include_header=True, encoding='utf-8')</code>","text":"<p>Export DataFrame to CSV file or string.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def to_csv(\n    self,\n    path: str | Path | None = None,\n    *,\n    delimiter: str = \",\",\n    include_header: bool = True,\n    encoding: str = \"utf-8\",\n) -&gt; str | None:\n    \"\"\"Export DataFrame to CSV file or string.\"\"\"\n    # Write to string buffer or file\n    if path is None:\n        # Return as string\n        output = io.StringIO()\n        writer = csv.writer(output, delimiter=delimiter)\n\n        if include_header:\n            writer.writerow(self.columns)\n\n        writer.writerows(self.data)\n\n        return output.getvalue()\n    else:\n        # Write to file\n        path_obj = Path(path)\n        with path_obj.open(\"w\", encoding=encoding, newline=\"\") as f:\n            writer = csv.writer(f, delimiter=delimiter)\n\n            if include_header:\n                writer.writerow(self.columns)\n\n            writer.writerows(self.data)\n\n        return None\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.head","title":"<code>head(n=5)</code>","text":"<p>Return first n rows.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def head(self, n: int = 5) -&gt; Self:\n    \"\"\"Return first n rows.\"\"\"\n    if n &gt;= 0:\n        selected_data = self.data[:n]\n    else:\n        selected_data = self.data[:n] if n != 0 else self.data\n    return self.__class__(columns=self.columns, data=selected_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.tail","title":"<code>tail(n=5)</code>","text":"<p>Return last n rows.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def tail(self, n: int = 5) -&gt; Self:\n    \"\"\"Return last n rows.\"\"\"\n    if n &gt;= 0:\n        selected_data = self.data[-n:] if n &gt; 0 else []\n    else:\n        selected_data = self.data[abs(n) :]\n    return self.__class__(columns=self.columns, data=selected_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.sample","title":"<code>sample(n=None, frac=None, *, random_state=None)</code>","text":"<p>Return random sample of rows.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def sample(\n    self,\n    n: int | None = None,\n    frac: float | None = None,\n    *,\n    random_state: int | None = None,\n) -&gt; Self:\n    \"\"\"Return random sample of rows.\"\"\"\n    # Validate parameters\n    if n is None and frac is None:\n        raise ValueError(\"Either n or frac must be provided\")\n    if n is not None and frac is not None:\n        raise ValueError(\"n and frac are mutually exclusive\")\n\n    # Set random seed if provided\n    if random_state is not None:\n        random.seed(random_state)\n\n    # Calculate sample size\n    total_rows = len(self.data)\n    if frac is not None:\n        if frac &gt; 1.0:\n            raise ValueError(\"frac must be &lt;= 1.0\")\n        sample_size = int(total_rows * frac)\n    else:\n        sample_size = min(n, total_rows) if n is not None else 0\n\n    # Sample indices\n    if sample_size &gt;= total_rows:\n        sampled_indices = list(range(total_rows))\n        random.shuffle(sampled_indices)\n    else:\n        sampled_indices = random.sample(range(total_rows), sample_size)\n\n    # Extract sampled rows\n    sampled_data = [self.data[i] for i in sampled_indices]\n\n    return self.__class__(columns=self.columns, data=sampled_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.select","title":"<code>select(columns)</code>","text":"<p>Return DataFrame with only specified columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def select(self, columns: list[str]) -&gt; Self:\n    \"\"\"Return DataFrame with only specified columns.\"\"\"\n    # Validate all columns exist\n    for col in columns:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Get column indices\n    indices = [self.columns.index(col) for col in columns]\n\n    # Extract data for selected columns\n    new_data = [[row[i] for i in indices] for row in self.data]\n\n    return self.__class__(columns=columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.drop","title":"<code>drop(columns)</code>","text":"<p>Return DataFrame without specified columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def drop(self, columns: list[str]) -&gt; Self:\n    \"\"\"Return DataFrame without specified columns.\"\"\"\n    # Validate all columns exist\n    for col in columns:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Get columns to keep\n    keep_cols = [c for c in self.columns if c not in columns]\n\n    # Get indices for columns to keep\n    indices = [self.columns.index(col) for col in keep_cols]\n\n    # Extract data for kept columns\n    new_data = [[row[i] for i in indices] for row in self.data]\n\n    return self.__class__(columns=keep_cols, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.rename","title":"<code>rename(mapper)</code>","text":"<p>Return DataFrame with renamed columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def rename(self, mapper: dict[str, str]) -&gt; Self:\n    \"\"\"Return DataFrame with renamed columns.\"\"\"\n    # Validate all old column names exist\n    for old_name in mapper:\n        if old_name not in self.columns:\n            raise KeyError(f\"Column '{old_name}' not found in DataFrame\")\n\n    # Create new column list\n    new_cols = [mapper.get(col, col) for col in self.columns]\n\n    # Check for duplicates\n    if len(new_cols) != len(set(new_cols)):\n        raise ValueError(\"Renaming would create duplicate column names\")\n\n    return self.__class__(columns=new_cols, data=self.data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.rename_columns","title":"<code>rename_columns(mapper)</code>","text":"<p>Return DataFrame with renamed columns (alias for rename).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def rename_columns(self, mapper: dict[str, str]) -&gt; Self:\n    \"\"\"Return DataFrame with renamed columns (alias for rename).\"\"\"\n    return self.rename(mapper)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.validate_structure","title":"<code>validate_structure()</code>","text":"<p>Validate DataFrame structure.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def validate_structure(self) -&gt; None:\n    \"\"\"Validate DataFrame structure.\"\"\"\n    # Check for empty column names\n    for i, col in enumerate(self.columns):\n        if col == \"\":\n            raise ValueError(f\"Column at index {i} is empty\")\n\n    # Check for duplicate column names\n    if len(self.columns) != len(set(self.columns)):\n        duplicates = [col for col in self.columns if self.columns.count(col) &gt; 1]\n        raise ValueError(f\"Duplicate column names found: {set(duplicates)}\")\n\n    # Check all rows have same length as columns\n    num_cols = len(self.columns)\n    for i, row in enumerate(self.data):\n        if len(row) != num_cols:\n            raise ValueError(f\"Row {i} has {len(row)} values, expected {num_cols}\")\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.infer_types","title":"<code>infer_types()</code>","text":"<p>Infer column data types.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def infer_types(self) -&gt; dict[str, str]:\n    \"\"\"Infer column data types.\"\"\"\n    result: dict[str, str] = {}\n\n    for col_idx, col_name in enumerate(self.columns):\n        # Extract all values for this column\n        values = [row[col_idx] for row in self.data]\n\n        # Filter out None values for type checking\n        non_null_values = [v for v in values if v is not None]\n\n        if not non_null_values:\n            result[col_name] = \"null\"\n            continue\n\n        # Check types\n        types_found = set()\n        for val in non_null_values:\n            if isinstance(val, bool):\n                types_found.add(\"bool\")\n            elif isinstance(val, int):\n                types_found.add(\"int\")\n            elif isinstance(val, float):\n                types_found.add(\"float\")\n            elif isinstance(val, str):\n                types_found.add(\"str\")\n            else:\n                types_found.add(\"other\")\n\n        # Determine final type\n        if len(types_found) &gt; 1:\n            # Special case: int and float can be treated as float\n            if types_found == {\"int\", \"float\"}:\n                result[col_name] = \"float\"\n            else:\n                result[col_name] = \"mixed\"\n        elif \"bool\" in types_found:\n            result[col_name] = \"bool\"\n        elif \"int\" in types_found:\n            result[col_name] = \"int\"\n        elif \"float\" in types_found:\n            result[col_name] = \"float\"\n        elif \"str\" in types_found:\n            result[col_name] = \"str\"\n        else:\n            result[col_name] = \"mixed\"\n\n    return result\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.has_nulls","title":"<code>has_nulls()</code>","text":"<p>Check for null values in each column.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def has_nulls(self) -&gt; dict[str, bool]:\n    \"\"\"Check for null values in each column.\"\"\"\n    result: dict[str, bool] = {}\n\n    for col_idx, col_name in enumerate(self.columns):\n        # Check if any value in this column is None\n        has_null = any(row[col_idx] is None for row in self.data)\n        result[col_name] = has_null\n\n    return result\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.__len__","title":"<code>__len__()</code>","text":"<p>Return number of rows.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of rows.\"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over rows as dictionaries.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def __iter__(self) -&gt; Any:\n    \"\"\"Iterate over rows as dictionaries.\"\"\"\n    for row in self.data:\n        yield dict(zip(self.columns, row))\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.from_json","title":"<code>from_json(json_string)</code>  <code>classmethod</code>","text":"<p>Create DataFrame from JSON string (array of objects).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>@classmethod\ndef from_json(cls, json_string: str) -&gt; Self:\n    \"\"\"Create DataFrame from JSON string (array of objects).\"\"\"\n    records = json.loads(json_string)\n    if not isinstance(records, list):\n        raise ValueError(\"JSON must be an array of objects\")\n    return cls.from_records(records)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.to_json","title":"<code>to_json(orient='records')</code>","text":"<p>Export DataFrame as JSON string.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def to_json(self, orient: Literal[\"records\", \"columns\"] = \"records\") -&gt; str:\n    \"\"\"Export DataFrame as JSON string.\"\"\"\n    # Map \"columns\" to \"list\" for to_dict()\n    dict_orient: Literal[\"dict\", \"list\", \"records\"] = \"list\" if orient == \"columns\" else orient\n    return json.dumps(self.to_dict(orient=dict_orient))\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.get_column","title":"<code>get_column(column)</code>","text":"<p>Get all values for a column.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def get_column(self, column: str) -&gt; list[Any]:\n    \"\"\"Get all values for a column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n    idx = self.columns.index(column)\n    return [row[idx] for row in self.data]\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Support df['col'] and df[['col1', 'col2']].</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def __getitem__(self, key: str | list[str]) -&gt; list[Any] | Self:\n    \"\"\"Support df['col'] and df[['col1', 'col2']].\"\"\"\n    if isinstance(key, str):\n        return self.get_column(key)\n    return self.select(key)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.unique","title":"<code>unique(column)</code>","text":"<p>Get unique values from a column (preserves order).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def unique(self, column: str) -&gt; list[Any]:\n    \"\"\"Get unique values from a column (preserves order).\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    seen = set()\n    result = []\n    for row in self.data:\n        val = row[col_idx]\n        if val not in seen:\n            seen.add(val)\n            result.append(val)\n    return result\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.value_counts","title":"<code>value_counts(column)</code>","text":"<p>Count occurrences of each unique value in column.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def value_counts(self, column: str) -&gt; dict[Any, int]:\n    \"\"\"Count occurrences of each unique value in column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    counts: dict[Any, int] = {}\n    for row in self.data:\n        val = row[col_idx]\n        counts[val] = counts.get(val, 0) + 1\n    return counts\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.sort","title":"<code>sort(by, ascending=True)</code>","text":"<p>Sort DataFrame by column.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def sort(self, by: str, ascending: bool = True) -&gt; Self:\n    \"\"\"Sort DataFrame by column.\"\"\"\n    if by not in self.columns:\n        raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n    col_idx = self.columns.index(by)\n\n    # Sort with None values at the end\n    def sort_key(row: list[Any]) -&gt; tuple[int, Any]:\n        val = row[col_idx]\n        if val is None:\n            # Use a tuple to ensure None sorts last\n            return (1, None) if ascending else (0, None)\n        return (0, val) if ascending else (1, val)\n\n    sorted_data = sorted(self.data, key=sort_key, reverse=not ascending)\n    return self.__class__(columns=self.columns, data=sorted_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.filter","title":"<code>filter(predicate)</code>","text":"<p>Filter rows using a predicate function.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def filter(self, predicate: Any) -&gt; Self:\n    \"\"\"Filter rows using a predicate function.\"\"\"\n    filtered_data = []\n    for row in self.data:\n        row_dict = dict(zip(self.columns, row))\n        if predicate(row_dict):\n            filtered_data.append(row)\n    return self.__class__(columns=self.columns, data=filtered_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.apply","title":"<code>apply(func, column)</code>","text":"<p>Apply function to column values.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def apply(self, func: Any, column: str) -&gt; Self:\n    \"\"\"Apply function to column values.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    new_data = []\n    for row in self.data:\n        new_row = row.copy()\n        new_row[col_idx] = func(row[col_idx])\n        new_data.append(new_row)\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.add_column","title":"<code>add_column(name, values)</code>","text":"<p>Add new column to DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def add_column(self, name: str, values: list[Any]) -&gt; Self:\n    \"\"\"Add new column to DataFrame.\"\"\"\n    if name in self.columns:\n        raise ValueError(f\"Column '{name}' already exists\")\n\n    if len(values) != len(self.data):\n        raise ValueError(f\"Values length ({len(values)}) must match row count ({len(self.data)})\")\n\n    new_columns = self.columns + [name]\n    new_data = [row + [values[i]] for i, row in enumerate(self.data)]\n\n    return self.__class__(columns=new_columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.drop_rows","title":"<code>drop_rows(indices)</code>","text":"<p>Drop rows by index.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def drop_rows(self, indices: list[int]) -&gt; Self:\n    \"\"\"Drop rows by index.\"\"\"\n    indices_set = set(indices)\n    new_data = [row for i, row in enumerate(self.data) if i not in indices_set]\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.drop_duplicates","title":"<code>drop_duplicates(subset=None)</code>","text":"<p>Remove duplicate rows.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def drop_duplicates(self, subset: list[str] | None = None) -&gt; Self:\n    \"\"\"Remove duplicate rows.\"\"\"\n    # Validate subset columns\n    if subset is not None:\n        for col in subset:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n        col_indices = [self.columns.index(col) for col in subset]\n    else:\n        col_indices = list(range(len(self.columns)))\n\n    # Track seen values\n    seen = set()\n    new_data = []\n\n    for row in self.data:\n        # Create tuple of relevant column values\n        key = tuple(row[i] for i in col_indices)\n\n        if key not in seen:\n            seen.add(key)\n            new_data.append(row)\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.fillna","title":"<code>fillna(value)</code>","text":"<p>Replace None values.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def fillna(self, value: Any | dict[str, Any]) -&gt; Self:\n    \"\"\"Replace None values.\"\"\"\n    if isinstance(value, dict):\n        # Validate column names\n        for col in value:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n        # Create mapping of column index to fill value\n        fill_map = {self.columns.index(col): val for col, val in value.items()}\n\n        # Fill values\n        new_data = []\n        for row in self.data:\n            new_row = [fill_map[i] if i in fill_map and val is None else val for i, val in enumerate(row)]\n            new_data.append(new_row)\n    else:\n        # Single fill value for all None\n        new_data = [[value if val is None else val for val in row] for row in self.data]\n\n    return self.__class__(columns=self.columns, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.concat","title":"<code>concat(other)</code>","text":"<p>Concatenate DataFrames vertically (stack rows).</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def concat(self, other: Self) -&gt; Self:\n    \"\"\"Concatenate DataFrames vertically (stack rows).\"\"\"\n    if self.columns != other.columns:\n        raise ValueError(f\"Column mismatch: {self.columns} != {other.columns}\")\n\n    combined_data = self.data + other.data\n    return self.__class__(columns=self.columns, data=combined_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.melt","title":"<code>melt(id_vars=None, value_vars=None, var_name='variable', value_name='value')</code>","text":"<p>Unpivot DataFrame from wide to long format.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def melt(\n    self,\n    id_vars: list[str] | None = None,\n    value_vars: list[str] | None = None,\n    var_name: str = \"variable\",\n    value_name: str = \"value\",\n) -&gt; Self:\n    \"\"\"Unpivot DataFrame from wide to long format.\"\"\"\n    # Handle empty DataFrame\n    if not self.columns or not self.data:\n        return self.__class__(columns=[var_name, value_name], data=[])\n\n    # Default id_vars to empty list if not specified\n    if id_vars is None:\n        id_vars = []\n\n    # Validate id_vars exist\n    for col in id_vars:\n        if col not in self.columns:\n            raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # Default value_vars to all non-id columns\n    if value_vars is None:\n        value_vars = [col for col in self.columns if col not in id_vars]\n    else:\n        # Validate value_vars exist\n        for col in value_vars:\n            if col not in self.columns:\n                raise KeyError(f\"Column '{col}' not found in DataFrame\")\n\n    # If no value_vars to melt, return empty result\n    if not value_vars:\n        # Return just id columns if all columns are id_vars\n        if id_vars:\n            return self.select(id_vars)\n        return self.__class__(columns=[var_name, value_name], data=[])\n\n    # Check for column name conflicts\n    new_columns = id_vars + [var_name, value_name]\n    if len(new_columns) != len(set(new_columns)):\n        raise ValueError(\n            f\"Duplicate column names in result: {new_columns}. \"\n            f\"Choose different var_name or value_name to avoid conflicts.\"\n        )\n\n    # Get indices for id and value columns\n    id_indices = [self.columns.index(col) for col in id_vars]\n    value_indices = [(self.columns.index(col), col) for col in value_vars]\n\n    # Build melted data\n    melted_data: list[list[Any]] = []\n\n    for row in self.data:\n        # Extract id values for this row\n        id_values = [row[idx] for idx in id_indices]\n\n        # Create one new row for each value_var\n        for val_idx, var_col_name in value_indices:\n            new_row = id_values + [var_col_name, row[val_idx]]\n            melted_data.append(new_row)\n\n    return self.__class__(columns=new_columns, data=melted_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.pivot","title":"<code>pivot(index, columns, values)</code>","text":"<p>Pivot DataFrame from long to wide format.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def pivot(self, index: str, columns: str, values: str) -&gt; Self:\n    \"\"\"Pivot DataFrame from long to wide format.\"\"\"\n    # Validate columns exist\n    for col_name, param in [(index, \"index\"), (columns, \"columns\"), (values, \"values\")]:\n        if col_name not in self.columns:\n            raise KeyError(f\"Column '{col_name}' not found in DataFrame (parameter: {param})\")\n\n    # Get column indices\n    index_idx = self.columns.index(index)\n    columns_idx = self.columns.index(columns)\n    values_idx = self.columns.index(values)\n\n    # Build pivot structure: dict[index_value, dict[column_value, value]]\n    pivot_dict: dict[Any, dict[Any, Any]] = {}\n    column_values_set: set[Any] = set()\n\n    for row in self.data:\n        idx_val = row[index_idx]\n        col_val = row[columns_idx]\n        val = row[values_idx]\n\n        # Track column values for final column list\n        column_values_set.add(col_val)\n\n        # Initialize nested dict if needed\n        if idx_val not in pivot_dict:\n            pivot_dict[idx_val] = {}\n\n        # Check for duplicates\n        if col_val in pivot_dict[idx_val]:\n            raise ValueError(\n                f\"Duplicate entries found for index='{idx_val}' and columns='{col_val}'. \"\n                f\"Cannot reshape with duplicate index/column combinations. \"\n                f\"Consider using aggregation or removing duplicates first.\"\n            )\n\n        pivot_dict[idx_val] = {**pivot_dict[idx_val], col_val: val}\n\n    # Sort column values for consistent ordering\n    column_values = sorted(column_values_set, key=lambda x: (x is None, x))\n\n    # Build result columns: [index_column, col1, col2, ...]\n    result_columns = [index] + column_values\n\n    # Build result data\n    result_data: list[list[Any]] = []\n    for idx_val in sorted(pivot_dict.keys(), key=lambda x: (x is None, x)):\n        row_dict = pivot_dict[idx_val]\n        # Build row: [index_value, value_for_col1, value_for_col2, ...]\n        row = [idx_val] + [row_dict.get(col_val, None) for col_val in column_values]\n        result_data.append(row)\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.merge","title":"<code>merge(other, on=None, how='inner', left_on=None, right_on=None, suffixes=('_x', '_y'))</code>","text":"<p>Merge DataFrames using database-style join.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def merge(\n    self,\n    other: Self,\n    on: str | list[str] | None = None,\n    how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\",\n    left_on: str | list[str] | None = None,\n    right_on: str | list[str] | None = None,\n    suffixes: tuple[str, str] = (\"_x\", \"_y\"),\n) -&gt; Self:\n    \"\"\"Merge DataFrames using database-style join.\"\"\"\n    # Determine join keys\n    if on is not None:\n        if left_on is not None or right_on is not None:\n            raise ValueError(\"Cannot specify both 'on' and 'left_on'/'right_on'\")\n        left_keys = [on] if isinstance(on, str) else on\n        right_keys = left_keys\n    elif left_on is not None and right_on is not None:\n        left_keys = [left_on] if isinstance(left_on, str) else left_on\n        right_keys = [right_on] if isinstance(right_on, str) else right_on\n        if len(left_keys) != len(right_keys):\n            raise ValueError(\"left_on and right_on must have same length\")\n    else:\n        raise ValueError(\"Must specify either 'on' or both 'left_on' and 'right_on'\")\n\n    # Validate join keys exist\n    for key in left_keys:\n        if key not in self.columns:\n            raise KeyError(f\"Join key '{key}' not found in left DataFrame\")\n    for key in right_keys:\n        if key not in other.columns:\n            raise KeyError(f\"Join key '{key}' not found in right DataFrame\")\n\n    # Get indices for join keys\n    left_key_indices = [self.columns.index(k) for k in left_keys]\n    right_key_indices = [other.columns.index(k) for k in right_keys]\n\n    # Build lookup dict for right DataFrame: key_tuple -&gt; list[row_indices]\n    right_lookup: dict[tuple[Any, ...], list[int]] = {}\n    for row_idx, row in enumerate(other.data):\n        key_tuple = tuple(row[idx] for idx in right_key_indices)\n        if key_tuple not in right_lookup:\n            right_lookup[key_tuple] = []\n        right_lookup[key_tuple].append(row_idx)\n\n    # Determine result columns\n    left_suffix, right_suffix = suffixes\n\n    # Start with left DataFrame columns\n    result_columns = self.columns.copy()\n\n    # Add right DataFrame columns (excluding join keys if using 'on')\n    for col in other.columns:\n        if on is not None and col in left_keys:\n            # Skip join key columns from right when using 'on'\n            continue\n\n        if col in result_columns:\n            # Handle collision with suffix\n            result_columns.append(f\"{col}{right_suffix}\")\n            # Also need to rename left column\n            left_col_idx = result_columns.index(col)\n            result_columns[left_col_idx] = f\"{col}{left_suffix}\"\n        else:\n            result_columns.append(col)\n\n    # Get indices of right columns to include\n    right_col_indices = []\n    for col in other.columns:\n        if on is not None and col in right_keys:\n            continue\n        right_col_indices.append(other.columns.index(col))\n\n    # Perform join\n    result_data: list[list[Any]] = []\n    matched_right_indices: set[int] = set()\n\n    for left_row in self.data:\n        # Extract key from left row\n        left_key_tuple = tuple(left_row[idx] for idx in left_key_indices)\n\n        # Find matching rows in right DataFrame\n        right_matches = right_lookup.get(left_key_tuple, [])\n\n        if right_matches:\n            # Join matched rows\n            for right_idx in right_matches:\n                matched_right_indices.add(right_idx)\n                right_row = other.data[right_idx]\n\n                # Build result row: left columns + right columns (excluding join keys)\n                result_row = left_row.copy()\n                for col_idx in right_col_indices:\n                    result_row.append(right_row[col_idx])\n\n                result_data.append(result_row)\n        else:\n            # No match\n            if how in (\"left\", \"outer\"):\n                # Include left row with None for right columns\n                result_row = left_row.copy()\n                result_row.extend([None] * len(right_col_indices))\n                result_data.append(result_row)\n\n    # Handle right/outer joins - add unmatched right rows\n    if how in (\"right\", \"outer\"):\n        for right_idx, right_row in enumerate(other.data):\n            if right_idx not in matched_right_indices:\n                # Build row with None for left columns\n                result_row = [None] * len(self.columns)\n\n                # Fill in join key values if using 'on'\n                if on is not None:\n                    for left_idx, right_idx_key in zip(left_key_indices, right_key_indices):\n                        result_row[left_idx] = right_row[right_idx_key]\n\n                # Add right columns\n                for col_idx in right_col_indices:\n                    result_row.append(right_row[col_idx])\n\n                result_data.append(result_row)\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.transpose","title":"<code>transpose()</code>","text":"<p>Transpose DataFrame by swapping rows and columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def transpose(self) -&gt; Self:\n    \"\"\"Transpose DataFrame by swapping rows and columns.\"\"\"\n    if not self.data:\n        # Empty DataFrame - return with swapped structure\n        return self.__class__(columns=[], data=[])\n\n    # First column becomes the new column names\n    # Remaining columns become data rows\n    if not self.columns:\n        return self.__class__(columns=[], data=[])\n\n    # Extract first column values as new column names\n    # Convert to strings to ensure valid column names\n    new_columns = [str(row[0]) for row in self.data]\n\n    # Transpose the remaining columns\n    num_original_cols = len(self.columns)\n    if num_original_cols == 1:\n        # Only one column (the index) - result is just column names as rows\n        single_col_data = [[col] for col in self.columns]\n        return self.__class__(columns=new_columns if new_columns else [\"0\"], data=single_col_data)\n\n    # Build transposed data\n    # Each original column (except first) becomes a row\n    # Each original row becomes a column\n    result_data: list[list[Any]] = []\n\n    for col_idx in range(1, num_original_cols):\n        # Original column name becomes first value in new row\n        row = [self.columns[col_idx]]\n        # Add values from each original row for this column\n        for orig_row in self.data:\n            row.append(orig_row[col_idx])\n        result_data.append(row)\n\n    # New columns: first is placeholder for original column names, rest are from first column\n    result_columns = [\"index\"] + new_columns\n\n    return self.__class__(columns=result_columns, data=result_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.describe","title":"<code>describe()</code>","text":"<p>Generate statistical summary for numeric columns.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def describe(self) -&gt; Self:\n    \"\"\"Generate statistical summary for numeric columns.\"\"\"\n    import statistics\n\n    stats_rows: list[list[Any]] = []\n    stat_names = [\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]\n\n    for col_idx in range(len(self.columns)):\n        # Extract numeric values (filter None and non-numeric)\n        values = []\n        for row in self.data:\n            val = row[col_idx]\n            if val is not None and isinstance(val, (int, float)) and not isinstance(val, bool):\n                values.append(float(val))\n\n        if not values:\n            # Non-numeric column - fill with None\n            stats_rows.append([None] * len(stat_names))\n            continue\n\n        # Calculate statistics\n        count = len(values)\n        mean = statistics.mean(values)\n        std = statistics.stdev(values) if count &gt; 1 else 0.0\n        min_val = min(values)\n        max_val = max(values)\n\n        # Quantiles\n        sorted_vals = sorted(values)\n        try:\n            q25 = statistics.quantiles(sorted_vals, n=4)[0] if count &gt; 1 else sorted_vals[0]\n            q50 = statistics.median(sorted_vals)\n            q75 = statistics.quantiles(sorted_vals, n=4)[2] if count &gt; 1 else sorted_vals[0]\n        except statistics.StatisticsError:\n            q25 = q50 = q75 = sorted_vals[0] if sorted_vals else 0.0\n\n        stats_rows.append([count, mean, std, min_val, q25, q50, q75, max_val])\n\n    # Transpose to make stats the rows and columns the columns\n    transposed_data = [\n        [stats_rows[col_idx][stat_idx] for col_idx in range(len(self.columns))]\n        for stat_idx in range(len(stat_names))\n    ]\n\n    return self.__class__(columns=self.columns, data=transposed_data).add_column(\"stat\", stat_names)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.groupby","title":"<code>groupby(by)</code>","text":"<p>Group DataFrame by column values.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def groupby(self, by: str) -&gt; \"GroupBy\":\n    \"\"\"Group DataFrame by column values.\"\"\"\n    if by not in self.columns:\n        raise KeyError(f\"Column '{by}' not found in DataFrame\")\n\n    return GroupBy(self, by)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.equals","title":"<code>equals(other)</code>","text":"<p>Check if two DataFrames are identical.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def equals(self, other: Any) -&gt; bool:\n    \"\"\"Check if two DataFrames are identical.\"\"\"\n    if not isinstance(other, DataFrame):\n        return False\n    return self.columns == other.columns and self.data == other.data\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.deepcopy","title":"<code>deepcopy()</code>","text":"<p>Create a deep copy of the DataFrame.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def deepcopy(self) -&gt; Self:\n    \"\"\"Create a deep copy of the DataFrame.\"\"\"\n    import copy\n\n    return self.__class__(columns=self.columns.copy(), data=copy.deepcopy(self.data))\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.isna","title":"<code>isna()</code>","text":"<p>Return DataFrame of booleans showing None locations.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def isna(self) -&gt; Self:\n    \"\"\"Return DataFrame of booleans showing None locations.\"\"\"\n    null_data = [[val is None for val in row] for row in self.data]\n    return self.__class__(columns=self.columns, data=null_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.notna","title":"<code>notna()</code>","text":"<p>Return DataFrame of booleans showing non-None locations.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def notna(self) -&gt; Self:\n    \"\"\"Return DataFrame of booleans showing non-None locations.\"\"\"\n    not_null_data = [[val is not None for val in row] for row in self.data]\n    return self.__class__(columns=self.columns, data=not_null_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.dropna","title":"<code>dropna(axis=0, how='any')</code>","text":"<p>Drop rows or columns with None values.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def dropna(self, axis: Literal[0, 1] = 0, how: Literal[\"any\", \"all\"] = \"any\") -&gt; Self:\n    \"\"\"Drop rows or columns with None values.\"\"\"\n    if axis == 0:\n        # Drop rows\n        if how == \"any\":\n            # Drop rows with any None\n            new_data = [row for row in self.data if not any(val is None for val in row)]\n        else:\n            # Drop rows with all None\n            new_data = [row for row in self.data if not all(val is None for val in row)]\n        return self.__class__(columns=self.columns, data=new_data)\n    else:\n        # Drop columns (axis=1)\n        cols_to_keep = []\n        indices_to_keep = []\n\n        for col_idx, col_name in enumerate(self.columns):\n            col_values = [row[col_idx] for row in self.data]\n\n            if how == \"any\":\n                # Keep column if no None values\n                if not any(val is None for val in col_values):\n                    cols_to_keep.append(col_name)\n                    indices_to_keep.append(col_idx)\n            else:\n                # Keep column if not all None\n                if not all(val is None for val in col_values):\n                    cols_to_keep.append(col_name)\n                    indices_to_keep.append(col_idx)\n\n        # Extract data for kept columns\n        new_data = [[row[i] for i in indices_to_keep] for row in self.data]\n        return self.__class__(columns=cols_to_keep, data=new_data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.DataFrame.nunique","title":"<code>nunique(column)</code>","text":"<p>Count number of unique values in column.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def nunique(self, column: str) -&gt; int:\n    \"\"\"Count number of unique values in column.\"\"\"\n    if column not in self.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.columns.index(column)\n    unique_values = set()\n    for row in self.data:\n        val = row[col_idx]\n        # Count None as a unique value\n        unique_values.add(val)\n    return len(unique_values)\n</code></pre>"},{"location":"api-reference/#groupby","title":"GroupBy","text":""},{"location":"api-reference/#chapkit.data.GroupBy","title":"<code>GroupBy</code>","text":"<p>GroupBy helper for aggregations.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>class GroupBy:\n    \"\"\"GroupBy helper for aggregations.\"\"\"\n\n    def __init__(self, dataframe: DataFrame, by: str):\n        \"\"\"Initialize GroupBy helper.\"\"\"\n        self.dataframe = dataframe\n        self.by = by\n        self.by_idx = dataframe.columns.index(by)\n\n        # Build groups\n        self.groups: dict[Any, list[list[Any]]] = {}\n        for row in dataframe.data:\n            key = row[self.by_idx]\n            if key not in self.groups:\n                self.groups[key] = []\n            self.groups[key].append(row)\n\n    def count(self) -&gt; DataFrame:\n        \"\"\"Count rows per group.\"\"\"\n        data = [[key, len(rows)] for key, rows in self.groups.items()]\n        return DataFrame(columns=[self.by, \"count\"], data=data)\n\n    def sum(self, column: str) -&gt; DataFrame:\n        \"\"\"Sum numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            total = sum(values) if values else None\n            data.append([key, total])\n\n        return DataFrame(columns=[self.by, f\"{column}_sum\"], data=data)\n\n    def mean(self, column: str) -&gt; DataFrame:\n        \"\"\"Calculate mean of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        import statistics\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            avg = statistics.mean(values) if values else None\n            data.append([key, avg])\n\n        return DataFrame(columns=[self.by, f\"{column}_mean\"], data=data)\n\n    def min(self, column: str) -&gt; DataFrame:\n        \"\"\"Find minimum of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            minimum = min(values) if values else None\n            data.append([key, minimum])\n\n        return DataFrame(columns=[self.by, f\"{column}_min\"], data=data)\n\n    def max(self, column: str) -&gt; DataFrame:\n        \"\"\"Find maximum of numeric column per group.\"\"\"\n        if column not in self.dataframe.columns:\n            raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n        col_idx = self.dataframe.columns.index(column)\n        data = []\n\n        for key, rows in self.groups.items():\n            values = [\n                row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n            ]\n            maximum = max(values) if values else None\n            data.append([key, maximum])\n\n        return DataFrame(columns=[self.by, f\"{column}_max\"], data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.data.GroupBy.__init__","title":"<code>__init__(dataframe, by)</code>","text":"<p>Initialize GroupBy helper.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def __init__(self, dataframe: DataFrame, by: str):\n    \"\"\"Initialize GroupBy helper.\"\"\"\n    self.dataframe = dataframe\n    self.by = by\n    self.by_idx = dataframe.columns.index(by)\n\n    # Build groups\n    self.groups: dict[Any, list[list[Any]]] = {}\n    for row in dataframe.data:\n        key = row[self.by_idx]\n        if key not in self.groups:\n            self.groups[key] = []\n        self.groups[key].append(row)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy.count","title":"<code>count()</code>","text":"<p>Count rows per group.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def count(self) -&gt; DataFrame:\n    \"\"\"Count rows per group.\"\"\"\n    data = [[key, len(rows)] for key, rows in self.groups.items()]\n    return DataFrame(columns=[self.by, \"count\"], data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy.sum","title":"<code>sum(column)</code>","text":"<p>Sum numeric column per group.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def sum(self, column: str) -&gt; DataFrame:\n    \"\"\"Sum numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        total = sum(values) if values else None\n        data.append([key, total])\n\n    return DataFrame(columns=[self.by, f\"{column}_sum\"], data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy.mean","title":"<code>mean(column)</code>","text":"<p>Calculate mean of numeric column per group.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def mean(self, column: str) -&gt; DataFrame:\n    \"\"\"Calculate mean of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    import statistics\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        avg = statistics.mean(values) if values else None\n        data.append([key, avg])\n\n    return DataFrame(columns=[self.by, f\"{column}_mean\"], data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy.min","title":"<code>min(column)</code>","text":"<p>Find minimum of numeric column per group.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def min(self, column: str) -&gt; DataFrame:\n    \"\"\"Find minimum of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        minimum = min(values) if values else None\n        data.append([key, minimum])\n\n    return DataFrame(columns=[self.by, f\"{column}_min\"], data=data)\n</code></pre>"},{"location":"api-reference/#chapkit.data.GroupBy.max","title":"<code>max(column)</code>","text":"<p>Find maximum of numeric column per group.</p> Source code in <code>src/chapkit/data/dataframe.py</code> <pre><code>def max(self, column: str) -&gt; DataFrame:\n    \"\"\"Find maximum of numeric column per group.\"\"\"\n    if column not in self.dataframe.columns:\n        raise KeyError(f\"Column '{column}' not found in DataFrame\")\n\n    col_idx = self.dataframe.columns.index(column)\n    data = []\n\n    for key, rows in self.groups.items():\n        values = [\n            row[col_idx] for row in rows if row[col_idx] is not None and isinstance(row[col_idx], (int, float))\n        ]\n        maximum = max(values) if values else None\n        data.append([key, maximum])\n\n    return DataFrame(columns=[self.by, f\"{column}_max\"], data=data)\n</code></pre>"},{"location":"api-reference/#config-module","title":"Config Module","text":"<p>Key-value configuration storage with Pydantic schema validation.</p>"},{"location":"api-reference/#models_1","title":"Models","text":""},{"location":"api-reference/#chapkit.config.models","title":"<code>models</code>","text":"<p>Config ORM models for key-value configuration storage and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.models-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.models.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>Entity</code></p> <p>ORM model for configuration with JSON data storage.</p> Source code in <code>src/chapkit/config/models.py</code> <pre><code>class Config(Entity):\n    \"\"\"ORM model for configuration with JSON data storage.\"\"\"\n\n    __tablename__ = \"configs\"\n\n    name: Mapped[str] = mapped_column(index=True)\n    _data_json: Mapped[dict[str, Any]] = mapped_column(\"data\", JSON, nullable=False)\n\n    @property\n    def data(self) -&gt; dict[str, Any]:\n        \"\"\"Return JSON data as dict.\"\"\"\n        return self._data_json\n\n    @data.setter\n    def data(self, value: BaseConfig | dict[str, Any]) -&gt; None:\n        \"\"\"Serialize Pydantic model to JSON or store dict directly.\"\"\"\n        if isinstance(value, dict):\n            self._data_json = value\n        elif hasattr(value, \"model_dump\") and callable(value.model_dump):\n            # BaseConfig or other Pydantic model\n            self._data_json = value.model_dump(mode=\"json\")\n        else:\n            raise TypeError(f\"data must be a BaseConfig subclass or dict, got {type(value)}\")\n</code></pre>"},{"location":"api-reference/#chapkit.config.models.Config-attributes","title":"Attributes","text":""},{"location":"api-reference/#chapkit.config.models.Config.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>Return JSON data as dict.</p>"},{"location":"api-reference/#chapkit.config.models.ConfigArtifact","title":"<code>ConfigArtifact</code>","text":"<p>               Bases: <code>Base</code></p> <p>Junction table linking Configs to root Artifacts.</p> Source code in <code>src/chapkit/config/models.py</code> <pre><code>class ConfigArtifact(Base):\n    \"\"\"Junction table linking Configs to root Artifacts.\"\"\"\n\n    __tablename__ = \"config_artifacts\"\n\n    config_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"configs.id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n    )\n\n    artifact_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"artifacts.id\", ondelete=\"CASCADE\"),\n        primary_key=True,\n        unique=True,\n    )\n\n    __table_args__ = (UniqueConstraint(\"artifact_id\", name=\"uq_artifact_id\"),)\n</code></pre>"},{"location":"api-reference/#schemas_2","title":"Schemas","text":""},{"location":"api-reference/#chapkit.config.schemas","title":"<code>schemas</code>","text":"<p>Config schemas for key-value configuration with JSON data.</p>"},{"location":"api-reference/#chapkit.config.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.schemas.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for configuration schemas with arbitrary extra fields allowed.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class BaseConfig(BaseModel):\n    \"\"\"Base class for configuration schemas with arbitrary extra fields allowed.\"\"\"\n\n    model_config = {\"extra\": \"allow\"}\n\n    # Reserved parameters (CHAP-interpreted)\n    prediction_periods: int  # Required, no default\n    additional_continuous_covariates: list[str] = []\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigIn","title":"<code>ConfigIn</code>","text":"<p>               Bases: <code>EntityIn</code></p> <p>Input schema for creating or updating configurations.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class ConfigIn[DataT: BaseConfig](EntityIn):\n    \"\"\"Input schema for creating or updating configurations.\"\"\"\n\n    name: str\n    data: DataT\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut","title":"<code>ConfigOut</code>","text":"<p>               Bases: <code>EntityOut</code></p> <p>Output schema for configuration entities.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class ConfigOut[DataT: BaseConfig](EntityOut):\n    \"\"\"Output schema for configuration entities.\"\"\"\n\n    name: str\n    data: DataT\n\n    model_config = {\"ser_json_timedelta\": \"float\", \"ser_json_bytes\": \"base64\"}\n\n    @field_validator(\"data\", mode=\"before\")\n    @classmethod\n    def convert_dict_to_model(cls, v: Any, info: ValidationInfo) -&gt; Any:\n        \"\"\"Convert dict to BaseConfig model if data_cls is provided in validation context.\"\"\"\n        if isinstance(v, BaseConfig):\n            return v\n        if isinstance(v, dict):\n            if info.context and \"data_cls\" in info.context:\n                data_cls = info.context[\"data_cls\"]\n                return data_cls.model_validate(v)\n        return v\n\n    @field_serializer(\"data\", when_used=\"json\")\n    def serialize_data(self, value: DataT) -&gt; dict[str, Any]:\n        \"\"\"Serialize BaseConfig data to JSON dict.\"\"\"\n        if isinstance(value, BaseConfig):  # pyright: ignore[reportUnnecessaryIsInstance]\n            return value.model_dump(mode=\"json\")\n        return value\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.schemas.ConfigOut.convert_dict_to_model","title":"<code>convert_dict_to_model(v, info)</code>  <code>classmethod</code>","text":"<p>Convert dict to BaseConfig model if data_cls is provided in validation context.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>@field_validator(\"data\", mode=\"before\")\n@classmethod\ndef convert_dict_to_model(cls, v: Any, info: ValidationInfo) -&gt; Any:\n    \"\"\"Convert dict to BaseConfig model if data_cls is provided in validation context.\"\"\"\n    if isinstance(v, BaseConfig):\n        return v\n    if isinstance(v, dict):\n        if info.context and \"data_cls\" in info.context:\n            data_cls = info.context[\"data_cls\"]\n            return data_cls.model_validate(v)\n    return v\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.ConfigOut.serialize_data","title":"<code>serialize_data(value)</code>","text":"<p>Serialize BaseConfig data to JSON dict.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>@field_serializer(\"data\", when_used=\"json\")\ndef serialize_data(self, value: DataT) -&gt; dict[str, Any]:\n    \"\"\"Serialize BaseConfig data to JSON dict.\"\"\"\n    if isinstance(value, BaseConfig):  # pyright: ignore[reportUnnecessaryIsInstance]\n        return value.model_dump(mode=\"json\")\n    return value\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.LinkArtifactRequest","title":"<code>LinkArtifactRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for linking an artifact to a config.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class LinkArtifactRequest(BaseModel):\n    \"\"\"Request schema for linking an artifact to a config.\"\"\"\n\n    artifact_id: ULID\n</code></pre>"},{"location":"api-reference/#chapkit.config.schemas.UnlinkArtifactRequest","title":"<code>UnlinkArtifactRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for unlinking an artifact from a config.</p> Source code in <code>src/chapkit/config/schemas.py</code> <pre><code>class UnlinkArtifactRequest(BaseModel):\n    \"\"\"Request schema for unlinking an artifact from a config.\"\"\"\n\n    artifact_id: ULID\n</code></pre>"},{"location":"api-reference/#repository_1","title":"Repository","text":""},{"location":"api-reference/#chapkit.config.repository","title":"<code>repository</code>","text":"<p>Config repository for database access and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.repository-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.repository.ConfigRepository","title":"<code>ConfigRepository</code>","text":"<p>               Bases: <code>BaseRepository[Config, ULID]</code></p> <p>Repository for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>class ConfigRepository(BaseRepository[Config, ULID]):\n    \"\"\"Repository for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initialize config repository with database session.\"\"\"\n        super().__init__(session, Config)\n\n    async def find_by_name(self, name: str) -&gt; Config | None:\n        \"\"\"Find a config by its unique name.\"\"\"\n        result = await self.s.scalars(select(self.model).where(self.model.name == name))\n        return result.one_or_none()\n\n    async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n        \"\"\"Link a config to a root artifact.\"\"\"\n        artifact = await self.s.get(Artifact, artifact_id)\n        if artifact is None:\n            raise ValueError(f\"Artifact {artifact_id} not found\")\n        if artifact.parent_id is not None:\n            raise ValueError(f\"Artifact {artifact_id} is not a root artifact (parent_id={artifact.parent_id})\")\n\n        link = ConfigArtifact(config_id=config_id, artifact_id=artifact_id)\n        self.s.add(link)\n\n    async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n        \"\"\"Unlink an artifact from its config.\"\"\"\n        stmt = sql_delete(ConfigArtifact).where(ConfigArtifact.artifact_id == artifact_id)\n        await self.s.execute(stmt)\n\n    async def delete_by_id(self, id: ULID) -&gt; None:\n        \"\"\"Delete a config and cascade delete all linked artifact trees.\"\"\"\n        from chapkit.artifact.repository import ArtifactRepository\n\n        linked_artifacts = await self.find_artifacts_for_config(id)\n\n        artifact_repo = ArtifactRepository(self.s)\n        for root_artifact in linked_artifacts:\n            subtree = await artifact_repo.find_subtree(root_artifact.id)\n            for artifact in subtree:\n                await self.s.delete(artifact)\n\n        await super().delete_by_id(id)\n\n    async def find_by_root_artifact_id(self, artifact_id: ULID) -&gt; Config | None:\n        \"\"\"Find the config linked to a root artifact.\"\"\"\n        stmt = (\n            select(Config)\n            .join(ConfigArtifact, Config.id == ConfigArtifact.config_id)\n            .where(ConfigArtifact.artifact_id == artifact_id)\n        )\n        result = await self.s.scalars(stmt)\n        return result.one_or_none()\n\n    async def find_artifacts_for_config(self, config_id: ULID) -&gt; list[Artifact]:\n        \"\"\"Find all root artifacts linked to a config.\"\"\"\n        stmt = (\n            select(Artifact)\n            .join(ConfigArtifact, Artifact.id == ConfigArtifact.artifact_id)\n            .where(ConfigArtifact.config_id == config_id)\n        )\n        result = await self.s.scalars(stmt)\n        return list(result.all())\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.__init__","title":"<code>__init__(session)</code>","text":"<p>Initialize config repository with database session.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initialize config repository with database session.\"\"\"\n    super().__init__(session, Config)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_by_name","title":"<code>find_by_name(name)</code>  <code>async</code>","text":"<p>Find a config by its unique name.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_by_name(self, name: str) -&gt; Config | None:\n    \"\"\"Find a config by its unique name.\"\"\"\n    result = await self.s.scalars(select(self.model).where(self.model.name == name))\n    return result.one_or_none()\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.link_artifact","title":"<code>link_artifact(config_id, artifact_id)</code>  <code>async</code>","text":"<p>Link a config to a root artifact.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n    \"\"\"Link a config to a root artifact.\"\"\"\n    artifact = await self.s.get(Artifact, artifact_id)\n    if artifact is None:\n        raise ValueError(f\"Artifact {artifact_id} not found\")\n    if artifact.parent_id is not None:\n        raise ValueError(f\"Artifact {artifact_id} is not a root artifact (parent_id={artifact.parent_id})\")\n\n    link = ConfigArtifact(config_id=config_id, artifact_id=artifact_id)\n    self.s.add(link)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.unlink_artifact","title":"<code>unlink_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Unlink an artifact from its config.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n    \"\"\"Unlink an artifact from its config.\"\"\"\n    stmt = sql_delete(ConfigArtifact).where(ConfigArtifact.artifact_id == artifact_id)\n    await self.s.execute(stmt)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.delete_by_id","title":"<code>delete_by_id(id)</code>  <code>async</code>","text":"<p>Delete a config and cascade delete all linked artifact trees.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def delete_by_id(self, id: ULID) -&gt; None:\n    \"\"\"Delete a config and cascade delete all linked artifact trees.\"\"\"\n    from chapkit.artifact.repository import ArtifactRepository\n\n    linked_artifacts = await self.find_artifacts_for_config(id)\n\n    artifact_repo = ArtifactRepository(self.s)\n    for root_artifact in linked_artifacts:\n        subtree = await artifact_repo.find_subtree(root_artifact.id)\n        for artifact in subtree:\n            await self.s.delete(artifact)\n\n    await super().delete_by_id(id)\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_by_root_artifact_id","title":"<code>find_by_root_artifact_id(artifact_id)</code>  <code>async</code>","text":"<p>Find the config linked to a root artifact.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_by_root_artifact_id(self, artifact_id: ULID) -&gt; Config | None:\n    \"\"\"Find the config linked to a root artifact.\"\"\"\n    stmt = (\n        select(Config)\n        .join(ConfigArtifact, Config.id == ConfigArtifact.config_id)\n        .where(ConfigArtifact.artifact_id == artifact_id)\n    )\n    result = await self.s.scalars(stmt)\n    return result.one_or_none()\n</code></pre>"},{"location":"api-reference/#chapkit.config.repository.ConfigRepository.find_artifacts_for_config","title":"<code>find_artifacts_for_config(config_id)</code>  <code>async</code>","text":"<p>Find all root artifacts linked to a config.</p> Source code in <code>src/chapkit/config/repository.py</code> <pre><code>async def find_artifacts_for_config(self, config_id: ULID) -&gt; list[Artifact]:\n    \"\"\"Find all root artifacts linked to a config.\"\"\"\n    stmt = (\n        select(Artifact)\n        .join(ConfigArtifact, Artifact.id == ConfigArtifact.artifact_id)\n        .where(ConfigArtifact.config_id == config_id)\n    )\n    result = await self.s.scalars(stmt)\n    return list(result.all())\n</code></pre>"},{"location":"api-reference/#manager_1","title":"Manager","text":""},{"location":"api-reference/#chapkit.config.manager","title":"<code>manager</code>","text":"<p>Config manager for CRUD operations and artifact linking.</p>"},{"location":"api-reference/#chapkit.config.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.manager.ConfigManager","title":"<code>ConfigManager</code>","text":"<p>               Bases: <code>BaseManager[Config, ConfigIn[DataT], ConfigOut[DataT], ULID]</code></p> <p>Manager for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>class ConfigManager[DataT: BaseConfig](BaseManager[Config, ConfigIn[DataT], ConfigOut[DataT], ULID]):\n    \"\"\"Manager for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(self, repo: ConfigRepository, data_cls: type[DataT]) -&gt; None:\n        \"\"\"Initialize config manager with repository and data class.\"\"\"\n        super().__init__(repo, Config, ConfigOut)\n        self.repository: ConfigRepository = repo\n        self.data_cls = data_cls\n\n    async def find_by_name(self, name: str) -&gt; ConfigOut[DataT] | None:\n        \"\"\"Find a config by its unique name.\"\"\"\n        config = await self.repository.find_by_name(name)\n        if config:\n            return self._to_output_schema(config)\n        return None\n\n    async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n        \"\"\"Link a config to a root artifact.\"\"\"\n        await self.repository.link_artifact(config_id, artifact_id)\n        await self.repository.commit()\n\n    async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n        \"\"\"Unlink an artifact from its config.\"\"\"\n        await self.repository.unlink_artifact(artifact_id)\n        await self.repository.commit()\n\n    async def get_config_for_artifact(\n        self, artifact_id: ULID, artifact_repo: ArtifactRepository\n    ) -&gt; ConfigOut[DataT] | None:\n        \"\"\"Get the config for an artifact by traversing to its root.\"\"\"\n        root = await artifact_repo.get_root_artifact(artifact_id)\n        if root is None:\n            return None\n\n        config = await self.repository.find_by_root_artifact_id(root.id)\n        if config is None:\n            return None\n\n        return self._to_output_schema(config)\n\n    async def get_linked_artifacts(self, config_id: ULID) -&gt; list[ArtifactOut]:\n        \"\"\"Get all root artifacts linked to a config.\"\"\"\n        artifacts = await self.repository.find_artifacts_for_config(config_id)\n        return [ArtifactOut.model_validate(artifact, from_attributes=True) for artifact in artifacts]\n\n    def _to_output_schema(self, entity: Config) -&gt; ConfigOut[DataT]:\n        \"\"\"Convert ORM entity to output schema with proper data class validation.\"\"\"\n        return ConfigOut[DataT].model_validate(entity, from_attributes=True, context={\"data_cls\": self.data_cls})\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.manager.ConfigManager.__init__","title":"<code>__init__(repo, data_cls)</code>","text":"<p>Initialize config manager with repository and data class.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>def __init__(self, repo: ConfigRepository, data_cls: type[DataT]) -&gt; None:\n    \"\"\"Initialize config manager with repository and data class.\"\"\"\n    super().__init__(repo, Config, ConfigOut)\n    self.repository: ConfigRepository = repo\n    self.data_cls = data_cls\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.find_by_name","title":"<code>find_by_name(name)</code>  <code>async</code>","text":"<p>Find a config by its unique name.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def find_by_name(self, name: str) -&gt; ConfigOut[DataT] | None:\n    \"\"\"Find a config by its unique name.\"\"\"\n    config = await self.repository.find_by_name(name)\n    if config:\n        return self._to_output_schema(config)\n    return None\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.link_artifact","title":"<code>link_artifact(config_id, artifact_id)</code>  <code>async</code>","text":"<p>Link a config to a root artifact.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def link_artifact(self, config_id: ULID, artifact_id: ULID) -&gt; None:\n    \"\"\"Link a config to a root artifact.\"\"\"\n    await self.repository.link_artifact(config_id, artifact_id)\n    await self.repository.commit()\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.unlink_artifact","title":"<code>unlink_artifact(artifact_id)</code>  <code>async</code>","text":"<p>Unlink an artifact from its config.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def unlink_artifact(self, artifact_id: ULID) -&gt; None:\n    \"\"\"Unlink an artifact from its config.\"\"\"\n    await self.repository.unlink_artifact(artifact_id)\n    await self.repository.commit()\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.get_config_for_artifact","title":"<code>get_config_for_artifact(artifact_id, artifact_repo)</code>  <code>async</code>","text":"<p>Get the config for an artifact by traversing to its root.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def get_config_for_artifact(\n    self, artifact_id: ULID, artifact_repo: ArtifactRepository\n) -&gt; ConfigOut[DataT] | None:\n    \"\"\"Get the config for an artifact by traversing to its root.\"\"\"\n    root = await artifact_repo.get_root_artifact(artifact_id)\n    if root is None:\n        return None\n\n    config = await self.repository.find_by_root_artifact_id(root.id)\n    if config is None:\n        return None\n\n    return self._to_output_schema(config)\n</code></pre>"},{"location":"api-reference/#chapkit.config.manager.ConfigManager.get_linked_artifacts","title":"<code>get_linked_artifacts(config_id)</code>  <code>async</code>","text":"<p>Get all root artifacts linked to a config.</p> Source code in <code>src/chapkit/config/manager.py</code> <pre><code>async def get_linked_artifacts(self, config_id: ULID) -&gt; list[ArtifactOut]:\n    \"\"\"Get all root artifacts linked to a config.\"\"\"\n    artifacts = await self.repository.find_artifacts_for_config(config_id)\n    return [ArtifactOut.model_validate(artifact, from_attributes=True) for artifact in artifacts]\n</code></pre>"},{"location":"api-reference/#router_2","title":"Router","text":""},{"location":"api-reference/#chapkit.config.router","title":"<code>router</code>","text":"<p>Config CRUD router with artifact linking operations.</p>"},{"location":"api-reference/#chapkit.config.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.config.router.ConfigRouter","title":"<code>ConfigRouter</code>","text":"<p>               Bases: <code>CrudRouter[ConfigIn[BaseConfig], ConfigOut[BaseConfig]]</code></p> <p>CRUD router for Config entities with artifact linking operations.</p> Source code in <code>src/chapkit/config/router.py</code> <pre><code>class ConfigRouter(CrudRouter[ConfigIn[BaseConfig], ConfigOut[BaseConfig]]):\n    \"\"\"CRUD router for Config entities with artifact linking operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: Sequence[str],\n        manager_factory: Any,\n        entity_in_type: type[ConfigIn[BaseConfig]],\n        entity_out_type: type[ConfigOut[BaseConfig]],\n        permissions: CrudPermissions | None = None,\n        enable_artifact_operations: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize config router with entity types and manager factory.\"\"\"\n        self.enable_artifact_operations = enable_artifact_operations\n        super().__init__(\n            prefix=prefix,\n            tags=list(tags),\n            entity_in_type=entity_in_type,\n            entity_out_type=entity_out_type,\n            manager_factory=manager_factory,\n            permissions=permissions,\n            **kwargs,\n        )\n\n    def _register_schema_route(self) -&gt; None:\n        \"\"\"Register JSON schema endpoint for the config data type only.\"\"\"\n        entity_out_type = self.entity_out_type\n\n        async def get_schema() -&gt; dict[str, Any]:\n            \"\"\"Return the config schema (data field) instead of the full ConfigOut schema.\"\"\"\n            full_schema = entity_out_type.model_json_schema()\n\n            # Extract the config schema from the data field's $ref\n            if \"$defs\" in full_schema and \"data\" in full_schema.get(\"properties\", {}):\n                data_prop = full_schema[\"properties\"][\"data\"]\n                if \"$ref\" in data_prop:\n                    # Extract schema name from $ref (e.g., \"#/$defs/DiseaseConfig\")\n                    ref_name = data_prop[\"$ref\"].split(\"/\")[-1]\n                    if ref_name in full_schema[\"$defs\"]:\n                        return full_schema[\"$defs\"][ref_name]\n\n            # Fallback to full schema if extraction fails\n            return full_schema\n\n        self.register_collection_operation(\n            name=\"schema\",\n            handler=get_schema,\n            http_method=\"GET\",\n            response_model=dict[str, Any],\n        )\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register config CRUD routes and artifact linking operations.\"\"\"\n        super()._register_routes()\n\n        if not self.enable_artifact_operations:\n            return\n\n        manager_factory = self.manager_factory\n\n        async def link_artifact(\n            entity_id: str,\n            request: LinkArtifactRequest,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; None:\n            config_id = self._parse_ulid(entity_id)\n\n            try:\n                await manager.link_artifact(config_id, request.artifact_id)\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n\n        async def unlink_artifact(\n            entity_id: str,\n            request: UnlinkArtifactRequest,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; None:\n            try:\n                await manager.unlink_artifact(request.artifact_id)\n            except Exception as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n\n        async def get_linked_artifacts(\n            entity_id: str,\n            manager: ConfigManager[BaseConfig] = Depends(manager_factory),\n        ) -&gt; list[ArtifactOut]:\n            config_id = self._parse_ulid(entity_id)\n            return await manager.get_linked_artifacts(config_id)\n\n        self.register_entity_operation(\n            \"link-artifact\",\n            link_artifact,\n            http_method=\"POST\",\n            status_code=status.HTTP_204_NO_CONTENT,\n            summary=\"Link artifact to config\",\n            description=\"Link a config to a root artifact (parent_id IS NULL)\",\n        )\n\n        self.register_entity_operation(\n            \"unlink-artifact\",\n            unlink_artifact,\n            http_method=\"POST\",\n            status_code=status.HTTP_204_NO_CONTENT,\n            summary=\"Unlink artifact from config\",\n            description=\"Remove the link between a config and an artifact\",\n        )\n\n        self.register_entity_operation(\n            \"artifacts\",\n            get_linked_artifacts,\n            http_method=\"GET\",\n            response_model=list[ArtifactOut],\n            summary=\"Get linked artifacts\",\n            description=\"Get all root artifacts linked to this config\",\n        )\n</code></pre>"},{"location":"api-reference/#chapkit.config.router.ConfigRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.config.router.ConfigRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, entity_in_type, entity_out_type, permissions=None, enable_artifact_operations=False, **kwargs)</code>","text":"<p>Initialize config router with entity types and manager factory.</p> Source code in <code>src/chapkit/config/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: Sequence[str],\n    manager_factory: Any,\n    entity_in_type: type[ConfigIn[BaseConfig]],\n    entity_out_type: type[ConfigOut[BaseConfig]],\n    permissions: CrudPermissions | None = None,\n    enable_artifact_operations: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize config router with entity types and manager factory.\"\"\"\n    self.enable_artifact_operations = enable_artifact_operations\n    super().__init__(\n        prefix=prefix,\n        tags=list(tags),\n        entity_in_type=entity_in_type,\n        entity_out_type=entity_out_type,\n        manager_factory=manager_factory,\n        permissions=permissions,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api-reference/#ml-module","title":"ML Module","text":"<p>Train/predict workflows with artifact-based model storage and timing metadata.</p>"},{"location":"api-reference/#schemas_3","title":"Schemas","text":""},{"location":"api-reference/#chapkit.ml.schemas","title":"<code>schemas</code>","text":"<p>Pydantic schemas for ML train/predict operations.</p> Migration Note <p>TrainedModelArtifactData and PredictionArtifactData have been replaced by MLTrainingWorkspaceArtifactData and MLPredictionArtifactData from chapkit.artifact.schemas.</p> <p>Key changes: - ml_type field renamed to type - model field moved to content - predictions field moved to content - Added nested metadata structure - Added content_type and content_size fields - Removed training_artifact_id (use parent_id instead) - Removed model_type and model_size_bytes (metadata only)</p>"},{"location":"api-reference/#chapkit.ml.schemas-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.schemas.MLPredictionArtifactData","title":"<code>MLPredictionArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[MLMetadata]</code></p> <p>Schema for ML prediction artifact data with results.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLPredictionArtifactData(BaseArtifactData[MLMetadata]):\n    \"\"\"Schema for ML prediction artifact data with results.\"\"\"\n\n    type: Literal[\"ml_prediction\"] = Field(default=\"ml_prediction\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: MLMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.MLTrainingWorkspaceArtifactData","title":"<code>MLTrainingWorkspaceArtifactData</code>","text":"<p>               Bases: <code>BaseArtifactData[MLMetadata]</code></p> <p>Schema for ML training workspace artifact data.</p> Source code in <code>src/chapkit/artifact/schemas.py</code> <pre><code>class MLTrainingWorkspaceArtifactData(BaseArtifactData[MLMetadata]):\n    \"\"\"Schema for ML training workspace artifact data.\"\"\"\n\n    type: Literal[\"ml_training_workspace\"] = Field(default=\"ml_training_workspace\", frozen=True)  # pyright: ignore[reportIncompatibleVariableOverride]\n    metadata: MLMetadata\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.TrainRequest","title":"<code>TrainRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for training a model.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class TrainRequest(BaseModel):\n    \"\"\"Request schema for training a model.\"\"\"\n\n    config_id: ULID = Field(description=\"ID of the config to use for training\")\n    data: DataFrame = Field(description=\"Training data as DataFrame\")\n    geo: FeatureCollection | None = Field(default=None, description=\"Optional geospatial data\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.TrainResponse","title":"<code>TrainResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for train operation submission.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class TrainResponse(BaseModel):\n    \"\"\"Response schema for train operation submission.\"\"\"\n\n    job_id: str = Field(description=\"ID of the training job in the scheduler\")\n    artifact_id: str = Field(description=\"ID that will contain the trained model artifact\")\n    message: str = Field(description=\"Human-readable message\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.PredictRequest","title":"<code>PredictRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request schema for making predictions.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class PredictRequest(BaseModel):\n    \"\"\"Request schema for making predictions.\"\"\"\n\n    artifact_id: ULID = Field(description=\"ID of the artifact containing the trained model\")\n    historic: DataFrame = Field(description=\"Historic data as DataFrame\")\n    future: DataFrame = Field(description=\"Future/prediction data as DataFrame\")\n    geo: FeatureCollection | None = Field(default=None, description=\"Optional geospatial data\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.PredictResponse","title":"<code>PredictResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response schema for predict operation submission.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class PredictResponse(BaseModel):\n    \"\"\"Response schema for predict operation submission.\"\"\"\n\n    job_id: str = Field(description=\"ID of the prediction job in the scheduler\")\n    artifact_id: str = Field(description=\"ID that will contain the prediction artifact\")\n    message: str = Field(description=\"Human-readable message\")\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol","title":"<code>ModelRunnerProtocol</code>","text":"<p>               Bases: <code>Protocol[ConfigT]</code></p> <p>Protocol defining the interface for model runners.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>class ModelRunnerProtocol(Protocol[ConfigT]):\n    \"\"\"Protocol defining the interface for model runners.\"\"\"\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n        ...\n\n    async def create_training_artifact(\n        self,\n        training_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact data structure from training result.\"\"\"\n        ...\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Make predictions using a trained model and return predictions.\"\"\"\n        ...\n\n    async def create_prediction_artifact(\n        self,\n        prediction_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact data structure from prediction result.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model and return the trained model object (must be pickleable).</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.create_training_artifact","title":"<code>create_training_artifact(training_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact data structure from training result.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def create_training_artifact(\n    self,\n    training_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact data structure from training result.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions using a trained model and return predictions.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Make predictions using a trained model and return predictions.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.schemas.ModelRunnerProtocol.create_prediction_artifact","title":"<code>create_prediction_artifact(prediction_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact data structure from prediction result.</p> Source code in <code>src/chapkit/ml/schemas.py</code> <pre><code>async def create_prediction_artifact(\n    self,\n    prediction_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact data structure from prediction result.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#manager_2","title":"Manager","text":""},{"location":"api-reference/#chapkit.ml.manager","title":"<code>manager</code>","text":"<p>Manager for ML train/predict operations with artifact-based storage.</p>"},{"location":"api-reference/#chapkit.ml.manager-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.manager.MLManager","title":"<code>MLManager</code>","text":"<p>               Bases: <code>Generic[ConfigT]</code></p> <p>Manager for ML train/predict operations with job scheduling and artifact storage.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>class MLManager(Generic[ConfigT]):\n    \"\"\"Manager for ML train/predict operations with job scheduling and artifact storage.\"\"\"\n\n    def __init__(\n        self,\n        runner: ModelRunnerProtocol[ConfigT],\n        scheduler: ChapkitScheduler,\n        database: Database,\n        config_schema: type[ConfigT],\n        min_prediction_periods: int = 0,\n        max_prediction_periods: int = 100,\n    ) -&gt; None:\n        \"\"\"Initialize ML manager with runner, scheduler, database, and config schema.\"\"\"\n        self.runner = runner\n        self.scheduler = scheduler\n        self.database = database\n        self.config_schema = config_schema\n        self.min_prediction_periods = min_prediction_periods\n        self.max_prediction_periods = max_prediction_periods\n\n    def _validate_prediction_periods(self, config_data: BaseConfig) -&gt; None:\n        \"\"\"Validate that config prediction_periods is within allowed bounds.\"\"\"\n        periods = config_data.prediction_periods\n        if periods &lt; self.min_prediction_periods:\n            raise ValueError(\n                f\"prediction_periods ({periods}) is below the minimum allowed value ({self.min_prediction_periods})\"\n            )\n        if periods &gt; self.max_prediction_periods:\n            raise ValueError(\n                f\"prediction_periods ({periods}) exceeds the maximum allowed value ({self.max_prediction_periods})\"\n            )\n\n    async def execute_train(self, request: TrainRequest) -&gt; TrainResponse:\n        \"\"\"Submit a training job to the scheduler and return job/artifact IDs.\"\"\"\n        # Pre-allocate artifact ID for the trained model\n        artifact_id = ULID()\n\n        # Submit job to scheduler\n        job_id = await self.scheduler.add_job(\n            self._train_task,\n            request,\n            artifact_id,\n        )\n\n        return TrainResponse(\n            job_id=str(job_id),\n            artifact_id=str(artifact_id),\n            message=f\"Training job submitted. Job ID: {job_id}\",\n        )\n\n    async def execute_predict(self, request: PredictRequest) -&gt; PredictResponse:\n        \"\"\"Submit a prediction job to the scheduler and return job/artifact IDs.\"\"\"\n        # Pre-allocate artifact ID for predictions\n        artifact_id = ULID()\n\n        # Submit job to scheduler\n        job_id = await self.scheduler.add_job(\n            self._predict_task,\n            request,\n            artifact_id,\n        )\n\n        return PredictResponse(\n            job_id=str(job_id),\n            artifact_id=str(artifact_id),\n            message=f\"Prediction job submitted. Job ID: {job_id}\",\n        )\n\n    async def _train_task(self, request: TrainRequest, artifact_id: ULID) -&gt; ULID:\n        \"\"\"Execute training task and store trained model in artifact.\"\"\"\n        # Load config\n        async with self.database.session() as session:\n            config_repo = ConfigRepository(session)\n            config_manager: ConfigManager[ConfigT] = ConfigManager(config_repo, self.config_schema)\n            config = await config_manager.find_by_id(request.config_id)\n\n            if config is None:\n                raise ValueError(f\"Config {request.config_id} not found\")\n\n            self._validate_prediction_periods(config.data)\n\n        # Train model with timing\n        training_started_at = datetime.datetime.now(datetime.UTC)\n        training_result = await self.runner.on_train(\n            config=config.data,\n            data=request.data,\n            geo=request.geo,\n        )\n        training_completed_at = datetime.datetime.now(datetime.UTC)\n        training_duration = (training_completed_at - training_started_at).total_seconds()\n\n        # Extract workspace_dir before try block (for cleanup in finally)\n        workspace_dir_str = training_result.get(\"workspace_dir\") if isinstance(training_result, dict) else None\n        workspace_dir = Path(workspace_dir_str) if workspace_dir_str else None\n\n        try:\n            # Let runner create artifact structure\n            artifact_data_dict = await self.runner.create_training_artifact(\n                training_result=training_result,\n                config_id=str(request.config_id),\n                started_at=training_started_at,\n                completed_at=training_completed_at,\n                duration_seconds=round(training_duration, 2),\n            )\n\n            # Validate artifact structure with Pydantic\n            from chapkit.artifact.schemas import MLTrainingWorkspaceArtifactData\n\n            MLTrainingWorkspaceArtifactData.model_validate(artifact_data_dict)\n\n            # Store artifact\n            async with self.database.session() as session:\n                artifact_repo = ArtifactRepository(session)\n                artifact_manager = ArtifactManager(artifact_repo)\n                config_repo = ConfigRepository(session)\n\n                await artifact_manager.save(\n                    ArtifactIn(\n                        id=artifact_id,\n                        data=artifact_data_dict,  # Use dict directly (PickleType)\n                        parent_id=None,\n                        level=0,\n                    )\n                )\n\n                # Link config to root artifact for tree traversal\n                await config_repo.link_artifact(request.config_id, artifact_id)\n                await config_repo.commit()\n\n        finally:\n            # Cleanup workspace if created by ShellModelRunner\n            if workspace_dir and workspace_dir.exists():\n                shutil.rmtree(workspace_dir, ignore_errors=True)\n\n        return artifact_id\n\n    async def _predict_task(self, request: PredictRequest, artifact_id: ULID) -&gt; ULID:\n        \"\"\"Execute prediction task and store predictions in artifact.\"\"\"\n        # Load training artifact\n        async with self.database.session() as session:\n            artifact_repo = ArtifactRepository(session)\n            artifact_manager = ArtifactManager(artifact_repo)\n            training_artifact = await artifact_manager.find_by_id(request.artifact_id)\n\n            if training_artifact is None:\n                raise ValueError(f\"Training artifact {request.artifact_id} not found\")\n\n        # Extract model and config_id from artifact\n        training_data = training_artifact.data\n        if not isinstance(training_data, dict) or training_data.get(\"type\") != \"ml_training_workspace\":\n            raise ValueError(f\"Artifact {request.artifact_id} is not a training artifact\")\n\n        # Check training status - block prediction on failed training\n        training_metadata = training_data.get(\"metadata\", {})\n        training_status = training_metadata.get(\"status\", \"unknown\")\n\n        if training_status == \"failed\":\n            exit_code = training_metadata.get(\"exit_code\", \"unknown\")\n            raise ValueError(\n                f\"Cannot predict using failed training artifact {request.artifact_id}. \"\n                f\"Training script exited with code {exit_code}.\"\n            )\n\n        # Check if artifact is workspace (ZIP) or pickled model\n        is_workspace = training_data.get(\"content_type\") == \"application/zip\"\n        extracted_workspace = None\n        prediction_workspace_dir = None\n\n        try:\n            if is_workspace:\n                # Extract workspace from zip\n                import pickle\n                import tempfile\n                import zipfile\n                from io import BytesIO\n\n                workspace_content = training_data[\"content\"]\n                extracted_workspace = Path(tempfile.mkdtemp(prefix=\"chapkit_workspace_extract_\"))\n\n                # Extract zip to temp directory\n                zip_buffer = BytesIO(workspace_content)\n                with zipfile.ZipFile(zip_buffer, \"r\") as zf:\n                    zf.extractall(extracted_workspace)\n\n                # Determine how to pass model based on runner type\n                from .runner import ShellModelRunner\n\n                if isinstance(self.runner, ShellModelRunner):\n                    # ShellModelRunner: pass workspace directory to runner\n                    trained_model = {\n                        \"workspace_dir\": str(extracted_workspace),\n                    }\n                else:\n                    # FunctionalModelRunner with workspace: load pickled model from workspace\n                    model_pickle_path = extracted_workspace / \"model.pickle\"\n                    if model_pickle_path.exists():\n                        try:\n                            trained_model = pickle.loads(model_pickle_path.read_bytes())\n                        except (pickle.UnpicklingError, EOFError, TypeError) as e:\n                            raise ValueError(\n                                f\"Failed to load model from {model_pickle_path}: \"\n                                f\"corrupted or incompatible pickle file. {e}\"\n                            ) from e\n                    else:\n                        raise ValueError(\n                            f\"Training artifact workspace missing model.pickle file at {model_pickle_path}\"\n                        )\n            else:\n                # Pickled model handling (FunctionalModelRunner without workspace)\n                trained_model = training_data[\"content\"]\n\n            config_id = ULID.from_str(training_metadata[\"config_id\"])\n\n            # Load config\n            async with self.database.session() as session:\n                config_repo = ConfigRepository(session)\n                config_manager: ConfigManager[ConfigT] = ConfigManager(config_repo, self.config_schema)\n                config = await config_manager.find_by_id(config_id)\n\n                if config is None:\n                    raise ValueError(f\"Config {config_id} not found\")\n\n                self._validate_prediction_periods(config.data)\n\n            # Make predictions with timing\n            prediction_started_at = datetime.datetime.now(datetime.UTC)\n            prediction_result = await self.runner.on_predict(\n                config=config.data,\n                model=trained_model,\n                historic=request.historic,\n                future=request.future,\n                geo=request.geo,\n            )\n            prediction_completed_at = datetime.datetime.now(datetime.UTC)\n            prediction_duration = (prediction_completed_at - prediction_started_at).total_seconds()\n\n            # Extract predictions from result (handles both new dict format and legacy DataFrame)\n            if isinstance(prediction_result, dict) and \"content\" in prediction_result:\n                # New unified format: {content, workspace_dir, exit_code, stdout, stderr}\n                predictions = prediction_result[\"content\"]\n                workspace_dir_str = prediction_result.get(\"workspace_dir\")\n            else:\n                # Legacy format: BaseModelRunner subclasses return DataFrame directly\n                predictions = prediction_result\n                workspace_dir_str = None\n\n            # Create workspace artifact if workspace was created (both runners can produce workspace)\n            if workspace_dir_str:\n                prediction_workspace_dir = Path(workspace_dir_str)\n\n                # Create workspace artifact data (zips workspace)\n                # Runner returns ml_prediction_workspace type for debugging artifacts\n                workspace_artifact_dict = await self.runner.create_prediction_artifact(\n                    prediction_result=prediction_result,\n                    config_id=str(config_id),\n                    started_at=prediction_started_at,\n                    completed_at=prediction_completed_at,\n                    duration_seconds=round(prediction_duration, 2),\n                )\n            else:\n                workspace_artifact_dict = None\n\n            # Create prediction artifact with DataFrame (same for both runners)\n            from chapkit.artifact.schemas import MLMetadata, MLPredictionArtifactData\n\n            metadata = MLMetadata(\n                status=\"success\",\n                config_id=str(config_id),\n                started_at=prediction_started_at.isoformat(),\n                completed_at=prediction_completed_at.isoformat(),\n                duration_seconds=round(prediction_duration, 2),\n            )\n\n            # Create and validate artifact data structure with Pydantic\n            artifact_data_model = MLPredictionArtifactData(\n                type=\"ml_prediction\",\n                metadata=metadata,\n                content=predictions,\n                content_type=\"application/vnd.chapkit.dataframe+json\",\n                content_size=None,\n            )\n\n            # Construct dict manually to preserve Python objects (database uses PickleType)\n            artifact_data_dict = {\n                \"type\": artifact_data_model.type,\n                \"metadata\": artifact_data_model.metadata.model_dump(),\n                \"content\": predictions,  # Keep as Python object (DataFrame)\n                \"content_type\": artifact_data_model.content_type,\n                \"content_size\": artifact_data_model.content_size,\n            }\n\n            # Store artifacts\n            async with self.database.session() as session:\n                artifact_repo = ArtifactRepository(session)\n                artifact_manager = ArtifactManager(artifact_repo)\n\n                # 1. Store prediction artifact (level 1, parent = training artifact)\n                await artifact_manager.save(\n                    ArtifactIn(\n                        id=artifact_id,\n                        data=artifact_data_dict,\n                        parent_id=request.artifact_id,\n                        level=1,\n                    )\n                )\n\n                # 2. Store workspace artifact if workspace was created (level 2, parent = prediction artifact)\n                if workspace_artifact_dict is not None:\n                    workspace_artifact_id = ULID()\n                    await artifact_manager.save(\n                        ArtifactIn(\n                            id=workspace_artifact_id,\n                            data=workspace_artifact_dict,\n                            parent_id=artifact_id,\n                            level=2,\n                        )\n                    )\n\n        finally:\n            # Cleanup extracted training workspace\n            if extracted_workspace and extracted_workspace.exists():\n                shutil.rmtree(extracted_workspace, ignore_errors=True)\n            # Cleanup prediction workspace (created by ShellModelRunner)\n            if prediction_workspace_dir and prediction_workspace_dir.exists():\n                shutil.rmtree(prediction_workspace_dir, ignore_errors=True)\n\n        return artifact_id\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.manager.MLManager.__init__","title":"<code>__init__(runner, scheduler, database, config_schema, min_prediction_periods=0, max_prediction_periods=100)</code>","text":"<p>Initialize ML manager with runner, scheduler, database, and config schema.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>def __init__(\n    self,\n    runner: ModelRunnerProtocol[ConfigT],\n    scheduler: ChapkitScheduler,\n    database: Database,\n    config_schema: type[ConfigT],\n    min_prediction_periods: int = 0,\n    max_prediction_periods: int = 100,\n) -&gt; None:\n    \"\"\"Initialize ML manager with runner, scheduler, database, and config schema.\"\"\"\n    self.runner = runner\n    self.scheduler = scheduler\n    self.database = database\n    self.config_schema = config_schema\n    self.min_prediction_periods = min_prediction_periods\n    self.max_prediction_periods = max_prediction_periods\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager.execute_train","title":"<code>execute_train(request)</code>  <code>async</code>","text":"<p>Submit a training job to the scheduler and return job/artifact IDs.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>async def execute_train(self, request: TrainRequest) -&gt; TrainResponse:\n    \"\"\"Submit a training job to the scheduler and return job/artifact IDs.\"\"\"\n    # Pre-allocate artifact ID for the trained model\n    artifact_id = ULID()\n\n    # Submit job to scheduler\n    job_id = await self.scheduler.add_job(\n        self._train_task,\n        request,\n        artifact_id,\n    )\n\n    return TrainResponse(\n        job_id=str(job_id),\n        artifact_id=str(artifact_id),\n        message=f\"Training job submitted. Job ID: {job_id}\",\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.manager.MLManager.execute_predict","title":"<code>execute_predict(request)</code>  <code>async</code>","text":"<p>Submit a prediction job to the scheduler and return job/artifact IDs.</p> Source code in <code>src/chapkit/ml/manager.py</code> <pre><code>async def execute_predict(self, request: PredictRequest) -&gt; PredictResponse:\n    \"\"\"Submit a prediction job to the scheduler and return job/artifact IDs.\"\"\"\n    # Pre-allocate artifact ID for predictions\n    artifact_id = ULID()\n\n    # Submit job to scheduler\n    job_id = await self.scheduler.add_job(\n        self._predict_task,\n        request,\n        artifact_id,\n    )\n\n    return PredictResponse(\n        job_id=str(job_id),\n        artifact_id=str(artifact_id),\n        message=f\"Prediction job submitted. Job ID: {job_id}\",\n    )\n</code></pre>"},{"location":"api-reference/#router_3","title":"Router","text":""},{"location":"api-reference/#chapkit.ml.router","title":"<code>router</code>","text":"<p>REST API router for ML train/predict operations.</p>"},{"location":"api-reference/#chapkit.ml.router-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.ml.router.MLRouter","title":"<code>MLRouter</code>","text":"<p>               Bases: <code>Router</code></p> <p>Router with $train and $predict collection operations.</p> Source code in <code>src/chapkit/ml/router.py</code> <pre><code>class MLRouter(Router):\n    \"\"\"Router with $train and $predict collection operations.\"\"\"\n\n    def __init__(\n        self,\n        prefix: str,\n        tags: list[str],\n        manager_factory: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize ML router with manager factory.\"\"\"\n        self.manager_factory = manager_factory\n        super().__init__(prefix=prefix, tags=tags, **kwargs)\n\n    def _register_routes(self) -&gt; None:\n        \"\"\"Register ML train and predict routes.\"\"\"\n        from fastapi import HTTPException\n\n        manager_factory = self.manager_factory\n\n        @self.router.post(\n            \"/$train\",\n            response_model=TrainResponse,\n            status_code=status.HTTP_202_ACCEPTED,\n            summary=\"Train model\",\n            description=\"Submit a training job to the scheduler\",\n        )\n        async def train(\n            request: TrainRequest,\n            manager: MLManager = Depends(manager_factory),\n        ) -&gt; TrainResponse:\n            \"\"\"Train a model asynchronously and return job/artifact IDs.\"\"\"\n            try:\n                response = await manager.execute_train(request)\n                train_counter, _ = _get_counters()\n                train_counter.add(1)\n                return response\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n            except RuntimeError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=str(e),\n                )\n\n        @self.router.post(\n            \"/$predict\",\n            response_model=PredictResponse,\n            status_code=status.HTTP_202_ACCEPTED,\n            summary=\"Make predictions\",\n            description=\"Submit a prediction job to the scheduler\",\n        )\n        async def predict(\n            request: PredictRequest,\n            manager: MLManager = Depends(manager_factory),\n        ) -&gt; PredictResponse:\n            \"\"\"Make predictions asynchronously and return job/artifact IDs.\"\"\"\n            try:\n                response = await manager.execute_predict(request)\n                _, predict_counter = _get_counters()\n                predict_counter.add(1)\n                return response\n            except ValueError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_400_BAD_REQUEST,\n                    detail=str(e),\n                )\n            except RuntimeError as e:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=str(e),\n                )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.router.MLRouter-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.router.MLRouter.__init__","title":"<code>__init__(prefix, tags, manager_factory, **kwargs)</code>","text":"<p>Initialize ML router with manager factory.</p> Source code in <code>src/chapkit/ml/router.py</code> <pre><code>def __init__(\n    self,\n    prefix: str,\n    tags: list[str],\n    manager_factory: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize ML router with manager factory.\"\"\"\n    self.manager_factory = manager_factory\n    super().__init__(prefix=prefix, tags=tags, **kwargs)\n</code></pre>"},{"location":"api-reference/#model-runners","title":"Model Runners","text":"<p>Protocol and implementations for ML model training and prediction.</p>"},{"location":"api-reference/#basemodelrunner","title":"BaseModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner","title":"<code>BaseModelRunner</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[ConfigT]</code></p> <p>Abstract base class for model runners with lifecycle hooks.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class BaseModelRunner(ABC, Generic[ConfigT]):\n    \"\"\"Abstract base class for model runners with lifecycle hooks.\"\"\"\n\n    async def on_init(self) -&gt; None:\n        \"\"\"Optional initialization hook called before training or prediction.\"\"\"\n        pass\n\n    async def on_cleanup(self) -&gt; None:\n        \"\"\"Optional cleanup hook called after training or prediction.\"\"\"\n        pass\n\n    async def create_training_artifact(\n        self,\n        training_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact data structure from training result.\n\n        Default implementation assumes training_result is a pickleable object.\n        Runners can override to customize artifact creation (e.g., workspace zipping).\n\n        Returns dict compatible with MLTrainingWorkspaceArtifactData structure.\n        \"\"\"\n        from chapkit.artifact.schemas import MLMetadata\n\n        metadata = MLMetadata(\n            status=\"success\",\n            config_id=config_id,\n            started_at=started_at.isoformat(),\n            completed_at=completed_at.isoformat(),\n            duration_seconds=duration_seconds,\n        )\n\n        return {\n            \"type\": \"ml_training_workspace\",\n            \"metadata\": metadata.model_dump(),\n            \"content\": training_result,  # Pickled model\n            \"content_type\": \"application/x-pickle\",\n            \"content_size\": None,\n        }\n\n    async def create_prediction_artifact(\n        self,\n        prediction_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact data structure from prediction result.\n\n        Default implementation assumes prediction_result is a DataFrame.\n        Runners can override to customize artifact creation (e.g., workspace zipping).\n\n        Returns dict compatible with MLPredictionArtifactData structure.\n        \"\"\"\n        from chapkit.artifact.schemas import MLMetadata\n\n        metadata = MLMetadata(\n            status=\"success\",\n            config_id=config_id,\n            started_at=started_at.isoformat(),\n            completed_at=completed_at.isoformat(),\n            duration_seconds=duration_seconds,\n        )\n\n        return {\n            \"type\": \"ml_prediction\",\n            \"metadata\": metadata.model_dump(),\n            \"content\": prediction_result,  # DataFrame\n            \"content_type\": \"application/vnd.chapkit.dataframe+json\",\n            \"content_size\": None,\n        }\n\n    @abstractmethod\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n        ...\n\n    @abstractmethod\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; DataFrame:\n        \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n        ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_init","title":"<code>on_init()</code>  <code>async</code>","text":"<p>Optional initialization hook called before training or prediction.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_init(self) -&gt; None:\n    \"\"\"Optional initialization hook called before training or prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_cleanup","title":"<code>on_cleanup()</code>  <code>async</code>","text":"<p>Optional cleanup hook called after training or prediction.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_cleanup(self) -&gt; None:\n    \"\"\"Optional cleanup hook called after training or prediction.\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.create_training_artifact","title":"<code>create_training_artifact(training_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact data structure from training result.</p> <p>Default implementation assumes training_result is a pickleable object. Runners can override to customize artifact creation (e.g., workspace zipping).</p> <p>Returns dict compatible with MLTrainingWorkspaceArtifactData structure.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_training_artifact(\n    self,\n    training_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact data structure from training result.\n\n    Default implementation assumes training_result is a pickleable object.\n    Runners can override to customize artifact creation (e.g., workspace zipping).\n\n    Returns dict compatible with MLTrainingWorkspaceArtifactData structure.\n    \"\"\"\n    from chapkit.artifact.schemas import MLMetadata\n\n    metadata = MLMetadata(\n        status=\"success\",\n        config_id=config_id,\n        started_at=started_at.isoformat(),\n        completed_at=completed_at.isoformat(),\n        duration_seconds=duration_seconds,\n    )\n\n    return {\n        \"type\": \"ml_training_workspace\",\n        \"metadata\": metadata.model_dump(),\n        \"content\": training_result,  # Pickled model\n        \"content_type\": \"application/x-pickle\",\n        \"content_size\": None,\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.create_prediction_artifact","title":"<code>create_prediction_artifact(prediction_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact data structure from prediction result.</p> <p>Default implementation assumes prediction_result is a DataFrame. Runners can override to customize artifact creation (e.g., workspace zipping).</p> <p>Returns dict compatible with MLPredictionArtifactData structure.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_prediction_artifact(\n    self,\n    prediction_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact data structure from prediction result.\n\n    Default implementation assumes prediction_result is a DataFrame.\n    Runners can override to customize artifact creation (e.g., workspace zipping).\n\n    Returns dict compatible with MLPredictionArtifactData structure.\n    \"\"\"\n    from chapkit.artifact.schemas import MLMetadata\n\n    metadata = MLMetadata(\n        status=\"success\",\n        config_id=config_id,\n        started_at=started_at.isoformat(),\n        completed_at=completed_at.isoformat(),\n        duration_seconds=duration_seconds,\n    )\n\n    return {\n        \"type\": \"ml_prediction\",\n        \"metadata\": metadata.model_dump(),\n        \"content\": prediction_result,  # DataFrame\n        \"content_type\": \"application/vnd.chapkit.dataframe+json\",\n        \"content_size\": None,\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Train a model and return the trained model object (must be pickleable).</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>@abstractmethod\nasync def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return the trained model object (must be pickleable).\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.BaseModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Make predictions using a trained model and return predictions as DataFrame.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>@abstractmethod\nasync def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; DataFrame:\n    \"\"\"Make predictions using a trained model and return predictions as DataFrame.\"\"\"\n    ...\n</code></pre>"},{"location":"api-reference/#functionalmodelrunner","title":"FunctionalModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner","title":"<code>FunctionalModelRunner</code>","text":"<p>               Bases: <code>BaseModelRunner[ConfigT]</code></p> <p>Functional model runner wrapping train and predict functions.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class FunctionalModelRunner(BaseModelRunner[ConfigT]):\n    \"\"\"Functional model runner wrapping train and predict functions.\"\"\"\n\n    def __init__(\n        self,\n        on_train: TrainFunction[ConfigT],\n        on_predict: PredictFunction[ConfigT],\n    ) -&gt; None:\n        \"\"\"Initialize functional runner with train and predict functions.\"\"\"\n        self._on_train = on_train\n        self._on_predict = on_predict\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model and return dict with content and workspace.\n\n        Returns:\n            Dict with keys: content (model), workspace_dir, exit_code, stdout, stderr\n        \"\"\"\n        workspace_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_functional_train_\"))\n        # Copy full project directory for reproducibility\n        prepare_workspace(Path.cwd(), workspace_dir)\n        # Write training input files\n        write_training_inputs(workspace_dir, config, data, geo)\n\n        # Execute training function\n        model = await self._on_train(config, data, geo)\n\n        # Write model.pickle\n        (workspace_dir / \"model.pickle\").write_bytes(pickle.dumps(model))\n\n        return {\n            \"content\": model,\n            \"workspace_dir\": str(workspace_dir),\n            \"exit_code\": None,\n            \"stdout\": None,\n            \"stderr\": None,\n        }\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Make predictions and return dict with content and workspace.\n\n        Returns:\n            Dict with keys: content (DataFrame), workspace_dir, exit_code, stdout, stderr\n        \"\"\"\n        workspace_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_functional_predict_\"))\n        # Copy full project directory for reproducibility\n        prepare_workspace(Path.cwd(), workspace_dir)\n        # Write config.yml\n        (workspace_dir / \"config.yml\").write_text(yaml.safe_dump(config.model_dump(), indent=2))\n        # Write prediction input files\n        write_prediction_inputs(workspace_dir, historic, future, geo)\n        # Write model.pickle (input model for prediction)\n        (workspace_dir / \"model.pickle\").write_bytes(pickle.dumps(model))\n\n        # Execute prediction function\n        predictions = await self._on_predict(config, model, historic, future, geo)\n\n        # Write predictions.csv\n        predictions.to_csv(workspace_dir / \"predictions.csv\")\n\n        return {\n            \"content\": predictions,\n            \"workspace_dir\": str(workspace_dir),\n            \"exit_code\": None,\n            \"stdout\": None,\n            \"stderr\": None,\n        }\n\n    async def create_training_artifact(\n        self,\n        training_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact from training result by zipping workspace.\"\"\"\n        # Extract content and workspace from unified result dict\n        if isinstance(training_result, dict) and \"content\" in training_result:\n            workspace_dir = training_result.get(\"workspace_dir\")\n        else:\n            workspace_dir = None\n\n        if not workspace_dir:\n            raise ValueError(\n                \"FunctionalModelRunner.create_training_artifact() requires workspace dict from on_train(). \"\n                \"Got result without workspace_dir.\"\n            )\n\n        # Zip workspace like ShellModelRunner\n        return await self._create_workspace_artifact(\n            workspace_dir=Path(workspace_dir),\n            config_id=config_id,\n            started_at=started_at,\n            completed_at=completed_at,\n            duration_seconds=duration_seconds,\n            artifact_type=\"ml_training_workspace\",\n        )\n\n    async def create_prediction_artifact(\n        self,\n        prediction_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact from prediction result by zipping workspace.\"\"\"\n        # Extract content and workspace from unified result dict\n        if isinstance(prediction_result, dict) and \"content\" in prediction_result:\n            workspace_dir = prediction_result.get(\"workspace_dir\")\n        else:\n            workspace_dir = None\n\n        if not workspace_dir:\n            raise ValueError(\n                \"FunctionalModelRunner.create_prediction_artifact() requires workspace dict from on_predict(). \"\n                \"Got result without workspace_dir.\"\n            )\n\n        # Zip workspace like ShellModelRunner (workspace artifact for debugging)\n        return await self._create_workspace_artifact(\n            workspace_dir=Path(workspace_dir),\n            config_id=config_id,\n            started_at=started_at,\n            completed_at=completed_at,\n            duration_seconds=duration_seconds,\n            artifact_type=\"ml_prediction_workspace\",\n        )\n\n    async def _create_workspace_artifact(\n        self,\n        workspace_dir: Path,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n        artifact_type: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact with workspace zip.\"\"\"\n        workspace_content = zip_workspace(workspace_dir)\n        return create_workspace_artifact(\n            workspace_content=workspace_content,\n            artifact_type=artifact_type,\n            config_id=config_id,\n            started_at=started_at,\n            completed_at=completed_at,\n            duration_seconds=duration_seconds,\n        )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.__init__","title":"<code>__init__(on_train, on_predict)</code>","text":"<p>Initialize functional runner with train and predict functions.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>def __init__(\n    self,\n    on_train: TrainFunction[ConfigT],\n    on_predict: PredictFunction[ConfigT],\n) -&gt; None:\n    \"\"\"Initialize functional runner with train and predict functions.\"\"\"\n    self._on_train = on_train\n    self._on_predict = on_predict\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model and return dict with content and workspace.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Dict with keys: content (model), workspace_dir, exit_code, stdout, stderr</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model and return dict with content and workspace.\n\n    Returns:\n        Dict with keys: content (model), workspace_dir, exit_code, stdout, stderr\n    \"\"\"\n    workspace_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_functional_train_\"))\n    # Copy full project directory for reproducibility\n    prepare_workspace(Path.cwd(), workspace_dir)\n    # Write training input files\n    write_training_inputs(workspace_dir, config, data, geo)\n\n    # Execute training function\n    model = await self._on_train(config, data, geo)\n\n    # Write model.pickle\n    (workspace_dir / \"model.pickle\").write_bytes(pickle.dumps(model))\n\n    return {\n        \"content\": model,\n        \"workspace_dir\": str(workspace_dir),\n        \"exit_code\": None,\n        \"stdout\": None,\n        \"stderr\": None,\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions and return dict with content and workspace.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Dict with keys: content (DataFrame), workspace_dir, exit_code, stdout, stderr</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Make predictions and return dict with content and workspace.\n\n    Returns:\n        Dict with keys: content (DataFrame), workspace_dir, exit_code, stdout, stderr\n    \"\"\"\n    workspace_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_functional_predict_\"))\n    # Copy full project directory for reproducibility\n    prepare_workspace(Path.cwd(), workspace_dir)\n    # Write config.yml\n    (workspace_dir / \"config.yml\").write_text(yaml.safe_dump(config.model_dump(), indent=2))\n    # Write prediction input files\n    write_prediction_inputs(workspace_dir, historic, future, geo)\n    # Write model.pickle (input model for prediction)\n    (workspace_dir / \"model.pickle\").write_bytes(pickle.dumps(model))\n\n    # Execute prediction function\n    predictions = await self._on_predict(config, model, historic, future, geo)\n\n    # Write predictions.csv\n    predictions.to_csv(workspace_dir / \"predictions.csv\")\n\n    return {\n        \"content\": predictions,\n        \"workspace_dir\": str(workspace_dir),\n        \"exit_code\": None,\n        \"stdout\": None,\n        \"stderr\": None,\n    }\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.create_training_artifact","title":"<code>create_training_artifact(training_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact from training result by zipping workspace.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_training_artifact(\n    self,\n    training_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact from training result by zipping workspace.\"\"\"\n    # Extract content and workspace from unified result dict\n    if isinstance(training_result, dict) and \"content\" in training_result:\n        workspace_dir = training_result.get(\"workspace_dir\")\n    else:\n        workspace_dir = None\n\n    if not workspace_dir:\n        raise ValueError(\n            \"FunctionalModelRunner.create_training_artifact() requires workspace dict from on_train(). \"\n            \"Got result without workspace_dir.\"\n        )\n\n    # Zip workspace like ShellModelRunner\n    return await self._create_workspace_artifact(\n        workspace_dir=Path(workspace_dir),\n        config_id=config_id,\n        started_at=started_at,\n        completed_at=completed_at,\n        duration_seconds=duration_seconds,\n        artifact_type=\"ml_training_workspace\",\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.FunctionalModelRunner.create_prediction_artifact","title":"<code>create_prediction_artifact(prediction_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact from prediction result by zipping workspace.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_prediction_artifact(\n    self,\n    prediction_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact from prediction result by zipping workspace.\"\"\"\n    # Extract content and workspace from unified result dict\n    if isinstance(prediction_result, dict) and \"content\" in prediction_result:\n        workspace_dir = prediction_result.get(\"workspace_dir\")\n    else:\n        workspace_dir = None\n\n    if not workspace_dir:\n        raise ValueError(\n            \"FunctionalModelRunner.create_prediction_artifact() requires workspace dict from on_predict(). \"\n            \"Got result without workspace_dir.\"\n        )\n\n    # Zip workspace like ShellModelRunner (workspace artifact for debugging)\n    return await self._create_workspace_artifact(\n        workspace_dir=Path(workspace_dir),\n        config_id=config_id,\n        started_at=started_at,\n        completed_at=completed_at,\n        duration_seconds=duration_seconds,\n        artifact_type=\"ml_prediction_workspace\",\n    )\n</code></pre>"},{"location":"api-reference/#shellmodelrunner","title":"ShellModelRunner","text":""},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner","title":"<code>ShellModelRunner</code>","text":"<p>               Bases: <code>BaseModelRunner[ConfigT]</code></p> <p>Shell-based model runner that executes external scripts for train/predict operations.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>class ShellModelRunner(BaseModelRunner[ConfigT]):\n    \"\"\"Shell-based model runner that executes external scripts for train/predict operations.\"\"\"\n\n    def __init__(\n        self,\n        train_command: str,\n        predict_command: str,\n    ) -&gt; None:\n        \"\"\"Initialize shell runner with full isolation support.\n\n        The runner automatically copies the entire project directory (current working directory)\n        to a temporary workspace, excluding .venv, node_modules, __pycache__, .git, and other\n        build artifacts.\n\n        Args:\n            train_command: Command template for training (use relative paths)\n            predict_command: Command template for prediction (use relative paths)\n        \"\"\"\n        self.train_command = train_command\n        self.predict_command = predict_command\n\n        # Project root is current working directory\n        # Users run: fastapi dev main.py (from project dir)\n        # Docker sets WORKDIR to project root\n        self.project_root = Path.cwd()\n\n        logger.info(\"shell_runner_initialized\", project_root=str(self.project_root))\n\n    async def create_training_artifact(\n        self,\n        training_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact with workspace zip from training result.\"\"\"\n        # Validate training_result is workspace dict from on_train()\n        if not isinstance(training_result, dict) or \"workspace_dir\" not in training_result:\n            raise ValueError(\n                \"ShellModelRunner.create_training_artifact() requires workspace dict from on_train(). \"\n                f\"Got: {type(training_result)}\"\n            )\n\n        # Extract workspace info from training_result dict\n        workspace_dir = Path(training_result[\"workspace_dir\"])\n        exit_code = training_result[\"exit_code\"]\n        stdout = training_result.get(\"stdout\", \"\")\n        stderr = training_result.get(\"stderr\", \"\")\n\n        # Determine status from exit code\n        status: Literal[\"success\", \"failed\"] = \"success\" if exit_code == 0 else \"failed\"\n\n        # Zip workspace and create artifact\n        workspace_content = zip_workspace(workspace_dir)\n        return create_workspace_artifact(\n            workspace_content=workspace_content,\n            artifact_type=\"ml_training_workspace\",\n            config_id=config_id,\n            started_at=started_at,\n            completed_at=completed_at,\n            duration_seconds=duration_seconds,\n            status=status,\n            exit_code=exit_code,\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n    async def on_train(\n        self,\n        config: ConfigT,\n        data: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Train a model by executing external training script (model file creation is optional).\"\"\"\n        temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_train_\"))\n\n        try:\n            # Copy entire project directory to temp workspace for full isolation\n            prepare_workspace(self.project_root, temp_dir)\n            # Write training input files\n            write_training_inputs(temp_dir, config, data, geo)\n\n            # Substitute variables in command (use relative paths)\n            command = self.train_command.format(\n                data_file=\"data.csv\",\n                geo_file=\"geo.json\" if geo else \"\",\n            )\n\n            logger.info(\"executing_train_script\", command=command, temp_dir=str(temp_dir))\n\n            # Execute subprocess with cwd=temp_dir (scripts can now use relative imports!)\n            result = await run_shell(command, cwd=str(temp_dir))\n            stdout = result[\"stdout\"]\n            stderr = result[\"stderr\"]\n            exit_code = result[\"returncode\"]\n\n            if exit_code != 0:\n                logger.error(\"train_script_failed\", exit_code=exit_code, stderr=stderr)\n            else:\n                logger.info(\"train_script_completed\", stdout=stdout, stderr=stderr)\n\n            # Return workspace directory for artifact storage\n            # Workspace preserved for both success and failure (manager will store artifact)\n            return {\n                \"content\": None,  # ShellRunner doesn't have in-memory model\n                \"workspace_dir\": str(temp_dir),\n                \"exit_code\": exit_code,\n                \"stdout\": stdout,\n                \"stderr\": stderr,\n            }\n\n        except Exception:\n            # Cleanup only on Python exception (not script failure)\n            shutil.rmtree(temp_dir, ignore_errors=True)\n            raise\n\n    async def create_prediction_artifact(\n        self,\n        prediction_result: Any,\n        config_id: str,\n        started_at: datetime.datetime,\n        completed_at: datetime.datetime,\n        duration_seconds: float,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Create artifact with workspace zip from prediction result.\"\"\"\n        # Validate prediction_result is workspace dict from on_predict()\n        if not isinstance(prediction_result, dict) or \"workspace_dir\" not in prediction_result:\n            raise ValueError(\n                \"ShellModelRunner.create_prediction_artifact() requires workspace dict from on_predict(). \"\n                f\"Got: {type(prediction_result)}\"\n            )\n\n        # Extract workspace info from prediction_result dict\n        workspace_dir = Path(prediction_result[\"workspace_dir\"])\n        exit_code = prediction_result[\"exit_code\"]\n        stdout = prediction_result.get(\"stdout\", \"\")\n        stderr = prediction_result.get(\"stderr\", \"\")\n\n        # Determine status from exit code\n        status: Literal[\"success\", \"failed\"] = \"success\" if exit_code == 0 else \"failed\"\n\n        # Zip workspace and create artifact\n        workspace_content = zip_workspace(workspace_dir)\n        return create_workspace_artifact(\n            workspace_content=workspace_content,\n            artifact_type=\"ml_prediction_workspace\",\n            config_id=config_id,\n            started_at=started_at,\n            completed_at=completed_at,\n            duration_seconds=duration_seconds,\n            status=status,\n            exit_code=exit_code,\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n    async def on_predict(\n        self,\n        config: ConfigT,\n        model: Any,\n        historic: DataFrame,\n        future: DataFrame,\n        geo: FeatureCollection | None = None,\n    ) -&gt; Any:\n        \"\"\"Make predictions by executing external prediction script.\"\"\"\n        temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_predict_\"))\n\n        try:\n            # Model must be workspace artifact from ShellModelRunner.on_train()\n            if not (isinstance(model, dict) and \"workspace_dir\" in model):\n                raise ValueError(\n                    \"ShellModelRunner.on_predict() requires workspace artifact from ShellModelRunner.on_train(). \"\n                    f\"Got: {type(model)}\"\n                )\n\n            # Extract and restore workspace from training artifact\n            workspace_dir = Path(model[\"workspace_dir\"])\n            logger.info(\"predict_using_workspace\", workspace_dir=str(workspace_dir))\n\n            # Copy workspace contents to temp_dir (preserves all training artifacts)\n            shutil.copytree(workspace_dir, temp_dir, dirs_exist_ok=True)\n\n            # Write prediction input files (always fresh for each prediction)\n            write_prediction_inputs(temp_dir, historic, future, geo)\n\n            # Output file path\n            output_file = temp_dir / \"predictions.csv\"\n\n            # Execute prediction command (workspace may contain model files, config, etc.)\n            command = self.predict_command.format(\n                historic_file=\"historic.csv\",\n                future_file=\"future.csv\",\n                output_file=\"predictions.csv\",\n                geo_file=\"geo.json\" if geo else \"\",\n            )\n\n            logger.info(\"executing_predict_script\", command=command, temp_dir=str(temp_dir))\n\n            # Execute subprocess with cwd=temp_dir (scripts can now use relative imports!)\n            result = await run_shell(command, cwd=str(temp_dir))\n            stdout = result[\"stdout\"]\n            stderr = result[\"stderr\"]\n            exit_code = result[\"returncode\"]\n\n            if exit_code != 0:\n                logger.error(\"predict_script_failed\", exit_code=exit_code, stderr=stderr)\n            else:\n                logger.info(\"predict_script_completed\", stdout=stdout, stderr=stderr)\n\n            # Load predictions from file\n            if not output_file.exists():\n                raise RuntimeError(f\"Prediction script did not create output file at {output_file}\")\n\n            predictions = DataFrame.from_csv(output_file)\n\n            # Return workspace directory for artifact storage (like on_train)\n            # Workspace preserved for both success and failure (manager will store artifact)\n            return {\n                \"content\": predictions,  # DataFrame loaded from predictions.csv\n                \"workspace_dir\": str(temp_dir),\n                \"exit_code\": exit_code,\n                \"stdout\": stdout,\n                \"stderr\": stderr,\n            }\n\n        except Exception:\n            # Cleanup only on Python exception (not script failure)\n            shutil.rmtree(temp_dir, ignore_errors=True)\n            raise\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.__init__","title":"<code>__init__(train_command, predict_command)</code>","text":"<p>Initialize shell runner with full isolation support.</p> <p>The runner automatically copies the entire project directory (current working directory) to a temporary workspace, excluding .venv, node_modules, pycache, .git, and other build artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>train_command</code> <code>str</code> <p>Command template for training (use relative paths)</p> required <code>predict_command</code> <code>str</code> <p>Command template for prediction (use relative paths)</p> required Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>def __init__(\n    self,\n    train_command: str,\n    predict_command: str,\n) -&gt; None:\n    \"\"\"Initialize shell runner with full isolation support.\n\n    The runner automatically copies the entire project directory (current working directory)\n    to a temporary workspace, excluding .venv, node_modules, __pycache__, .git, and other\n    build artifacts.\n\n    Args:\n        train_command: Command template for training (use relative paths)\n        predict_command: Command template for prediction (use relative paths)\n    \"\"\"\n    self.train_command = train_command\n    self.predict_command = predict_command\n\n    # Project root is current working directory\n    # Users run: fastapi dev main.py (from project dir)\n    # Docker sets WORKDIR to project root\n    self.project_root = Path.cwd()\n\n    logger.info(\"shell_runner_initialized\", project_root=str(self.project_root))\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.create_training_artifact","title":"<code>create_training_artifact(training_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact with workspace zip from training result.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_training_artifact(\n    self,\n    training_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact with workspace zip from training result.\"\"\"\n    # Validate training_result is workspace dict from on_train()\n    if not isinstance(training_result, dict) or \"workspace_dir\" not in training_result:\n        raise ValueError(\n            \"ShellModelRunner.create_training_artifact() requires workspace dict from on_train(). \"\n            f\"Got: {type(training_result)}\"\n        )\n\n    # Extract workspace info from training_result dict\n    workspace_dir = Path(training_result[\"workspace_dir\"])\n    exit_code = training_result[\"exit_code\"]\n    stdout = training_result.get(\"stdout\", \"\")\n    stderr = training_result.get(\"stderr\", \"\")\n\n    # Determine status from exit code\n    status: Literal[\"success\", \"failed\"] = \"success\" if exit_code == 0 else \"failed\"\n\n    # Zip workspace and create artifact\n    workspace_content = zip_workspace(workspace_dir)\n    return create_workspace_artifact(\n        workspace_content=workspace_content,\n        artifact_type=\"ml_training_workspace\",\n        config_id=config_id,\n        started_at=started_at,\n        completed_at=completed_at,\n        duration_seconds=duration_seconds,\n        status=status,\n        exit_code=exit_code,\n        stdout=stdout,\n        stderr=stderr,\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.on_train","title":"<code>on_train(config, data, geo=None)</code>  <code>async</code>","text":"<p>Train a model by executing external training script (model file creation is optional).</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_train(\n    self,\n    config: ConfigT,\n    data: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Train a model by executing external training script (model file creation is optional).\"\"\"\n    temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_train_\"))\n\n    try:\n        # Copy entire project directory to temp workspace for full isolation\n        prepare_workspace(self.project_root, temp_dir)\n        # Write training input files\n        write_training_inputs(temp_dir, config, data, geo)\n\n        # Substitute variables in command (use relative paths)\n        command = self.train_command.format(\n            data_file=\"data.csv\",\n            geo_file=\"geo.json\" if geo else \"\",\n        )\n\n        logger.info(\"executing_train_script\", command=command, temp_dir=str(temp_dir))\n\n        # Execute subprocess with cwd=temp_dir (scripts can now use relative imports!)\n        result = await run_shell(command, cwd=str(temp_dir))\n        stdout = result[\"stdout\"]\n        stderr = result[\"stderr\"]\n        exit_code = result[\"returncode\"]\n\n        if exit_code != 0:\n            logger.error(\"train_script_failed\", exit_code=exit_code, stderr=stderr)\n        else:\n            logger.info(\"train_script_completed\", stdout=stdout, stderr=stderr)\n\n        # Return workspace directory for artifact storage\n        # Workspace preserved for both success and failure (manager will store artifact)\n        return {\n            \"content\": None,  # ShellRunner doesn't have in-memory model\n            \"workspace_dir\": str(temp_dir),\n            \"exit_code\": exit_code,\n            \"stdout\": stdout,\n            \"stderr\": stderr,\n        }\n\n    except Exception:\n        # Cleanup only on Python exception (not script failure)\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        raise\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.create_prediction_artifact","title":"<code>create_prediction_artifact(prediction_result, config_id, started_at, completed_at, duration_seconds)</code>  <code>async</code>","text":"<p>Create artifact with workspace zip from prediction result.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def create_prediction_artifact(\n    self,\n    prediction_result: Any,\n    config_id: str,\n    started_at: datetime.datetime,\n    completed_at: datetime.datetime,\n    duration_seconds: float,\n) -&gt; dict[str, Any]:\n    \"\"\"Create artifact with workspace zip from prediction result.\"\"\"\n    # Validate prediction_result is workspace dict from on_predict()\n    if not isinstance(prediction_result, dict) or \"workspace_dir\" not in prediction_result:\n        raise ValueError(\n            \"ShellModelRunner.create_prediction_artifact() requires workspace dict from on_predict(). \"\n            f\"Got: {type(prediction_result)}\"\n        )\n\n    # Extract workspace info from prediction_result dict\n    workspace_dir = Path(prediction_result[\"workspace_dir\"])\n    exit_code = prediction_result[\"exit_code\"]\n    stdout = prediction_result.get(\"stdout\", \"\")\n    stderr = prediction_result.get(\"stderr\", \"\")\n\n    # Determine status from exit code\n    status: Literal[\"success\", \"failed\"] = \"success\" if exit_code == 0 else \"failed\"\n\n    # Zip workspace and create artifact\n    workspace_content = zip_workspace(workspace_dir)\n    return create_workspace_artifact(\n        workspace_content=workspace_content,\n        artifact_type=\"ml_prediction_workspace\",\n        config_id=config_id,\n        started_at=started_at,\n        completed_at=completed_at,\n        duration_seconds=duration_seconds,\n        status=status,\n        exit_code=exit_code,\n        stdout=stdout,\n        stderr=stderr,\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.ml.runner.ShellModelRunner.on_predict","title":"<code>on_predict(config, model, historic, future, geo=None)</code>  <code>async</code>","text":"<p>Make predictions by executing external prediction script.</p> Source code in <code>src/chapkit/ml/runner.py</code> <pre><code>async def on_predict(\n    self,\n    config: ConfigT,\n    model: Any,\n    historic: DataFrame,\n    future: DataFrame,\n    geo: FeatureCollection | None = None,\n) -&gt; Any:\n    \"\"\"Make predictions by executing external prediction script.\"\"\"\n    temp_dir = Path(tempfile.mkdtemp(prefix=\"chapkit_ml_predict_\"))\n\n    try:\n        # Model must be workspace artifact from ShellModelRunner.on_train()\n        if not (isinstance(model, dict) and \"workspace_dir\" in model):\n            raise ValueError(\n                \"ShellModelRunner.on_predict() requires workspace artifact from ShellModelRunner.on_train(). \"\n                f\"Got: {type(model)}\"\n            )\n\n        # Extract and restore workspace from training artifact\n        workspace_dir = Path(model[\"workspace_dir\"])\n        logger.info(\"predict_using_workspace\", workspace_dir=str(workspace_dir))\n\n        # Copy workspace contents to temp_dir (preserves all training artifacts)\n        shutil.copytree(workspace_dir, temp_dir, dirs_exist_ok=True)\n\n        # Write prediction input files (always fresh for each prediction)\n        write_prediction_inputs(temp_dir, historic, future, geo)\n\n        # Output file path\n        output_file = temp_dir / \"predictions.csv\"\n\n        # Execute prediction command (workspace may contain model files, config, etc.)\n        command = self.predict_command.format(\n            historic_file=\"historic.csv\",\n            future_file=\"future.csv\",\n            output_file=\"predictions.csv\",\n            geo_file=\"geo.json\" if geo else \"\",\n        )\n\n        logger.info(\"executing_predict_script\", command=command, temp_dir=str(temp_dir))\n\n        # Execute subprocess with cwd=temp_dir (scripts can now use relative imports!)\n        result = await run_shell(command, cwd=str(temp_dir))\n        stdout = result[\"stdout\"]\n        stderr = result[\"stderr\"]\n        exit_code = result[\"returncode\"]\n\n        if exit_code != 0:\n            logger.error(\"predict_script_failed\", exit_code=exit_code, stderr=stderr)\n        else:\n            logger.info(\"predict_script_completed\", stdout=stdout, stderr=stderr)\n\n        # Load predictions from file\n        if not output_file.exists():\n            raise RuntimeError(f\"Prediction script did not create output file at {output_file}\")\n\n        predictions = DataFrame.from_csv(output_file)\n\n        # Return workspace directory for artifact storage (like on_train)\n        # Workspace preserved for both success and failure (manager will store artifact)\n        return {\n            \"content\": predictions,  # DataFrame loaded from predictions.csv\n            \"workspace_dir\": str(temp_dir),\n            \"exit_code\": exit_code,\n            \"stdout\": stdout,\n            \"stderr\": stderr,\n        }\n\n    except Exception:\n        # Cleanup only on Python exception (not script failure)\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        raise\n</code></pre>"},{"location":"api-reference/#scheduler","title":"Scheduler","text":"<p>Chapkit job scheduler with artifact tracking for ML/task workflows.</p>"},{"location":"api-reference/#chapkit.scheduler","title":"<code>scheduler</code>","text":"<p>Chapkit-specific job scheduler with artifact tracking.</p>"},{"location":"api-reference/#chapkit.scheduler-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.scheduler.ChapkitJobRecord","title":"<code>ChapkitJobRecord</code>","text":"<p>               Bases: <code>JobRecord</code></p> <p>Job record extended with artifact_id tracking for ML/task workflows.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>class ChapkitJobRecord(JobRecord):\n    \"\"\"Job record extended with artifact_id tracking for ML/task workflows.\"\"\"\n\n    artifact_id: ULID | None = Field(default=None, description=\"ID of artifact created by job (if job returns a ULID)\")\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitScheduler","title":"<code>ChapkitScheduler</code>","text":"<p>               Bases: <code>InMemoryScheduler</code>, <code>ABC</code></p> <p>Abstract base class for Chapkit job schedulers with artifact tracking.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>class ChapkitScheduler(InMemoryScheduler, ABC):\n    \"\"\"Abstract base class for Chapkit job schedulers with artifact tracking.\"\"\"\n\n    async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n        \"\"\"Get complete job record with artifact_id if available.\"\"\"\n        raise NotImplementedError\n\n    async def list_records(\n        self, *, status_filter: JobStatus | None = None, reverse: bool = False\n    ) -&gt; list[ChapkitJobRecord]:\n        \"\"\"List all job records with optional status filtering.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitScheduler-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.scheduler.ChapkitScheduler.get_record","title":"<code>get_record(job_id)</code>  <code>async</code>","text":"<p>Get complete job record with artifact_id if available.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n    \"\"\"Get complete job record with artifact_id if available.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.ChapkitScheduler.list_records","title":"<code>list_records(*, status_filter=None, reverse=False)</code>  <code>async</code>","text":"<p>List all job records with optional status filtering.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def list_records(\n    self, *, status_filter: JobStatus | None = None, reverse: bool = False\n) -&gt; list[ChapkitJobRecord]:\n    \"\"\"List all job records with optional status filtering.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.InMemoryChapkitScheduler","title":"<code>InMemoryChapkitScheduler</code>","text":"<p>               Bases: <code>ChapkitScheduler</code></p> <p>In-memory scheduler with automatic artifact tracking for jobs that return ULIDs.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>class InMemoryChapkitScheduler(ChapkitScheduler):\n    \"\"\"In-memory scheduler with automatic artifact tracking for jobs that return ULIDs.\"\"\"\n\n    # Override with ChapkitJobRecord type to support artifact_id tracking\n    # dict is invariant, but we always use ChapkitJobRecord in this subclass\n    _records: dict[ULID, ChapkitJobRecord] = PrivateAttr(default_factory=dict)  # type: ignore[assignment]  # pyright: ignore[reportIncompatibleVariableOverride]\n\n    async def add_job(\n        self,\n        target: JobTarget,\n        /,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; ULID:\n        \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n        now = datetime.now(timezone.utc)\n        jid = ULID()\n\n        record = ChapkitJobRecord(\n            id=jid,\n            status=JobStatus.pending,\n            submitted_at=now,\n        )\n\n        async with self._lock:\n            if jid in self._tasks:\n                raise RuntimeError(f\"Job {jid!r} already scheduled\")\n            self._records[jid] = record\n\n        async def _execute_target() -&gt; Any:\n            if inspect.isawaitable(target):\n                if args or kwargs:\n                    # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                    if inspect.iscoroutine(target):\n                        target.close()\n                    raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n                return await target\n            if inspect.iscoroutinefunction(target):\n                return await target(*args, **kwargs)\n            return await asyncio.to_thread(target, *args, **kwargs)\n\n        async def _runner() -&gt; Any:\n            if self._sema:\n                async with self._sema:\n                    return await self._run_with_state(jid, _execute_target)\n            else:\n                return await self._run_with_state(jid, _execute_target)\n\n        task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n        def _drain(t: asyncio.Task[Any]) -&gt; None:\n            try:\n                t.result()\n            except Exception:\n                pass\n\n        task.add_done_callback(_drain)\n\n        async with self._lock:\n            self._tasks[jid] = task\n\n        return jid\n\n    async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n        \"\"\"Get complete job record with artifact_id if available.\"\"\"\n        async with self._lock:\n            if job_id not in self._records:\n                raise KeyError(f\"Job {job_id} not found\")\n            return self._records[job_id]\n\n    async def list_records(\n        self, *, status_filter: JobStatus | None = None, reverse: bool = False\n    ) -&gt; list[ChapkitJobRecord]:\n        \"\"\"List all job records with optional status filtering.\"\"\"\n        async with self._lock:\n            records = list(self._records.values())\n            if status_filter:\n                records = [r for r in records if r.status == status_filter]\n            if reverse:\n                records = list(reversed(records))\n            return records\n\n    async def _run_with_state(\n        self,\n        jid: ULID,\n        exec_fn: JobExecutor,\n    ) -&gt; Any:\n        \"\"\"Execute job function and track artifact_id if result is a ULID.\"\"\"\n        async with self._lock:\n            rec = self._records[jid]\n            rec.status = JobStatus.running\n            rec.started_at = datetime.now(timezone.utc)\n\n        try:\n            result = await exec_fn()\n\n            # Track artifact_id if job returns a ULID\n            artifact_id: ULID | None = result if isinstance(result, ULID) else None\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.completed\n                rec.finished_at = datetime.now(timezone.utc)\n                rec.artifact_id = artifact_id\n                self._results[jid] = result\n\n            return result\n\n        except asyncio.CancelledError:\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.canceled\n                rec.finished_at = datetime.now(timezone.utc)\n\n            raise\n\n        except Exception as e:\n            tb = traceback.format_exc()\n            # Extract clean error message (exception type and message only)\n            error_lines = tb.strip().split(\"\\n\")\n            clean_error = error_lines[-1] if error_lines else str(e)\n\n            async with self._lock:\n                rec = self._records[jid]\n                rec.status = JobStatus.failed\n                rec.finished_at = datetime.now(timezone.utc)\n                rec.error = clean_error\n                rec.error_traceback = tb\n\n            raise\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.InMemoryChapkitScheduler-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.scheduler.InMemoryChapkitScheduler.add_job","title":"<code>add_job(target, /, *args, **kwargs)</code>  <code>async</code>","text":"<p>Add a job to the scheduler and return its ID.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def add_job(\n    self,\n    target: JobTarget,\n    /,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ULID:\n    \"\"\"Add a job to the scheduler and return its ID.\"\"\"\n    now = datetime.now(timezone.utc)\n    jid = ULID()\n\n    record = ChapkitJobRecord(\n        id=jid,\n        status=JobStatus.pending,\n        submitted_at=now,\n    )\n\n    async with self._lock:\n        if jid in self._tasks:\n            raise RuntimeError(f\"Job {jid!r} already scheduled\")\n        self._records[jid] = record\n\n    async def _execute_target() -&gt; Any:\n        if inspect.isawaitable(target):\n            if args or kwargs:\n                # Close the coroutine to avoid \"coroutine was never awaited\" warning\n                if inspect.iscoroutine(target):\n                    target.close()\n                raise TypeError(\"Args/kwargs not supported when target is an awaitable object.\")\n            return await target\n        if inspect.iscoroutinefunction(target):\n            return await target(*args, **kwargs)\n        return await asyncio.to_thread(target, *args, **kwargs)\n\n    async def _runner() -&gt; Any:\n        if self._sema:\n            async with self._sema:\n                return await self._run_with_state(jid, _execute_target)\n        else:\n            return await self._run_with_state(jid, _execute_target)\n\n    task = asyncio.create_task(_runner(), name=f\"{self.name}-job-{jid}\")\n\n    def _drain(t: asyncio.Task[Any]) -&gt; None:\n        try:\n            t.result()\n        except Exception:\n            pass\n\n    task.add_done_callback(_drain)\n\n    async with self._lock:\n        self._tasks[jid] = task\n\n    return jid\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.InMemoryChapkitScheduler.get_record","title":"<code>get_record(job_id)</code>  <code>async</code>","text":"<p>Get complete job record with artifact_id if available.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def get_record(self, job_id: ULID) -&gt; ChapkitJobRecord:\n    \"\"\"Get complete job record with artifact_id if available.\"\"\"\n    async with self._lock:\n        if job_id not in self._records:\n            raise KeyError(f\"Job {job_id} not found\")\n        return self._records[job_id]\n</code></pre>"},{"location":"api-reference/#chapkit.scheduler.InMemoryChapkitScheduler.list_records","title":"<code>list_records(*, status_filter=None, reverse=False)</code>  <code>async</code>","text":"<p>List all job records with optional status filtering.</p> Source code in <code>src/chapkit/scheduler.py</code> <pre><code>async def list_records(\n    self, *, status_filter: JobStatus | None = None, reverse: bool = False\n) -&gt; list[ChapkitJobRecord]:\n    \"\"\"List all job records with optional status filtering.\"\"\"\n    async with self._lock:\n        records = list(self._records.values())\n        if status_filter:\n            records = [r for r in records if r.status == status_filter]\n        if reverse:\n            records = list(reversed(records))\n        return records\n</code></pre>"},{"location":"api-reference/#api-layer","title":"API Layer","text":"<p>FastAPI-specific components built on servicekit.</p>"},{"location":"api-reference/#dependencies","title":"Dependencies","text":"<p>FastAPI dependency injection functions.</p>"},{"location":"api-reference/#chapkit.api.dependencies","title":"<code>dependencies</code>","text":"<p>Feature-specific FastAPI dependency injection for managers.</p>"},{"location":"api-reference/#chapkit.api.dependencies-classes","title":"Classes","text":""},{"location":"api-reference/#chapkit.api.dependencies-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.api.dependencies.get_config_manager","title":"<code>get_config_manager(session)</code>  <code>async</code>","text":"<p>Get a config manager instance for dependency injection.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_config_manager(session: Annotated[AsyncSession, Depends(get_session)]) -&gt; ConfigManager[BaseConfig]:\n    \"\"\"Get a config manager instance for dependency injection.\"\"\"\n    repo = ConfigRepository(session)\n    return ConfigManager[BaseConfig](repo, BaseConfig)\n</code></pre>"},{"location":"api-reference/#chapkit.api.dependencies.get_artifact_manager","title":"<code>get_artifact_manager(session)</code>  <code>async</code>","text":"<p>Get an artifact manager instance for dependency injection.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_artifact_manager(session: Annotated[AsyncSession, Depends(get_session)]) -&gt; ArtifactManager:\n    \"\"\"Get an artifact manager instance for dependency injection.\"\"\"\n    artifact_repo = ArtifactRepository(session)\n    return ArtifactManager(artifact_repo)\n</code></pre>"},{"location":"api-reference/#chapkit.api.dependencies.get_ml_manager","title":"<code>get_ml_manager()</code>  <code>async</code>","text":"<p>Get an ML manager instance for dependency injection.</p> <p>Note: This is a placeholder. The actual dependency is built by ServiceBuilder with the runner in closure, then overridden via app.dependency_overrides.</p> Source code in <code>src/chapkit/api/dependencies.py</code> <pre><code>async def get_ml_manager() -&gt; MLManager:\n    \"\"\"Get an ML manager instance for dependency injection.\n\n    Note: This is a placeholder. The actual dependency is built by ServiceBuilder\n    with the runner in closure, then overridden via app.dependency_overrides.\n    \"\"\"\n    raise RuntimeError(\"ML manager dependency not configured. Use ServiceBuilder.with_ml() to enable ML operations.\")\n</code></pre>"},{"location":"api-reference/#alembic-helpers","title":"Alembic Helpers","text":"<p>Reusable migration helpers for chapkit database tables.</p>"},{"location":"api-reference/#chapkit.alembic_helpers","title":"<code>alembic_helpers</code>","text":"<p>Reusable Alembic migration helpers for chapkit tables.</p> <p>This module provides helper functions for creating and dropping chapkit's database tables in Alembic migrations. Using helpers instead of raw Alembic operations provides:</p> <ul> <li>Reusability across migrations</li> <li>Consistent table definitions</li> <li>Clear documentation</li> <li>Easier maintenance</li> </ul> <p>Users can create their own helper modules following this pattern for custom tables.</p> Example Creating Your Own Helpers <p>Follow the same pattern for your custom tables:</p> <p>See examples/custom_migrations/ for a complete working example.</p>"},{"location":"api-reference/#chapkit.alembic_helpers--in-your-migration-file","title":"In your migration file","text":"<p>from chapkit.alembic_helpers import create_configs_table, drop_configs_table</p> <p>def upgrade() -&gt; None:     create_configs_table(op)</p> <p>def downgrade() -&gt; None:     drop_configs_table(op)</p>"},{"location":"api-reference/#chapkit.alembic_helpers--myappalembic_helperspy","title":"myapp/alembic_helpers.py","text":"<p>def create_users_table(op: Any) -&gt; None:     '''Create users table.'''     op.create_table(         'users',         sa.Column('email', sa.String(), nullable=False),         sa.Column('name', sa.String(), nullable=False),         sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),         sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),         sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),         sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),         sa.PrimaryKeyConstraint('id'),     )     op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)</p> <p>def drop_users_table(op: Any) -&gt; None:     '''Drop users table.'''     op.drop_index(op.f('ix_users_email'), table_name='users')     op.drop_table('users')</p>"},{"location":"api-reference/#chapkit.alembic_helpers-functions","title":"Functions","text":""},{"location":"api-reference/#chapkit.alembic_helpers.create_artifacts_table","title":"<code>create_artifacts_table(op)</code>","text":"<p>Create artifacts table for hierarchical artifact storage.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Create artifacts table for hierarchical artifact storage.\"\"\"\n    op.create_table(\n        \"artifacts\",\n        sa.Column(\"parent_id\", servicekit.types.ULIDType(length=26), nullable=True),\n        sa.Column(\"data\", sa.PickleType(), nullable=False),\n        sa.Column(\"level\", sa.Integer(), nullable=False),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.ForeignKeyConstraint([\"parent_id\"], [\"artifacts.id\"], ondelete=\"SET NULL\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_artifacts_level\"), \"artifacts\", [\"level\"], unique=False)\n    op.create_index(op.f(\"ix_artifacts_parent_id\"), \"artifacts\", [\"parent_id\"], unique=False)\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_artifacts_table","title":"<code>drop_artifacts_table(op)</code>","text":"<p>Drop artifacts table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Drop artifacts table.\"\"\"\n    op.drop_index(op.f(\"ix_artifacts_parent_id\"), table_name=\"artifacts\")\n    op.drop_index(op.f(\"ix_artifacts_level\"), table_name=\"artifacts\")\n    op.drop_table(\"artifacts\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_configs_table","title":"<code>create_configs_table(op)</code>","text":"<p>Create configs table for configuration storage.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_configs_table(op: Any) -&gt; None:\n    \"\"\"Create configs table for configuration storage.\"\"\"\n    op.create_table(\n        \"configs\",\n        sa.Column(\"name\", sa.String(), nullable=False),\n        sa.Column(\"data\", sa.JSON(), nullable=False),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_configs_name\"), \"configs\", [\"name\"], unique=False)\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_configs_table","title":"<code>drop_configs_table(op)</code>","text":"<p>Drop configs table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_configs_table(op: Any) -&gt; None:\n    \"\"\"Drop configs table.\"\"\"\n    op.drop_index(op.f(\"ix_configs_name\"), table_name=\"configs\")\n    op.drop_table(\"configs\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_config_artifacts_table","title":"<code>create_config_artifacts_table(op)</code>","text":"<p>Create config_artifacts junction table linking configs to artifacts.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_config_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Create config_artifacts junction table linking configs to artifacts.\"\"\"\n    op.create_table(\n        \"config_artifacts\",\n        sa.Column(\"config_id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"artifact_id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.ForeignKeyConstraint([\"artifact_id\"], [\"artifacts.id\"], ondelete=\"CASCADE\"),\n        sa.ForeignKeyConstraint([\"config_id\"], [\"configs.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"config_id\", \"artifact_id\"),\n        sa.UniqueConstraint(\"artifact_id\"),\n        sa.UniqueConstraint(\"artifact_id\", name=\"uq_artifact_id\"),\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_config_artifacts_table","title":"<code>drop_config_artifacts_table(op)</code>","text":"<p>Drop config_artifacts junction table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_config_artifacts_table(op: Any) -&gt; None:\n    \"\"\"Drop config_artifacts junction table.\"\"\"\n    op.drop_table(\"config_artifacts\")\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.create_tasks_table","title":"<code>create_tasks_table(op)</code>","text":"<p>Create tasks table for task execution infrastructure.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def create_tasks_table(op: Any) -&gt; None:\n    \"\"\"Create tasks table for task execution infrastructure.\"\"\"\n    op.create_table(\n        \"tasks\",\n        sa.Column(\"command\", sa.Text(), nullable=False),\n        sa.Column(\"task_type\", sa.Text(), nullable=False, server_default=\"shell\"),\n        sa.Column(\"parameters\", sa.JSON(), nullable=True),\n        sa.Column(\"enabled\", sa.Boolean(), nullable=False, server_default=\"1\"),\n        sa.Column(\"id\", servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"updated_at\", sa.DateTime(), server_default=sa.text(\"(CURRENT_TIMESTAMP)\"), nullable=False),\n        sa.Column(\"tags\", sa.JSON(), nullable=False, server_default=\"[]\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n</code></pre>"},{"location":"api-reference/#chapkit.alembic_helpers.drop_tasks_table","title":"<code>drop_tasks_table(op)</code>","text":"<p>Drop tasks table.</p> Source code in <code>src/chapkit/alembic_helpers.py</code> <pre><code>def drop_tasks_table(op: Any) -&gt; None:\n    \"\"\"Drop tasks table.\"\"\"\n    op.drop_table(\"tasks\")\n</code></pre>"},{"location":"guides/artifact-storage/","title":"Artifact Storage","text":"<p>Servicekit provides a hierarchical artifact storage system for managing ML models, datasets, experimental results, document versions, and any other structured data that needs parent-child relationships and tree-based organization.</p>"},{"location":"guides/artifact-storage/#quick-start","title":"Quick Start","text":"<pre><code>from chapkit.artifact import ArtifactHierarchy, ArtifactManager, ArtifactIn, ArtifactRepository\nfrom servicekit.api import BaseServiceBuilder, ServiceInfo\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n# Define artifact hierarchy\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",\n    level_labels={0: \"experiment\", 1: \"model\", 2: \"predictions\"}\n)\n\n# Create dependency for artifact manager\nasync def get_artifact_manager(session: AsyncSession = Depends(get_session)) -&gt; ArtifactManager:\n    return ArtifactManager(ArtifactRepository(session), hierarchy=hierarchy)\n\n# Build service\napp = (\n    BaseServiceBuilder(info=ServiceInfo(id=\"my-service\", display_name=\"My Service\"))\n    .with_health()\n    .with_database(\"sqlite+aiosqlite:///./data.db\")\n    .build()\n)\n\n# Add artifact router\nfrom chapkit.artifact import ArtifactRouter, ArtifactOut\n\nartifact_router = ArtifactRouter.create(\n    prefix=\"/api/v1/artifacts\",\n    tags=[\"Artifacts\"],\n    manager_factory=get_artifact_manager,\n    entity_in_type=ArtifactIn,\n    entity_out_type=ArtifactOut,\n)\napp.include_router(artifact_router)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/artifact-storage/#architecture","title":"Architecture","text":""},{"location":"guides/artifact-storage/#hierarchical-storage","title":"Hierarchical Storage","text":"<p>Artifacts are organized in parent-child trees with configurable level labels:</p> <pre><code>Experiment (level 0, parent_id=None)\n  \u251c\u2500&gt; Model V1 (level 1, parent_id=experiment_id)\n  \u2502    \u251c\u2500&gt; Predictions A (level 2, parent_id=model_v1_id)\n  \u2502    \u2514\u2500&gt; Predictions B (level 2, parent_id=model_v1_id)\n  \u2514\u2500&gt; Model V2 (level 1, parent_id=experiment_id)\n       \u2514\u2500&gt; Predictions C (level 2, parent_id=model_v2_id)\n</code></pre> <p>Benefits: - Complete lineage tracking - Immutable versioning - Multiple children per parent - Flexible hierarchy depths</p>"},{"location":"guides/artifact-storage/#level-labels","title":"Level Labels","text":"<p>Define semantic meaning for each hierarchy level:</p> <pre><code>ArtifactHierarchy(\n    name=\"document_versions\",\n    level_labels={\n        0: \"project\",\n        1: \"document\",\n        2: \"version\"\n    }\n)\n</code></pre>"},{"location":"guides/artifact-storage/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/artifact-storage/#artifacthierarchy","title":"ArtifactHierarchy","text":"<p>Defines the structure and validation rules for artifact trees.</p> <pre><code>from chapkit.artifact import ArtifactHierarchy\n\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",           # Unique identifier\n    level_labels={                # Semantic labels\n        0: \"experiment\",\n        1: \"trained_model\",\n        2: \"predictions\"\n    }\n)\n</code></pre> <p>Properties: - <code>name</code>: Unique identifier for the hierarchy - <code>level_labels</code>: Dict mapping level numbers to semantic labels - Validates parent-child relationships - Enforces level constraints</p>"},{"location":"guides/artifact-storage/#artifact","title":"Artifact","text":"<p>Base entity for hierarchical storage.</p> <pre><code>from chapkit.artifact import ArtifactIn\n\nartifact = ArtifactIn(\n    parent_id=parent_artifact_id,  # Optional: link to parent\n    data={\"key\": \"value\"},         # Any JSON-serializable data\n)\n</code></pre> <p>Fields: - <code>id</code>: ULID (auto-generated) - <code>parent_id</code>: Optional parent artifact ID - <code>level</code>: Hierarchy level (computed from parent) - <code>data</code>: JSON-serializable dictionary (can use typed schemas - see below) - <code>created_at</code>, <code>updated_at</code>: Timestamps</p>"},{"location":"guides/artifact-storage/#typed-artifact-data","title":"Typed Artifact Data","text":"<p>Chapkit provides strongly-typed schemas for artifact data to ensure consistency and enable validation:</p> <pre><code>from chapkit.artifact import (\n    MLTrainingWorkspaceArtifactData,\n    MLPredictionArtifactData,\n    GenericArtifactData,\n    MLMetadata\n)\n\n# ML training artifact with typed metadata\ntraining_data = MLTrainingWorkspaceArtifactData(\n    type=\"ml_training_workspace\",\n    metadata=MLMetadata(\n        status=\"success\",\n        config_id=\"01ABC123...\",\n        started_at=\"2025-10-18T10:00:00Z\",\n        completed_at=\"2025-10-18T10:05:00Z\",\n        duration_seconds=300.0\n    ),\n    content=trained_model,  # Python object (stored as PickleType)\n    content_type=\"application/x-pickle\",\n    content_size=1024\n)\n\n# Validate before saving\nartifact_data = training_data.model_dump()\nartifact = await manager.save(ArtifactIn(data=artifact_data))\n</code></pre> <p>Available Schemas: - <code>MLTrainingWorkspaceArtifactData</code>: For trained ML models with execution metadata - <code>MLPredictionArtifactData</code>: For prediction results with execution metadata - <code>GenericArtifactData</code>: For custom artifacts with flexible metadata</p> <p>Benefits: - Type safety and validation - Consistent metadata structure - Better IDE support and autocomplete - Clear separation of metadata vs content</p>"},{"location":"guides/artifact-storage/#artifactmanager","title":"ArtifactManager","text":"<p>Business logic layer for artifact operations.</p> <pre><code>from chapkit.artifact import ArtifactManager, ArtifactRepository\n\nmanager = ArtifactManager(repository, hierarchy=hierarchy)\n\n# Create root artifact\nroot = await manager.save(ArtifactIn(data={\"experiment\": \"v1\"}))\n\n# Create child artifact\nchild = await manager.save(ArtifactIn(\n    parent_id=root.id,\n    data={\"model\": \"trained\"}\n))\n\n# Query tree\ntree = await manager.build_tree(root.id)\n</code></pre>"},{"location":"guides/artifact-storage/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/artifact-storage/#post-apiv1artifacts","title":"POST /api/v1/artifacts","text":"<p>Create new artifact.</p> <p>Request: <pre><code>{\n  \"parent_id\": \"01PARENT123...\",\n  \"data\": {\n    \"experiment\": \"weather_prediction\",\n    \"version\": \"1.0.0\"\n  }\n}\n</code></pre></p> <p>Response (201 Created): <pre><code>{\n  \"id\": \"01ARTIFACT456...\",\n  \"parent_id\": \"01PARENT123...\",\n  \"level\": 1,\n  \"data\": {\n    \"experiment\": \"weather_prediction\",\n    \"version\": \"1.0.0\"\n  },\n  \"created_at\": \"2025-10-18T12:00:00Z\",\n  \"updated_at\": \"2025-10-18T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifacts","title":"GET /api/v1/artifacts","text":"<p>List all artifacts with pagination.</p> <p>Query Parameters: - <code>page</code>: Page number (default: 1) - <code>size</code>: Page size (default: 50)</p> <p>Response: <pre><code>{\n  \"items\": [...],\n  \"total\": 100,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 2\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifactsid","title":"GET /api/v1/artifacts/{id}","text":"<p>Get artifact by ID.</p>"},{"location":"guides/artifact-storage/#put-apiv1artifactsid","title":"PUT /api/v1/artifacts/{id}","text":"<p>Update artifact data.</p> <p>Request: <pre><code>{\n  \"data\": {\n    \"updated\": \"value\"\n  }\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#delete-apiv1artifactsid","title":"DELETE /api/v1/artifacts/{id}","text":"<p>Delete artifact (fails if artifact has children).</p>"},{"location":"guides/artifact-storage/#post-apiv1artifactstree","title":"POST /api/v1/artifacts/$tree","text":"<p>Get artifact tree structure.</p> <p>Request: <pre><code>{\n  \"root_id\": \"01ARTIFACT456...\",\n  \"max_depth\": 3\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": \"01ARTIFACT456...\",\n  \"level\": 0,\n  \"data\": {...},\n  \"children\": [\n    {\n      \"id\": \"01CHILD789...\",\n      \"level\": 1,\n      \"data\": {...},\n      \"children\": [...]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifactsidexpand","title":"GET /api/v1/artifacts/{id}/$expand","text":"<p>Get artifact with hierarchy metadata but without children array.</p> <p>Response: <pre><code>{\n  \"id\": \"01ARTIFACT456...\",\n  \"parent_id\": null,\n  \"level\": 0,\n  \"level_label\": \"experiment\",\n  \"hierarchy\": \"ml_pipeline\",\n  \"data\": {...},\n  \"created_at\": \"2025-10-18T12:00:00Z\",\n  \"updated_at\": \"2025-10-18T12:00:00Z\"\n}\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifactsiddownload","title":"GET /api/v1/artifacts/{id}/$download","text":"<p>Download artifact content as a binary file.</p> <p>Response Headers: - <code>Content-Type</code>: Indicates format (e.g., <code>application/vnd.chapkit.dataframe+json</code>, <code>text/csv</code>, <code>application/zip</code>) - <code>Content-Disposition</code>: <code>attachment; filename=artifact_{id}.{ext}</code></p> <p>Supported Content Types: - DataFrames: Serialized to JSON (orient=records) or CSV - Binary data: ZIP files, images, pickled models - JSON: Generic dict content</p> <p>Example: <pre><code># Download predictions as JSON\ncurl -O -J http://localhost:8000/api/v1/artifacts/01PRED123.../$download\n\n# Returns: artifact_01PRED123....json with prediction data\n</code></pre></p> <p>Response (DataFrame): <pre><code>[\n  {\"rainfall\": 110.0, \"temperature\": 26.0, \"prediction\": 13.2},\n  {\"rainfall\": 90.0, \"temperature\": 28.0, \"prediction\": 8.5}\n]\n</code></pre></p>"},{"location":"guides/artifact-storage/#get-apiv1artifactsidmetadata","title":"GET /api/v1/artifacts/{id}/$metadata","text":"<p>Get only JSON-serializable metadata without binary content.</p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"config_id\": \"01CONFIG123...\",\n  \"started_at\": \"2025-10-18T10:00:00Z\",\n  \"completed_at\": \"2025-10-18T10:05:00Z\",\n  \"duration_seconds\": 300.0\n}\n</code></pre></p> <p>Use Cases: - Quick metadata inspection without downloading large files - Checking job status and timing information - Retrieving execution metadata for monitoring</p>"},{"location":"guides/artifact-storage/#data-storage-patterns","title":"Data Storage Patterns","text":""},{"location":"guides/artifact-storage/#storing-dataframes","title":"Storing DataFrames","text":"<p>Use <code>DataFrame</code> schema for tabular data:</p> <pre><code>from chapkit.artifact import ArtifactIn\nfrom servicekit.data import DataFrame\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"feature1\": [1.0, 2.0, 3.0],\n    \"feature2\": [4.0, 5.0, 6.0]\n})\n\ndata_frame = DataFrame.from_pandas(df)\n\nartifact = await manager.save(ArtifactIn(\n    data={\"dataframe\": data_frame.model_dump()}\n))\n\n# Retrieve and convert back\nartifact = await manager.find_by_id(artifact_id)\ndf = DataFrame(**artifact.data[\"dataframe\"]).to_pandas()\n</code></pre>"},{"location":"guides/artifact-storage/#storing-binary-data","title":"Storing Binary Data","text":"<p>Store large binary data externally and reference in artifact:</p> <pre><code># Save binary to external storage\nblob_url = await save_to_s3(binary_data)\n\n# Store reference in artifact\nartifact = await manager.save(ArtifactIn(\n    data={\n        \"blob_url\": blob_url,\n        \"blob_size\": len(binary_data),\n        \"blob_type\": \"model/pytorch\"\n    }\n))\n</code></pre>"},{"location":"guides/artifact-storage/#storing-model-objects","title":"Storing Model Objects","text":"<p>Use typed schemas for ML artifacts with PickleType storage:</p> <pre><code>from chapkit.artifact import MLTrainingWorkspaceArtifactData, MLMetadata\nfrom datetime import datetime\n\n# Create typed artifact data\ntraining_data = MLTrainingWorkspaceArtifactData(\n    type=\"ml_training_workspace\",\n    metadata=MLMetadata(\n        status=\"success\",\n        config_id=config_id,\n        started_at=started_at.isoformat(),\n        completed_at=datetime.now().isoformat(),\n        duration_seconds=duration\n    ),\n    content=trained_model,  # Python object stored directly\n    content_type=\"application/x-pickle\"\n)\n\n# Save artifact\nartifact = await manager.save(ArtifactIn(\n    parent_id=parent_id,\n    data=training_data.model_dump()\n))\n\n# Retrieve model\nartifact = await manager.find_by_id(artifact_id)\ntrained_model = artifact.data[\"content\"]  # Python object ready to use\n</code></pre> <p>Advantages: - No manual pickle/unpickle needed - Structured metadata with validation - Type-safe content access - Consistent format across artifacts</p>"},{"location":"guides/artifact-storage/#use-cases","title":"Use Cases","text":""},{"location":"guides/artifact-storage/#ml-model-lineage","title":"ML Model Lineage","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"ml_lineage\",\n    level_labels={\n        0: \"experiment\",\n        1: \"trained_model\",\n        2: \"predictions\"\n    }\n)\n\n# Track experiment\nexperiment = await manager.save(ArtifactIn(\n    data={\n        \"experiment_name\": \"weather_forecast\",\n        \"started_at\": \"2025-10-18T10:00:00Z\"\n    }\n))\n\n# Track trained model\nmodel = await manager.save(ArtifactIn(\n    parent_id=experiment.id,\n    data={\n        \"model_type\": \"LinearRegression\",\n        \"hyperparameters\": {\"alpha\": 0.01},\n        \"metrics\": {\"rmse\": 0.25}\n    }\n))\n\n# Track predictions\npredictions = await manager.save(ArtifactIn(\n    parent_id=model.id,\n    data={\n        \"prediction_date\": \"2025-10-20\",\n        \"num_predictions\": 1000\n    }\n))\n</code></pre>"},{"location":"guides/artifact-storage/#document-versioning","title":"Document Versioning","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"documents\",\n    level_labels={\n        0: \"project\",\n        1: \"document\",\n        2: \"version\"\n    }\n)\n\n# Project\nproject = await manager.save(ArtifactIn(\n    data={\"name\": \"API Documentation\"}\n))\n\n# Document\ndoc = await manager.save(ArtifactIn(\n    parent_id=project.id,\n    data={\"title\": \"Authentication Guide\"}\n))\n\n# Versions\nv1 = await manager.save(ArtifactIn(\n    parent_id=doc.id,\n    data={\"version\": \"1.0\", \"content\": \"...\"}\n))\n\nv2 = await manager.save(ArtifactIn(\n    parent_id=doc.id,\n    data={\"version\": \"1.1\", \"content\": \"...\"}\n))\n</code></pre>"},{"location":"guides/artifact-storage/#dataset-versioning","title":"Dataset Versioning","text":"<pre><code>hierarchy = ArtifactHierarchy(\n    name=\"datasets\",\n    level_labels={\n        0: \"dataset\",\n        1: \"version\",\n        2: \"partition\"\n    }\n)\n\n# Dataset\ndataset = await manager.save(ArtifactIn(\n    data={\"name\": \"customer_data\"}\n))\n\n# Version\nversion = await manager.save(ArtifactIn(\n    parent_id=dataset.id,\n    data={\"version\": \"2025-10\", \"rows\": 1000000}\n))\n\n# Partitions\ntrain = await manager.save(ArtifactIn(\n    parent_id=version.id,\n    data={\"partition\": \"train\", \"rows\": 800000}\n))\n\ntest = await manager.save(ArtifactIn(\n    parent_id=version.id,\n    data={\"partition\": \"test\", \"rows\": 200000}\n))\n</code></pre>"},{"location":"guides/artifact-storage/#testing","title":"Testing","text":""},{"location":"guides/artifact-storage/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chapkit.artifact import ArtifactManager, ArtifactRepository, ArtifactIn\n\n@pytest.mark.asyncio\nasync def test_artifact_hierarchy(session):\n    \"\"\"Test parent-child relationships.\"\"\"\n    repo = ArtifactRepository(session)\n    manager = ArtifactManager(repo, hierarchy=test_hierarchy)\n\n    # Create parent\n    parent = await manager.save(ArtifactIn(data={\"level\": \"root\"}))\n    assert parent.level == 0\n    assert parent.parent_id is None\n\n    # Create child\n    child = await manager.save(ArtifactIn(\n        parent_id=parent.id,\n        data={\"level\": \"child\"}\n    ))\n    assert child.level == 1\n    assert child.parent_id == parent.id\n\n    # Build tree\n    tree = await manager.build_tree(parent.id)\n    assert len(tree.children) == 1\n    assert tree.children[0].id == child.id\n</code></pre>"},{"location":"guides/artifact-storage/#curl-testing","title":"cURL Testing","text":"<pre><code># Create root artifact\nROOT_ID=$(curl -s -X POST http://localhost:8000/api/v1/artifacts \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\": {\"experiment\": \"test\"}}' | jq -r '.id')\n\n# Create child artifact\nCHILD_ID=$(curl -s -X POST http://localhost:8000/api/v1/artifacts \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"parent_id\": \"'$ROOT_ID'\",\n    \"data\": {\"model\": \"trained\"}\n  }' | jq -r '.id')\n\n# Get tree\ncurl -X POST http://localhost:8000/api/v1/artifacts/\\$tree \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"root_id\": \"'$ROOT_ID'\"}' | jq\n</code></pre>"},{"location":"guides/artifact-storage/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/artifact-storage/#database-size-management","title":"Database Size Management","text":"<p>Artifacts are stored in SQLite by default. Monitor database growth:</p> <pre><code># Check database size\ndu -h data.db\n\n# Count artifacts\nsqlite3 data.db \"SELECT COUNT(*) FROM artifacts;\"\n</code></pre> <p>Best Practices: - Implement retention policies - Archive old artifacts externally - Monitor BLOB storage growth - Use PostgreSQL for large deployments</p>"},{"location":"guides/artifact-storage/#indexing","title":"Indexing","text":"<p>Default indexes on <code>id</code>, <code>parent_id</code>, <code>level</code>, <code>created_at</code>. Add custom indexes for queries:</p> <pre><code>from sqlalchemy import Index\n\n# Add index on data-&gt;&gt;'experiment_name' for fast lookups\nIndex('ix_artifact_experiment', Artifact.data['experiment_name'].astext)\n</code></pre>"},{"location":"guides/artifact-storage/#backup-strategy","title":"Backup Strategy","text":"<pre><code># SQLite backup\nsqlite3 data.db \".backup backup_$(date +%Y%m%d).db\"\n\n# Automated backups\n0 2 * * * sqlite3 /app/data.db \".backup /backups/data_$(date +\\%Y\\%m\\%d).db\"\n</code></pre>"},{"location":"guides/artifact-storage/#complete-example","title":"Complete Example","text":"<p>See <code>examples/artifact_storage_api.py</code> for a complete working example with document versioning system.</p>"},{"location":"guides/artifact-storage/#next-steps","title":"Next Steps","text":"<ul> <li>Task Execution: Combine artifacts with tasks for ML pipelines</li> <li>Job Scheduler: Use background jobs for expensive artifact operations</li> <li>Monitoring: Track artifact creation rates with Prometheus</li> </ul>"},{"location":"guides/cli-scaffolding/","title":"CLI Scaffolding","text":"<p>Chapkit provides a CLI tool for quickly scaffolding new ML service projects with all necessary configuration files, Docker setup, and optional monitoring stack.</p>"},{"location":"guides/cli-scaffolding/#installation","title":"Installation","text":""},{"location":"guides/cli-scaffolding/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<p>Use chapkit with uvx (no installation needed):</p> <pre><code># One-off project creation\nuvx chapkit init my-ml-service\n</code></pre> <p>Or install it permanently:</p> <pre><code># Install globally\nuv tool install chapkit\n\n# Use the installed tool\nchapkit init my-ml-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#from-github-development","title":"From GitHub (Development)","text":"<p>To use the latest development version:</p> <pre><code># One-off project creation from GitHub\nuvx --from git+https://github.com/dhis2-chap/chapkit chapkit init my-ml-service\n\n# Or install from GitHub\nuv tool install git+https://github.com/dhis2-chap/chapkit\nchapkit init my-ml-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#managing-installed-tool","title":"Managing Installed Tool","text":"<p>If you installed chapkit with <code>uv tool install</code>, you can manage the installation:</p>"},{"location":"guides/cli-scaffolding/#check-installed-version","title":"Check Installed Version","text":"<pre><code># List installed tools and versions\nuv tool list\n\n# Check version\nchapkit --version\n</code></pre>"},{"location":"guides/cli-scaffolding/#upgrade-to-latest","title":"Upgrade to Latest","text":"<pre><code># Upgrade to latest version\nuv tool upgrade chapkit\n\n# Upgrade to specific version\nuv tool upgrade [email protected]\n</code></pre>"},{"location":"guides/cli-scaffolding/#uninstall","title":"Uninstall","text":"<pre><code># Remove installed tool\nuv tool uninstall chapkit\n</code></pre> <p>Note: When using <code>uvx chapkit</code>, the latest version is used automatically unless you specify a version with <code>@</code>:</p> <pre><code># Always uses latest\nuvx chapkit init my-service\n\n# Pin to specific version\nuvx [email protected] init my-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#quick-start","title":"Quick Start","text":"<p>Create and run a new project:</p> <pre><code># Create project (using uvx for one-off usage)\nuvx chapkit init my-ml-service\ncd my-ml-service\n\n# Install dependencies and run\nuv sync\nuv run python main.py\n</code></pre> <p>Visit http://localhost:8000/docs to interact with the ML API.</p>"},{"location":"guides/cli-scaffolding/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/cli-scaffolding/#chapkit-init","title":"<code>chapkit init</code>","text":"<p>Initialize a new chapkit ML service project.</p> <p>Usage:</p> <pre><code>chapkit init PROJECT_NAME [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>PROJECT_NAME</code> - Name of the project to create (required)</li> </ul> <p>Options:</p> <ul> <li><code>--path PATH</code> - Target directory (default: current directory)</li> <li><code>--with-monitoring</code> - Include Prometheus and Grafana monitoring stack</li> <li><code>--template TYPE</code> - Template type: <code>ml</code> (default), <code>ml-shell</code>, or <code>task</code></li> <li><code>--help</code> - Show help message</li> </ul> <p>Examples:</p> <pre><code># Using uvx (one-off, no installation needed)\nuvx chapkit init my-service\n\n# Using installed tool\nchapkit init my-service\n\n# Create project in specific location\nchapkit init my-service --path ~/projects\n\n# Create project with monitoring stack\nchapkit init my-service --with-monitoring\n\n# Create project with ml-shell template (language-agnostic)\nchapkit init my-service --template ml-shell\n\n# Create project with task template (task execution)\nchapkit init my-service --template task\n\n# Combine options\nchapkit init my-service --template ml-shell --with-monitoring\n\n# From GitHub (development version)\nuvx --from git+https://github.com/dhis2-chap/chapkit chapkit init my-service\n</code></pre>"},{"location":"guides/cli-scaffolding/#chapkit-artifact-list","title":"<code>chapkit artifact list</code>","text":"<p>List artifacts stored in a chapkit database or running service.</p> <p>Alias: <code>chapkit artifact ls</code></p> <p>Usage:</p> <pre><code>chapkit artifact list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--database, -d PATH</code> - Path to SQLite database file</li> <li><code>--url, -u URL</code> - Base URL of running chapkit service</li> <li><code>--type, -t TYPE</code> - Filter by artifact type (e.g., <code>ml_training_workspace</code>, <code>ml_prediction</code>)</li> <li><code>--help</code> - Show help message</li> </ul> <p>Note: Either <code>--database</code> or <code>--url</code> must be provided (mutually exclusive).</p> <p>Examples:</p> <pre><code># List from local database\nchapkit artifact list --database ./data/chapkit.db\n\n# List from running service\nchapkit artifact list --url http://localhost:8000\n\n# Filter by type\nchapkit artifact list --database ./data/chapkit.db --type ml_training_workspace\n</code></pre> <p>Output:</p> <p>The output shows artifacts with hierarchy indentation (2 spaces per level) for easier navigation:</p> <pre><code>ID                             TYPE                      SIZE       CONFIG         CREATED\n----------------------------------------------------------------------------------------------------\n01ABC123456789ABCDEFGHIJ       ml_training_workspace     1.2 MB     01CFG12345..   2024-01-15 10:30\n  01DEF987654321FEDCBA98       ml_prediction             45.3 KB    01CFG12345..   2024-01-15 10:35\n    01GHI111222333444555       ml_prediction_workspace   2.1 MB     01CFG12345..   2024-01-15 10:36\n</code></pre> <ul> <li>ID: Artifact ULID (indented by level)</li> <li>TYPE: Artifact type from metadata</li> <li>SIZE: Human-readable size</li> <li>CONFIG: Config ID from artifact metadata (truncated)</li> <li>CREATED: Timestamp in YYYY-MM-DD HH:MM format</li> </ul>"},{"location":"guides/cli-scaffolding/#chapkit-artifact-download","title":"<code>chapkit artifact download</code>","text":"<p>Download a ZIP artifact from a chapkit database or running service.</p> <p>Usage:</p> <pre><code>chapkit artifact download ARTIFACT_ID [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>ARTIFACT_ID</code> - Artifact ID (ULID) to download (required)</li> </ul> <p>Options:</p> <ul> <li><code>--database, -d PATH</code> - Path to SQLite database file</li> <li><code>--url, -u URL</code> - Base URL of running chapkit service</li> <li><code>--output, -o PATH</code> - Output path (default: <code>./&lt;artifact_id&gt;.zip</code> or <code>./&lt;artifact_id&gt;/</code> with <code>--extract</code>)</li> <li><code>--extract, -x</code> - Extract ZIP contents to a directory instead of saving as file</li> <li><code>--force, -f</code> - Overwrite existing output file or directory</li> <li><code>--help</code> - Show help message</li> </ul> <p>Note: Either <code>--database</code> or <code>--url</code> must be provided (mutually exclusive).</p> <p>Examples:</p> <pre><code># Download as ZIP file (default)\nchapkit artifact download 01ABC123... --database ./data/chapkit.db\n# Creates: 01ABC123....zip\n\n# Download with custom filename\nchapkit artifact download 01ABC123... --database ./data/chapkit.db -o model.zip\n\n# Extract to directory\nchapkit artifact download 01ABC123... --database ./data/chapkit.db --extract\n# Creates: 01ABC123.../\n\n# Extract to custom directory\nchapkit artifact download 01ABC123... --database ./data/chapkit.db --extract -o ./workspace\n\n# Download from running service\nchapkit artifact download 01ABC123... --url http://localhost:8000\n\n# Force overwrite existing\nchapkit artifact download 01ABC123... --database ./data/chapkit.db --force\n</code></pre>"},{"location":"guides/cli-scaffolding/#chapkit-test","title":"<code>chapkit test</code>","text":"<p>Run end-to-end tests against your ML service. This command only appears when inside a chapkit project directory.</p> <p>See Testing ML Services for full documentation.</p>"},{"location":"guides/cli-scaffolding/#template-types","title":"Template Types","text":""},{"location":"guides/cli-scaffolding/#ml-template-default","title":"ML Template (Default)","text":"<p>The ML template is the simpler approach where you define training and prediction logic as Python functions directly in <code>main.py</code>:</p> <p>Pros: - Simpler to understand and get started - All code in one file - Direct access to Python ecosystem - No external processes</p> <p>Cons: - Python-only workflows - Less isolation between training and prediction</p> <p>Best for: Python-centric ML workflows, prototyping, simpler models</p>"},{"location":"guides/cli-scaffolding/#ml-shell-template","title":"ML-Shell Template","text":"<p>The ML-shell template executes external scripts for training and prediction, enabling language-agnostic ML workflows:</p> <p>Pros: - Language-agnostic (Python, R, Julia, etc.) - Better isolation and testing - Can integrate existing scripts without modification - File-based data interchange (CSV, YAML, pickle)</p> <p>Cons: - More files to manage - Requires understanding of file I/O formats - Slightly more complex</p> <p>Best for: Multi-language environments, integrating existing scripts, team collaboration with different language preferences</p>"},{"location":"guides/cli-scaffolding/#task-template","title":"Task Template","text":"<p>The task template provides a general-purpose task execution system for Python functions:</p> <p>Pros: - Execute Python functions as tasks - Dependency injection (Database, ArtifactManager, etc.) - Registry-based task management - Job-based async execution - Task results stored in artifacts</p> <p>Cons: - Not ML-specific (no train/predict operations) - Requires understanding of task registry - More complex than simple function calls</p> <p>Best for: General-purpose automation, data processing pipelines, scheduled tasks, non-ML workflows</p>"},{"location":"guides/cli-scaffolding/#generated-project-structure","title":"Generated Project Structure","text":""},{"location":"guides/cli-scaffolding/#ml-template-default_1","title":"ML Template (Default)","text":"<pre><code>my-service/\n\u251c\u2500\u2500 main.py              # ML service with train/predict functions\n\u251c\u2500\u2500 pyproject.toml       # Python dependencies\n\u251c\u2500\u2500 Dockerfile           # Multi-stage Docker build\n\u251c\u2500\u2500 compose.yml          # Docker Compose configuration\n\u251c\u2500\u2500 data/                # Database directory\n\u2502   \u2514\u2500\u2500 chapkit.db       # SQLite database (created at runtime)\n\u251c\u2500\u2500 .gitignore           # Python gitignore\n\u2514\u2500\u2500 README.md            # Project documentation\n</code></pre>"},{"location":"guides/cli-scaffolding/#ml-shell-template_1","title":"ML-Shell Template","text":"<p>When using <code>--template ml-shell</code>, external scripts are generated:</p> <pre><code>my-service/\n\u251c\u2500\u2500 main.py              # ML service with command templates\n\u251c\u2500\u2500 scripts/             # External training/prediction scripts\n\u2502   \u251c\u2500\u2500 train_model.py   # Training script\n\u2502   \u2514\u2500\u2500 predict_model.py # Prediction script\n\u251c\u2500\u2500 pyproject.toml       # Python dependencies\n\u251c\u2500\u2500 Dockerfile           # Multi-stage Docker build\n\u251c\u2500\u2500 compose.yml          # Docker Compose configuration\n\u251c\u2500\u2500 data/                # Database directory\n\u2502   \u2514\u2500\u2500 chapkit.db       # SQLite database (created at runtime)\n\u251c\u2500\u2500 .gitignore           # Python gitignore\n\u2514\u2500\u2500 README.md            # Project documentation\n</code></pre>"},{"location":"guides/cli-scaffolding/#task-template_1","title":"Task Template","text":"<p>When using <code>--template task</code>, a task execution service is generated:</p> <pre><code>my-service/\n\u251c\u2500\u2500 main.py              # Task execution service with Python functions\n\u251c\u2500\u2500 pyproject.toml       # Python dependencies\n\u251c\u2500\u2500 Dockerfile           # Multi-stage Docker build\n\u251c\u2500\u2500 compose.yml          # Docker Compose configuration\n\u251c\u2500\u2500 data/                # Database directory\n\u2502   \u2514\u2500\u2500 chapkit.db       # SQLite database (created at runtime)\n\u251c\u2500\u2500 .gitignore           # Python gitignore\n\u2514\u2500\u2500 README.md            # Project documentation\n</code></pre>"},{"location":"guides/cli-scaffolding/#with-monitoring","title":"With Monitoring","text":"<p>When using <code>--with-monitoring</code>, additional files are generated:</p> <pre><code>my-service/\n...\n\u2514\u2500\u2500 monitoring/\n    \u251c\u2500\u2500 prometheus/\n    \u2502   \u2514\u2500\u2500 prometheus.yml\n    \u2514\u2500\u2500 grafana/\n        \u251c\u2500\u2500 provisioning/\n        \u2502   \u251c\u2500\u2500 datasources/prometheus.yml\n        \u2502   \u2514\u2500\u2500 dashboards/dashboard.yml\n        \u2514\u2500\u2500 dashboards/\n            \u2514\u2500\u2500 chapkit-service-metrics.json\n</code></pre>"},{"location":"guides/cli-scaffolding/#generated-files","title":"Generated Files","text":""},{"location":"guides/cli-scaffolding/#mainpy","title":"main.py","text":"<p>The generated <code>main.py</code> varies by template:</p> <p>ML Template (<code>ml</code>): - Config Schema: Pydantic model for ML parameters - Training Function: <code>on_train</code> with simple model example - Prediction Function: <code>on_predict</code> for inference - Service Info: Metadata (name, version, author, status) - Artifact Hierarchy: Storage structure for models and predictions - FastAPI App: Built using <code>MLServiceBuilder</code></p> <p>ML-Shell Template (<code>ml-shell</code>): - Similar to ML template but references external training/prediction scripts - Shell Commands: Command templates for executing scripts - Scripts Directory: Contains <code>train_model.py</code> and <code>predict_model.py</code></p> <p>Task Template (<code>task</code>): - Task Functions: Python functions registered with <code>@TaskRegistry.register()</code> - Task Manager: Handles task execution with dependency injection - Task Router: API endpoints for task CRUD and execution - FastAPI App: Built using <code>ServiceBuilder</code> (not ML-specific)</p> <p>Example structure (ML template):</p> <pre><code>class MyServiceConfig(BaseConfig):\n    # Add your model parameters here\n    prediction_periods: int = 3\n\nasync def on_train(config, data, geo=None):\n    # Training logic\n    model = {\"means\": data.select_dtypes(include=[\"number\"]).mean().to_dict()}\n    return model\n\nasync def on_predict(config, model, historic, future, geo=None):\n    # Prediction logic\n    return future\n\nrunner = FunctionalModelRunner(on_train=on_train, on_predict=on_predict)\napp = MLServiceBuilder(...).with_monitoring().build()\n</code></pre>"},{"location":"guides/cli-scaffolding/#pyprojecttoml","title":"pyproject.toml","text":"<p>Defines project metadata and dependencies:</p> <pre><code>[project]\nname = \"my-service\"\nversion = \"0.1.0\"\ndescription = \"ML service for my-service\"\nrequires-python = \"&gt;=3.13\"\ndependencies = [\"chapkit\"]\n\n[dependency-groups]\ndev = [\"uvicorn[standard]&gt;=0.30.0\"]\n</code></pre>"},{"location":"guides/cli-scaffolding/#dockerfile","title":"Dockerfile","text":"<p>Multi-stage Docker build with:</p> <ul> <li>Builder stage: UV-based dependency installation</li> <li>Runtime stage: Slim Python image with gunicorn/uvicorn</li> <li>Health checks and proper user setup</li> <li>Environment variables for configuration</li> </ul>"},{"location":"guides/cli-scaffolding/#composeyml","title":"compose.yml","text":"<p>Basic version:</p> <ul> <li>Single service (API) on port 8000</li> <li>Health checks</li> <li>Configurable workers and logging</li> </ul> <p>Monitoring version:</p> <ul> <li>API service (port 8000)</li> <li>Prometheus (port 9090)</li> <li>Grafana (port 3000, admin/admin)</li> <li>Pre-configured dashboards and datasources</li> </ul>"},{"location":"guides/cli-scaffolding/#customization","title":"Customization","text":""},{"location":"guides/cli-scaffolding/#update-model-logic","title":"Update Model Logic","text":"<p>Edit the <code>on_train</code> and <code>on_predict</code> functions in <code>main.py</code>:</p> <pre><code>async def on_train(config, data, geo=None):\n    # Your training logic here\n    from sklearn.ensemble import RandomForestRegressor\n    model = RandomForestRegressor()\n    model.fit(data[features], data[target])\n    return model\n</code></pre>"},{"location":"guides/cli-scaffolding/#add-configuration-parameters","title":"Add Configuration Parameters","text":"<p>Extend the config schema:</p> <pre><code>class MyServiceConfig(BaseConfig):\n    min_samples: int = 5\n    learning_rate: float = 0.001\n    features: list[str] = [\"feature_1\", \"feature_2\"]\n    prediction_periods: int = 3\n</code></pre>"},{"location":"guides/cli-scaffolding/#add-dependencies","title":"Add Dependencies","text":"<p>Use <code>uv</code> to add packages:</p> <pre><code>uv add scikit-learn pandas numpy\n</code></pre>"},{"location":"guides/cli-scaffolding/#customize-service-metadata","title":"Customize Service Metadata","text":"<p>Update the <code>MLServiceInfo</code>:</p> <pre><code>from chapkit.api import ModelMetadata, PeriodType\n\ninfo = MLServiceInfo(\n    id=\"production-model\",\n    display_name=\"Production Model\",\n    version=\"2.0.0\",\n    description=\"Detailed description here\",\n    model_metadata=ModelMetadata(\n        author=\"Your Team\",\n        author_assessed_status=AssessedStatus.green,\n        contact_email=\"team@example.com\",\n    ),\n    period_type=PeriodType.monthly,\n)\n</code></pre>"},{"location":"guides/cli-scaffolding/#development-workflow","title":"Development Workflow","text":""},{"location":"guides/cli-scaffolding/#local-development","title":"Local Development","text":"<pre><code># Install dependencies\nuv sync\n\n# Run development server\nuv run python main.py\n\n# Run tests (if added)\npytest\n\n# Lint code\nruff check .\n</code></pre>"},{"location":"guides/cli-scaffolding/#docker-development","title":"Docker Development","text":"<pre><code># Build and run\ndocker compose up --build\n\n# Run in background\ndocker compose up -d\n\n# View logs\ndocker compose logs -f\n\n# Stop services\ndocker compose down\n</code></pre>"},{"location":"guides/cli-scaffolding/#docker-data-management","title":"Docker Data Management","text":"<p>The generated <code>Dockerfile</code> and <code>compose.yml</code> are starting points designed to work out of the box. Customize them for your specific deployment needs.</p> <p>The following describes the default configuration. If you change <code>DATABASE_URL</code> or other settings, your setup may differ.</p> <p>Named Volumes</p> <p>Docker Compose uses named volumes (not bind mounts) for data persistence:</p> <pre><code>volumes:\n  - ck_my_service_data:/workspace/data\n</code></pre> <p>This approach:</p> <ul> <li>Works consistently across macOS, Linux, and Windows</li> <li>Avoids UID permission issues on Linux</li> <li>Data persists across container restarts</li> </ul> <p>Accessing Data</p> <pre><code># List files in data directory\ndocker compose exec api ls /workspace/data\n\n# Copy database out of container\ndocker compose cp api:/workspace/data/chapkit.db ./backup.db\n\n# Copy database into container\ndocker compose cp ./mydata.db api:/workspace/data/chapkit.db\n\n# Direct SQLite access\ndocker compose exec api sqlite3 /workspace/data/chapkit.db \".tables\"\n</code></pre> <p>Volume Management</p> <pre><code># List all Docker volumes\ndocker volume ls\n\n# Inspect volume details\ndocker volume inspect ck_my_service_data\n\n# Remove containers but keep data\ndocker compose down\n\n# Remove containers AND data (warning: data loss)\ndocker compose down -v\n</code></pre> <p>Using Bind Mounts</p> <p>If you need direct host filesystem access, modify <code>compose.yml</code> to use a bind mount:</p> <pre><code>volumes:\n  - ./data:/workspace/data  # Host path:container path\n</code></pre> <p>Note: On Linux, ensure the host directory has correct permissions for the container user (UID 10001).</p>"},{"location":"guides/cli-scaffolding/#access-services","title":"Access Services","text":"<ul> <li>API: http://localhost:8000</li> <li>API Docs: http://localhost:8000/docs</li> <li>Prometheus (if monitoring enabled): http://localhost:9090</li> <li>Grafana (if monitoring enabled): http://localhost:3000</li> </ul>"},{"location":"guides/cli-scaffolding/#next-steps","title":"Next Steps","text":"<p>After scaffolding your project:</p> <ol> <li>Customize the model: Update <code>on_train</code> and <code>on_predict</code> functions</li> <li>Add dependencies: Use <code>uv add</code> to install required packages</li> <li>Update configuration: Add model-specific parameters to config schema</li> <li>Test locally: Run with <code>uv run python main.py</code></li> <li>Dockerize: Build and test with <code>docker compose up --build</code></li> <li>Add tests: Create tests for your training and prediction logic</li> <li>Deploy: Use the generated Dockerfile for deployment</li> </ol>"},{"location":"guides/cli-scaffolding/#examples","title":"Examples","text":"<p>The <code>examples/</code> directory contains working examples for each template type:</p> <p>ML Template Examples: - <code>ml_functional/</code> - ML template with Python functions (FunctionalModelRunner) - <code>ml_class/</code> - Class-based ML runner approach - <code>quickstart/</code> - Complete ML service with config, artifacts, and ML endpoints</p> <p>ML-Shell Template Example: - <code>ml_shell/</code> - Language-agnostic ML with external scripts (ShellModelRunner)</p> <p>Task Template Example: - <code>task_execution/</code> - General-purpose task execution with Python functions</p> <p>Other Examples: - <code>ml_pipeline/</code> - Multi-stage ML pipeline with hierarchical artifacts - <code>full_featured/</code> - Comprehensive example with monitoring and custom routers - <code>config_artifact/</code> - Configuration with artifact linking - <code>artifact/</code> - Read-only artifact API - <code>custom_migrations/</code> - Database migrations with custom models</p>"},{"location":"guides/cli-scaffolding/#related-documentation","title":"Related Documentation","text":"<ul> <li>ML Workflows - Learn about model training and prediction</li> <li>Configuration Management - Working with configs</li> <li>Artifact Storage - Managing models and predictions</li> <li>Task Execution - Scheduling background jobs</li> </ul>"},{"location":"guides/configuration-management/","title":"Configuration Management","text":"<p>Chapkit provides a type-safe configuration management system for storing and managing application settings, environment configurations, and ML model parameters with JSON storage and optional artifact linking.</p>"},{"location":"guides/configuration-management/#quick-start","title":"Quick Start","text":"<pre><code>from chapkit import BaseConfig\nfrom chapkit.api import ServiceBuilder, ServiceInfo\n\nclass AppConfig(BaseConfig):\n    \"\"\"Application configuration schema.\"\"\"\n    debug: bool\n    api_host: str\n    api_port: int\n    prediction_periods: int = 3\n\napp = (\n    ServiceBuilder(info=ServiceInfo(id=\"my-service\", display_name=\"My Service\"))\n    .with_health()\n    .with_config(AppConfig)\n    .build()\n)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p> <p>Visit http://localhost:8000/docs to manage configurations via Swagger UI.</p>"},{"location":"guides/configuration-management/#architecture","title":"Architecture","text":""},{"location":"guides/configuration-management/#configuration-storage","title":"Configuration Storage","text":"<p>Configurations are stored as key-value pairs with JSON data:</p> <pre><code>Config\n  \u251c\u2500 name: \"production\" (unique identifier)\n  \u251c\u2500 data: {...}         (validated against schema)\n  \u251c\u2500 id: ULID            (auto-generated)\n  \u2514\u2500 created_at, updated_at, tags\n</code></pre>"},{"location":"guides/configuration-management/#type-safety-with-pydantic","title":"Type Safety with Pydantic","text":"<pre><code>class MLConfig(BaseConfig):\n    model_name: str\n    learning_rate: float = 0.001\n    epochs: int = 100\n    features: list[str]\n    prediction_periods: int = 3\n</code></pre> <p>Benefits: - Compile-time type checking - Runtime validation - Automatic API documentation - JSON schema generation - Extra fields allowed by default</p>"},{"location":"guides/configuration-management/#artifact-linking","title":"Artifact Linking","text":"<p>Link configs to trained models or experiment results:</p> <pre><code>Config(\"production_model\")\n  \u2514\u2500&gt; Trained Model Artifact (level 0)\n       \u251c\u2500&gt; Predictions 1 (level 1)\n       \u2514\u2500&gt; Predictions 2 (level 1)\n</code></pre>"},{"location":"guides/configuration-management/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/configuration-management/#baseconfig","title":"BaseConfig","text":"<p>Base class for all configuration schemas with flexible schema support.</p> <pre><code>from chapkit import BaseConfig\n\nclass DatabaseConfig(BaseConfig):\n    \"\"\"Database connection configuration.\"\"\"\n    host: str\n    port: int = 5432\n    username: str\n    password: str\n    database: str\n    ssl_enabled: bool = True\n    prediction_periods: int = 3\n</code></pre> <p>Features: - Inherits from <code>pydantic.BaseModel</code> - <code>extra=\"allow\"</code> - accepts arbitrary additional fields - JSON serializable - Validation on instantiation</p>"},{"location":"guides/configuration-management/#configin-configout","title":"ConfigIn / ConfigOut","text":"<p>Input and output schemas for API operations.</p> <pre><code>from chapkit import ConfigIn, ConfigOut\n\n# Create config\nconfig_in = ConfigIn[DatabaseConfig](\n    name=\"production_db\",\n    data=DatabaseConfig(\n        host=\"db.example.com\",\n        port=5432,\n        username=\"app_user\",\n        password=\"secret\",\n        database=\"prod\"\n    )\n)\n\n# Response schema\nconfig_out: ConfigOut[DatabaseConfig] = await manager.save(config_in)\n</code></pre>"},{"location":"guides/configuration-management/#configmanager","title":"ConfigManager","text":"<p>Business logic layer for configuration operations.</p> <pre><code>from chapkit import ConfigManager, ConfigRepository\n\nmanager = ConfigManager[AppConfig](repository, AppConfig)\n\n# Create/update\nconfig = await manager.save(ConfigIn(name=\"dev\", data=app_config))\n\n# Find by name\nconfig = await manager.find_by_name(\"dev\")\n\n# List all\nconfigs = await manager.find_all()\n\n# Delete\nawait manager.delete_by_id(config_id)\n</code></pre>"},{"location":"guides/configuration-management/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/configuration-management/#post-apiv1configs","title":"POST /api/v1/configs","text":"<p>Create new configuration.</p> <p>Request: <pre><code>{\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 8080,\n    \"max_connections\": 2000\n  }\n}\n</code></pre></p> <p>Response (201 Created): <pre><code>{\n  \"id\": \"01K72P5N5KCRM6MD3BRE4P07N8\",\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 8080,\n    \"max_connections\": 2000\n  },\n  \"created_at\": \"2025-10-24T12:00:00Z\",\n  \"updated_at\": \"2025-10-24T12:00:00Z\",\n  \"tags\": []\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#get-apiv1configs","title":"GET /api/v1/configs","text":"<p>List all configurations with pagination.</p> <p>Query Parameters: - <code>page</code>: Page number (default: 1) - <code>size</code>: Page size (default: 50)</p> <p>Response: <pre><code>{\n  \"items\": [...],\n  \"total\": 3,\n  \"page\": 1,\n  \"size\": 50,\n  \"pages\": 1\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#get-apiv1configsid","title":"GET /api/v1/configs/{id}","text":"<p>Get configuration by ID.</p>"},{"location":"guides/configuration-management/#put-apiv1configsid","title":"PUT /api/v1/configs/{id}","text":"<p>Update configuration.</p> <p>Request: <pre><code>{\n  \"name\": \"production\",\n  \"data\": {\n    \"debug\": false,\n    \"api_host\": \"0.0.0.0\",\n    \"api_port\": 9090,\n    \"max_connections\": 3000\n  }\n}\n</code></pre></p>"},{"location":"guides/configuration-management/#delete-apiv1configsid","title":"DELETE /api/v1/configs/{id}","text":"<p>Delete configuration.</p> <p>Note: When deleted, all linked artifact trees are cascade deleted.</p>"},{"location":"guides/configuration-management/#artifact-linking-operations","title":"Artifact Linking Operations","text":"<p>Enable artifact operations when building the service:</p> <pre><code>app = (\n    ServiceBuilder(info=info)\n    .with_config(MLConfig, enable_artifact_operations=True)\n    .with_artifacts(hierarchy=hierarchy)\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#post-apiv1configsidlink-artifact","title":"POST /api/v1/configs/{id}/$link-artifact","text":"<p>Link a root artifact to a config.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01MODEL456...\"\n}\n</code></pre></p> <p>Response: 204 No Content</p> <p>Validation: - Artifact must exist - Artifact must be a root (parent_id is NULL) - Each artifact can only be linked to one config</p>"},{"location":"guides/configuration-management/#post-apiv1configsidunlink-artifact","title":"POST /api/v1/configs/{id}/$unlink-artifact","text":"<p>Unlink an artifact from a config.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01MODEL456...\"\n}\n</code></pre></p> <p>Response: 204 No Content</p>"},{"location":"guides/configuration-management/#get-apiv1configsidartifacts","title":"GET /api/v1/configs/{id}/$artifacts","text":"<p>Get all root artifacts linked to a config.</p> <p>Response: <pre><code>[\n  {\n    \"id\": \"01MODEL456...\",\n    \"parent_id\": null,\n    \"level\": 0,\n    \"data\": {...},\n    \"created_at\": \"2025-10-24T10:00:00Z\",\n    \"updated_at\": \"2025-10-24T10:00:00Z\"\n  }\n]\n</code></pre></p>"},{"location":"guides/configuration-management/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/configuration-management/#environment-configurations","title":"Environment Configurations","text":"<pre><code>class EnvironmentConfig(BaseConfig):\n    \"\"\"Environment-specific configuration.\"\"\"\n    debug: bool\n    api_host: str\n    api_port: int\n    database_url: str\n    log_level: str = \"INFO\"\n    max_connections: int = 100\n    prediction_periods: int = 3\n\n# Create configs for different environments\nprod_config = ConfigIn(\n    name=\"production\",\n    data=EnvironmentConfig(\n        debug=False,\n        api_host=\"0.0.0.0\",\n        api_port=8080,\n        database_url=\"postgresql://...\",\n        log_level=\"WARNING\",\n        max_connections=2000\n    )\n)\n\ndev_config = ConfigIn(\n    name=\"development\",\n    data=EnvironmentConfig(\n        debug=True,\n        api_host=\"127.0.0.1\",\n        api_port=8000,\n        database_url=\"sqlite:///./dev.db\",\n        log_level=\"DEBUG\",\n        max_connections=10\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#ml-model-configurations","title":"ML Model Configurations","text":"<pre><code>class MLModelConfig(BaseConfig):\n    \"\"\"Machine learning model configuration.\"\"\"\n    model_type: str\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    features: list[str]\n    hyperparameters: dict[str, float]\n    prediction_periods: int = 3\n\nconfig = ConfigIn(\n    name=\"weather_model_v2\",\n    data=MLModelConfig(\n        model_type=\"RandomForest\",\n        learning_rate=0.001,\n        batch_size=32,\n        epochs=100,\n        features=[\"temperature\", \"humidity\", \"pressure\"],\n        hyperparameters={\n            \"n_estimators\": 100,\n            \"max_depth\": 10,\n            \"min_samples_split\": 2\n        }\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#nested-configurations","title":"Nested Configurations","text":"<pre><code>class DatabaseSettings(BaseModel):\n    \"\"\"Database connection settings.\"\"\"\n    host: str\n    port: int\n    ssl: bool = True\n\nclass CacheSettings(BaseModel):\n    \"\"\"Cache configuration.\"\"\"\n    enabled: bool = True\n    ttl_seconds: int = 3600\n\nclass AppConfig(BaseConfig):\n    \"\"\"Application configuration with nested settings.\"\"\"\n    app_name: str\n    version: str\n    database: DatabaseSettings\n    cache: CacheSettings\n    debug: bool = False\n    prediction_periods: int = 3\n\nconfig = ConfigIn(\n    name=\"app_config\",\n    data=AppConfig(\n        app_name=\"My API\",\n        version=\"1.0.0\",\n        database=DatabaseSettings(\n            host=\"db.example.com\",\n            port=5432,\n            ssl=True\n        ),\n        cache=CacheSettings(\n            enabled=True,\n            ttl_seconds=7200\n        ),\n        debug=False\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#extra-fields-support","title":"Extra Fields Support","text":"<pre><code># BaseConfig allows extra fields\nconfig = ConfigIn(\n    name=\"flexible_config\",\n    data=AppConfig(\n        required_field=\"value\",\n        dynamic_field=\"extra_value\",  # Not in schema but allowed\n        another_field=123\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#database-seeding","title":"Database Seeding","text":"<p>Seed configurations on application startup:</p> <pre><code>from fastapi import FastAPI\nfrom servicekit import Database\nfrom chapkit import ConfigIn, ConfigManager, ConfigRepository\n\nSEED_CONFIGS = [\n    (\"production\", EnvironmentConfig(debug=False, ...)),\n    (\"staging\", EnvironmentConfig(debug=True, ...)),\n    (\"local\", EnvironmentConfig(debug=True, ...)),\n]\n\nasync def seed_configs(app: FastAPI) -&gt; None:\n    \"\"\"Seed database with predefined configurations.\"\"\"\n    database: Database = app.state.database\n\n    async with database.session() as session:\n        repo = ConfigRepository(session)\n        manager = ConfigManager[EnvironmentConfig](repo, EnvironmentConfig)\n\n        # Clear existing configs (optional)\n        await manager.delete_all()\n\n        # Seed new configs\n        await manager.save_all(\n            ConfigIn(name=name, data=data)\n            for name, data in SEED_CONFIGS\n        )\n\napp = (\n    ServiceBuilder(info=info)\n    .with_config(EnvironmentConfig)\n    .on_startup(seed_configs)\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#complete-workflow-example","title":"Complete Workflow Example","text":""},{"location":"guides/configuration-management/#1-define-configuration-schema","title":"1. Define Configuration Schema","text":"<pre><code>class WeatherModelConfig(BaseConfig):\n    \"\"\"Configuration for weather prediction model.\"\"\"\n    model_version: str\n    training_features: list[str]\n    prediction_horizon_days: int\n    update_frequency: str\n    prediction_periods: int = 3\n</code></pre>"},{"location":"guides/configuration-management/#2-build-service","title":"2. Build Service","text":"<pre><code>app = (\n    ServiceBuilder(info=ServiceInfo(id=\"weather-model-service\", display_name=\"Weather Model Service\"))\n    .with_health()\n    .with_config(WeatherModelConfig, enable_artifact_operations=True)\n    .with_artifacts(hierarchy=ArtifactHierarchy(\n        name=\"weather_models\",\n        level_labels={0: \"ml_training_workspace\", 1: \"ml_prediction\"}\n    ))\n    .build()\n)\n</code></pre>"},{"location":"guides/configuration-management/#3-create-configuration","title":"3. Create Configuration","text":"<pre><code>CONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"weather_v1\",\n    \"data\": {\n      \"model_version\": \"1.0.0\",\n      \"training_features\": [\"temperature\", \"humidity\", \"pressure\"],\n      \"prediction_horizon_days\": 7,\n      \"update_frequency\": \"daily\"\n    }\n  }' | jq -r '.id')\n\necho \"Config ID: $CONFIG_ID\"\n</code></pre>"},{"location":"guides/configuration-management/#4-train-model-creates-artifact","title":"4. Train Model (Creates Artifact)","text":"<pre><code># Train model - creates artifact\nTRAIN_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {...}\n  }')\n\nMODEL_ARTIFACT_ID=$(echo $TRAIN_RESPONSE | jq -r '.artifact_id')\n</code></pre>"},{"location":"guides/configuration-management/#5-link-model-to-config","title":"5. Link Model to Config","text":"<pre><code>curl -X POST http://localhost:8000/api/v1/configs/$CONFIG_ID/\\$link-artifact \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"artifact_id\": \"'$MODEL_ARTIFACT_ID'\"}'\n</code></pre>"},{"location":"guides/configuration-management/#6-query-linked-artifacts","title":"6. Query Linked Artifacts","text":"<pre><code>curl http://localhost:8000/api/v1/configs/$CONFIG_ID/\\$artifacts | jq\n</code></pre>"},{"location":"guides/configuration-management/#7-update-configuration","title":"7. Update Configuration","text":"<pre><code>curl -X PUT http://localhost:8000/api/v1/configs/$CONFIG_ID \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"weather_v1\",\n    \"data\": {\n      \"model_version\": \"1.1.0\",\n      \"training_features\": [\"temperature\", \"humidity\", \"pressure\", \"wind_speed\"],\n      \"prediction_horizon_days\": 14,\n      \"update_frequency\": \"twice_daily\"\n    }\n  }' | jq\n</code></pre>"},{"location":"guides/configuration-management/#testing","title":"Testing","text":""},{"location":"guides/configuration-management/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chapkit import BaseConfig, ConfigIn, ConfigManager, ConfigRepository\n\nclass TestConfig(BaseConfig):\n    \"\"\"Test configuration schema.\"\"\"\n    setting: str\n    value: int\n    prediction_periods: int = 3\n\n@pytest.mark.asyncio\nasync def test_config_crud(session):\n    \"\"\"Test config CRUD operations.\"\"\"\n    repo = ConfigRepository(session)\n    manager = ConfigManager[TestConfig](repo, TestConfig)\n\n    # Create\n    config_in = ConfigIn(\n        name=\"test\",\n        data=TestConfig(setting=\"test_setting\", value=42)\n    )\n    config = await manager.save(config_in)\n\n    assert config.name == \"test\"\n    assert config.data.setting == \"test_setting\"\n    assert config.data.value == 42\n\n    # Find by name\n    found = await manager.find_by_name(\"test\")\n    assert found is not None\n    assert found.id == config.id\n\n    # Update\n    config_in.data.value = 100\n    updated = await manager.save(config_in)\n    assert updated.data.value == 100\n\n    # Delete\n    await manager.delete_by_id(config.id)\n    assert await manager.find_by_id(config.id) is None\n</code></pre>"},{"location":"guides/configuration-management/#integration-tests-with-artifact-linking","title":"Integration Tests with Artifact Linking","text":"<pre><code>@pytest.mark.asyncio\nasync def test_config_artifact_linking(session, artifact_manager, config_manager):\n    \"\"\"Test linking configs to artifacts.\"\"\"\n    # Create config\n    config = await config_manager.save(ConfigIn(\n        name=\"model_config\",\n        data=TestConfig(setting=\"ml\", value=1)\n    ))\n\n    # Create root artifact\n    artifact = await artifact_manager.save(ArtifactIn(\n        data={\"model\": \"trained\"}\n    ))\n\n    # Link artifact to config\n    await config_manager.link_artifact(config.id, artifact.id)\n\n    # Verify link\n    linked_artifacts = await config_manager.get_linked_artifacts(config.id)\n    assert len(linked_artifacts) == 1\n    assert linked_artifacts[0].id == artifact.id\n\n    # Unlink\n    await config_manager.unlink_artifact(artifact.id)\n    linked_artifacts = await config_manager.get_linked_artifacts(config.id)\n    assert len(linked_artifacts) == 0\n</code></pre>"},{"location":"guides/configuration-management/#curl-testing","title":"cURL Testing","text":"<pre><code># Create config\ncurl -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"test\", \"data\": {\"debug\": true, \"port\": 8000}}'\n\n# List configs\ncurl http://localhost:8000/api/v1/configs | jq\n\n# Get by ID\ncurl http://localhost:8000/api/v1/configs/01CONFIG123... | jq\n\n# Update\ncurl -X PUT http://localhost:8000/api/v1/configs/01CONFIG123... \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"test\", \"data\": {\"debug\": false, \"port\": 9000}}'\n\n# Delete\ncurl -X DELETE http://localhost:8000/api/v1/configs/01CONFIG123...\n</code></pre>"},{"location":"guides/configuration-management/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/configuration-management/#configuration-versioning","title":"Configuration Versioning","text":"<p>Use config names to track versions:</p> <pre><code># Version in name\nconfigs = [\n    ConfigIn(name=\"model_v1.0.0\", data=config_data_v1),\n    ConfigIn(name=\"model_v1.1.0\", data=config_data_v11),\n    ConfigIn(name=\"model_v2.0.0\", data=config_data_v2),\n]\n\n# Or in data\nclass VersionedConfig(BaseConfig):\n    version: str\n    settings: dict[str, object]\n    prediction_periods: int = 3\n\nconfig = ConfigIn(\n    name=\"production_model\",\n    data=VersionedConfig(\n        version=\"1.2.3\",\n        settings={...}\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#environment-variables","title":"Environment Variables","text":"<p>Load configurations from environment:</p> <pre><code>import os\nfrom pydantic_settings import BaseSettings\n\nclass EnvConfig(BaseSettings):\n    \"\"\"Configuration from environment variables.\"\"\"\n    database_url: str\n    api_key: str\n    debug: bool = False\n\n    class Config:\n        env_file = \".env\"\n\n# Load from environment\nenv_config = EnvConfig()\n\n# Store in database\nconfig_in = ConfigIn(\n    name=\"from_env\",\n    data=AppConfig(\n        database_url=env_config.database_url,\n        api_key=env_config.api_key,\n        debug=env_config.debug\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#secrets-management","title":"Secrets Management","text":"<p>Never store secrets in configs:</p> <pre><code># BAD: Secrets in database\nclass BadConfig(BaseConfig):\n    api_key: str  # Don't store in database!\n    password: str  # Don't store in database!\n\n# GOOD: Reference to secrets\nclass GoodConfig(BaseConfig):\n    secret_name: str  # Reference to secret manager\n    credential_id: str  # Reference to vault\n\n# Usage\nconfig = ConfigIn(\n    name=\"api_config\",\n    data=GoodConfig(\n        secret_name=\"production_api_key\",  # Load from AWS Secrets Manager\n        credential_id=\"vault:db/prod\"       # Load from HashiCorp Vault\n    )\n)\n</code></pre>"},{"location":"guides/configuration-management/#config-validation","title":"Config Validation","text":"<p>Add custom validation:</p> <pre><code>from pydantic import field_validator\n\nclass ValidatedConfig(BaseConfig):\n    \"\"\"Configuration with custom validation.\"\"\"\n    port: int\n    workers: int\n    timeout_seconds: int\n    prediction_periods: int = 3\n\n    @field_validator(\"port\")\n    @classmethod\n    def validate_port(cls, v: int) -&gt; int:\n        \"\"\"Validate port is in valid range.\"\"\"\n        if not 1024 &lt;= v &lt;= 65535:\n            raise ValueError(\"Port must be between 1024 and 65535\")\n        return v\n\n    @field_validator(\"workers\")\n    @classmethod\n    def validate_workers(cls, v: int) -&gt; int:\n        \"\"\"Validate worker count.\"\"\"\n        if v &lt; 1 or v &gt; 32:\n            raise ValueError(\"Workers must be between 1 and 32\")\n        return v\n</code></pre>"},{"location":"guides/configuration-management/#backup-configurations","title":"Backup Configurations","text":"<pre><code># Export all configs\ncurl http://localhost:8000/api/v1/configs?size=1000 | jq &gt; configs_backup.json\n\n# Restore\ncat configs_backup.json | jq -c '.items[]' | while read config; do\n  curl -X POST http://localhost:8000/api/v1/configs \\\n    -H \"Content-Type: application/json\" \\\n    -d \"$config\"\ndone\n</code></pre>"},{"location":"guides/configuration-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/configuration-management/#validation-errors","title":"Validation Errors","text":"<p>Problem: Config creation fails with validation errors.</p> <p>Cause: Data doesn't match schema.</p> <p>Solution: <pre><code># Check schema\nprint(YourConfig.model_json_schema())\n\n# Validate data before saving\ntry:\n    validated = YourConfig(debug=True, port=\"invalid\")\nexcept ValidationError as e:\n    print(e.errors())\n</code></pre></p>"},{"location":"guides/configuration-management/#extra-fields-not-saved","title":"Extra Fields Not Saved","text":"<p>Problem: Additional fields disappear after saving.</p> <p>Cause: Only fields in schema are saved unless using <code>extra=\"allow\"</code>.</p> <p>Solution: <pre><code># Ensure BaseConfig is used (has extra=\"allow\")\nclass MyConfig(BaseConfig):  # Inherits extra=\"allow\"\n    required_field: str\n    # Extra fields automatically allowed\n</code></pre></p>"},{"location":"guides/configuration-management/#artifact-link-fails","title":"Artifact Link Fails","text":"<p>Problem: \"Artifact is not a root artifact\" error.</p> <p>Cause: Trying to link a child artifact (has parent_id).</p> <p>Solution: <pre><code># Check artifact\ncurl http://localhost:8000/api/v1/artifacts/$ARTIFACT_ID | jq '.parent_id'\n\n# Should be null for root artifacts\n# Only link root artifacts to configs\n</code></pre></p>"},{"location":"guides/configuration-management/#config-deletion-cascade","title":"Config Deletion Cascade","text":"<p>Problem: Deleting config also deletes artifacts.</p> <p>Cause: Cascade delete removes entire artifact tree.</p> <p>Solution: <pre><code># Unlink artifacts before deleting config\nartifacts = await manager.get_linked_artifacts(config_id)\nfor artifact in artifacts:\n    await manager.unlink_artifact(artifact.id)\n\n# Then delete config\nawait manager.delete_by_id(config_id)\n</code></pre></p>"},{"location":"guides/configuration-management/#complete-example","title":"Complete Example","text":"<p>See <code>examples/config_basic/</code> for a complete working example with: - Custom configuration schema - Database seeding - Environment configurations - Custom service metadata - Docker deployment</p>"},{"location":"guides/configuration-management/#next-steps","title":"Next Steps","text":"<ul> <li>Artifact Storage: Link configs to trained models and results</li> <li>ML Workflows: Use configs for training and prediction</li> <li>Database Migrations: Add custom config tables</li> <li>Monitoring: Track config usage and changes</li> </ul>"},{"location":"guides/database-migrations/","title":"Database Migrations","text":"<p>Chapkit uses Alembic for database schema migrations, integrated with servicekit's async SQLAlchemy infrastructure. This guide covers the migration workflow and how to extend chapkit with your own custom database models.</p>"},{"location":"guides/database-migrations/#quick-start","title":"Quick Start","text":"<pre><code># Generate a new migration\nmake migrate MSG='add user table'\n\n# Apply migrations\nmake upgrade\n\n# Rollback last migration\nmake downgrade\n</code></pre> <p>Migrations are automatically applied when your application starts via <code>Database.init()</code>.</p>"},{"location":"guides/database-migrations/#architecture-overview","title":"Architecture Overview","text":""},{"location":"guides/database-migrations/#how-it-works","title":"How It Works","text":"<ol> <li>Base Metadata Registry: All ORM models inherit from <code>servicekit.models.Base</code> or <code>servicekit.models.Entity</code></li> <li>Model Registration: Models with <code>__tablename__</code> are automatically registered with <code>Base.metadata</code></li> <li>Alembic Auto-detection: Alembic's <code>env.py</code> uses <code>Base.metadata</code> as <code>target_metadata</code> to discover all tables</li> <li>Migration Generation: Alembic compares ORM models to current database schema and generates SQL operations</li> </ol>"},{"location":"guides/database-migrations/#chapkits-tables","title":"Chapkit's Tables","text":"<p>Chapkit provides these domain models: - configs - Configuration key-value storage - config_artifacts - Junction table linking configs to artifacts - artifacts - Hierarchical artifact storage - tasks - Task execution infrastructure</p> <p>All inherit from <code>servicekit.models.Entity</code> which provides: <code>id</code> (ULID), <code>created_at</code>, <code>updated_at</code>, <code>tags</code>.</p>"},{"location":"guides/database-migrations/#basic-workflow","title":"Basic Workflow","text":""},{"location":"guides/database-migrations/#1-modify-your-models","title":"1. Modify Your Models","text":"<pre><code># src/myapp/models.py\nfrom servicekit.models import Entity\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nclass User(Entity):\n    \"\"\"User model with email and name.\"\"\"\n    __tablename__ = \"users\"\n\n    email: Mapped[str] = mapped_column(unique=True, index=True)\n    name: Mapped[str]\n</code></pre> <p>Important: Models must inherit from <code>servicekit.models.Entity</code> or <code>Base</code> to be detected by Alembic.</p>"},{"location":"guides/database-migrations/#2-generate-migration","title":"2. Generate Migration","text":"<pre><code>make migrate MSG='add user table'\n</code></pre> <p>This creates a timestamped migration file in <code>alembic/versions/</code>: <pre><code>alembic/versions/20251024_1430_a1b2c3d4e5f6_add_user_table.py\n</code></pre></p>"},{"location":"guides/database-migrations/#3-review-migration","title":"3. Review Migration","text":"<pre><code>def upgrade() -&gt; None:\n    \"\"\"Apply database schema changes.\"\"\"\n    op.create_table(\n        'users',\n        sa.Column('email', sa.String(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n    )\n    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)\n\ndef downgrade() -&gt; None:\n    \"\"\"Revert database schema changes.\"\"\"\n    op.drop_index(op.f('ix_users_email'), table_name='users')\n    op.drop_table('users')\n</code></pre>"},{"location":"guides/database-migrations/#4-apply-migration","title":"4. Apply Migration","text":"<pre><code># Apply manually\nmake upgrade\n\n# Or restart app (auto-applies on startup)\nfastapi dev main.py\n</code></pre>"},{"location":"guides/database-migrations/#5-verify-schema","title":"5. Verify Schema","text":"<pre><code># Check migration history\nuv run alembic history\n\n# Check current version\nuv run alembic current\n\n# View database schema (SQLite example)\nsqlite3 app.db \".schema users\"\n</code></pre>"},{"location":"guides/database-migrations/#using-your-own-alembic-setup","title":"Using Your Own Alembic Setup","text":"<p>For larger projects, you may want to maintain your own Alembic configuration separate from chapkit while still reusing chapkit's infrastructure.</p>"},{"location":"guides/database-migrations/#project-structure","title":"Project Structure","text":"<pre><code>myproject/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 myapp/\n\u2502       \u251c\u2500\u2500 models.py          # Your custom models\n\u2502       \u2514\u2500\u2500 alembic_helpers.py # Optional: your migration helpers\n\u251c\u2500\u2500 alembic/                   # Your alembic directory\n\u2502   \u251c\u2500\u2500 env.py                 # Your env.py imports servicekit.Base\n\u2502   \u2514\u2500\u2500 versions/              # Your migrations\n\u251c\u2500\u2500 alembic.ini                # Your alembic config\n\u2514\u2500\u2500 main.py\n</code></pre>"},{"location":"guides/database-migrations/#setup-steps","title":"Setup Steps","text":""},{"location":"guides/database-migrations/#1-initialize-your-alembic","title":"1. Initialize Your Alembic","text":"<pre><code>uv run alembic init alembic\n</code></pre>"},{"location":"guides/database-migrations/#2-configure-alembicini","title":"2. Configure alembic.ini","text":"<pre><code>[alembic]\nscript_location = alembic\nfile_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s\ntimezone = UTC\nsqlalchemy.url = sqlite+aiosqlite:///./app.db\n</code></pre>"},{"location":"guides/database-migrations/#3-update-envpy","title":"3. Update env.py","text":"<p>Key Change: Import <code>Base</code> from servicekit to include all models (both chapkit's and yours):</p> <pre><code>\"\"\"Alembic environment configuration for async migrations.\"\"\"\nimport asyncio\nimport sys\nfrom logging.config import fileConfig\nfrom pathlib import Path\n\nfrom alembic import context\nfrom servicekit import Base  # Import Base from servicekit\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\n\n# Add parent directory to path for model imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\n# Import your custom models so they register with Base.metadata\nfrom myapp.models import User, Order  # Your models here\n\n# Alembic Config object\nconfig = context.config\n\n# Configure logging\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Target metadata from servicekit.Base (includes all models)\ntarget_metadata = Base.metadata\n\n\ndef run_migrations_offline() -&gt; None:\n    \"\"\"Run migrations in 'offline' mode.\"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection: Connection) -&gt; None:\n    \"\"\"Run migrations with the provided connection.\"\"\"\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_async_migrations() -&gt; None:\n    \"\"\"Run migrations using async engine.\"\"\"\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n\n    await connectable.dispose()\n\n\ndef run_migrations_online() -&gt; None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n    # Create event loop in thread to avoid conflicts with existing async code\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        loop.run_until_complete(run_async_migrations())\n    finally:\n        loop.close()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n</code></pre> <p>Critical Points: - Import <code>Base</code> from <code>servicekit</code> - Import all your custom models at the top - Optionally import chapkit's models if you want those tables too - Set <code>target_metadata = Base.metadata</code></p> <p>Note: Only imported models are included in migrations. If you don't import chapkit's models (Config, Artifact, Task), those tables won't be created. The example in <code>examples/custom_migrations/</code> demonstrates a standalone setup with only custom tables.</p>"},{"location":"guides/database-migrations/#4-create-your-models","title":"4. Create Your Models","text":"<pre><code># src/myapp/models.py\n\"\"\"Custom application models.\"\"\"\nfrom servicekit.models import Entity\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import Mapped, mapped_column\nfrom servicekit.types import ULIDType\nfrom ulid import ULID\n\nclass User(Entity):\n    \"\"\"User account model.\"\"\"\n    __tablename__ = \"users\"\n\n    email: Mapped[str] = mapped_column(unique=True, index=True)\n    name: Mapped[str]\n    is_active: Mapped[bool] = mapped_column(default=True)\n\nclass Order(Entity):\n    \"\"\"Order model with user relationship.\"\"\"\n    __tablename__ = \"orders\"\n\n    user_id: Mapped[ULID] = mapped_column(\n        ULIDType,\n        ForeignKey(\"users.id\", ondelete=\"CASCADE\"),\n        nullable=False,\n        index=True\n    )\n    total_amount: Mapped[float]\n    status: Mapped[str] = mapped_column(default=\"pending\")\n</code></pre>"},{"location":"guides/database-migrations/#5-generate-migrations","title":"5. Generate Migrations","text":"<pre><code># Using alembic directly\nuv run alembic revision --autogenerate -m \"add user and order tables\"\n\n# Format the generated file\nuv run ruff format alembic/versions/\n</code></pre>"},{"location":"guides/database-migrations/#6-optional-reuse-chapkits-helper-pattern","title":"6. Optional: Reuse Chapkit's Helper Pattern","text":"<p>You can create your own migration helpers following chapkit's pattern:</p> <pre><code># src/myapp/alembic_helpers.py\n\"\"\"Migration helpers for myapp tables.\"\"\"\nfrom typing import Any\nimport sqlalchemy as sa\nimport servicekit.types\n\n\ndef create_users_table(op: Any) -&gt; None:\n    \"\"\"Create users table.\"\"\"\n    op.create_table(\n        'users',\n        sa.Column('email', sa.String(), nullable=False),\n        sa.Column('name', sa.String(), nullable=False),\n        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='1'),\n        sa.Column('id', servicekit.types.ULIDType(length=26), nullable=False),\n        sa.Column('created_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('updated_at', sa.DateTime(), server_default=sa.text('(CURRENT_TIMESTAMP)'), nullable=False),\n        sa.Column('tags', sa.JSON(), nullable=False, server_default='[]'),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n    )\n    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=False)\n\n\ndef drop_users_table(op: Any) -&gt; None:\n    \"\"\"Drop users table.\"\"\"\n    op.drop_index(op.f('ix_users_email'), table_name='users')\n    op.drop_table('users')\n</code></pre> <p>Then use in migrations, optionally mixing with chapkit's helpers:</p> <pre><code>\"\"\"Add user and order tables.\"\"\"\nfrom alembic import op\nfrom myapp.alembic_helpers import create_users_table, drop_users_table\n\n# OPTIONAL: Import chapkit's helpers if you want those tables too\nfrom chapkit.alembic_helpers import (\n    create_configs_table,\n    create_artifacts_table,\n    drop_configs_table,\n    drop_artifacts_table,\n)\n\nrevision = 'a1b2c3d4e5f6'\ndown_revision = None\n\ndef upgrade() -&gt; None:\n    \"\"\"Apply database schema changes.\"\"\"\n    # Create chapkit tables (optional)\n    create_configs_table(op)\n    create_artifacts_table(op)\n\n    # Create your custom tables\n    create_users_table(op)\n\ndef downgrade() -&gt; None:\n    \"\"\"Revert database schema changes.\"\"\"\n    drop_users_table(op)\n    drop_artifacts_table(op)\n    drop_configs_table(op)\n</code></pre> <p>Benefits of Helpers: - Reusable across migrations and projects - Mix chapkit's helpers with your own - Consistent table definitions - Easier to maintain - Clear upgrade/downgrade operations - No need to regenerate migrations when adding chapkit tables</p>"},{"location":"guides/database-migrations/#model-import-requirements","title":"Model Import Requirements","text":"<p>Critical: Models must be imported before Alembic runs for auto-detection to work.</p>"},{"location":"guides/database-migrations/#where-to-import","title":"Where to Import","text":"<p>Option 1: In alembic/env.py (Recommended) <pre><code>from servicekit import Base\nfrom myapp.models import User, Order, Product  # Explicit imports\n</code></pre></p> <p>Option 2: In your app module <pre><code># src/myapp/__init__.py\nfrom .models import User, Order, Product\n\n__all__ = [\"User\", \"Order\", \"Product\"]\n</code></pre></p> <p>Then import in env.py: <pre><code>import myapp  # Imports __init__.py which imports models\n</code></pre></p>"},{"location":"guides/database-migrations/#common-mistake","title":"Common Mistake","text":"<pre><code># BAD: Models not imported\nfrom servicekit import Base\ntarget_metadata = Base.metadata  # Won't include your models!\n</code></pre> <pre><code># GOOD: Models imported before metadata is used\nfrom servicekit import Base\nfrom myapp.models import User, Order\ntarget_metadata = Base.metadata  # Now includes User and Order\n</code></pre>"},{"location":"guides/database-migrations/#mixing-chapkit-and-custom-migrations","title":"Mixing Chapkit and Custom Migrations","text":"<p>You can combine chapkit's tables with your custom tables in migrations.</p> <p>Recommended Approach: Reuse Helpers</p> <p>Import chapkit's helpers directly in your migrations:</p> <pre><code>from alembic import op\nfrom chapkit.alembic_helpers import (\n    create_configs_table,\n    create_artifacts_table,\n    create_tasks_table,\n)\nfrom myapp.alembic_helpers import create_users_table\n\ndef upgrade() -&gt; None:\n    # Create chapkit tables\n    create_configs_table(op)\n    create_artifacts_table(op)\n    create_tasks_table(op)\n\n    # Create your tables\n    create_users_table(op)\n</code></pre> <p>Benefits: - \u2705 No need to regenerate migrations - \u2705 Reuse tested helper functions - \u2705 Explicit control over table creation order - \u2705 Mix and match as needed - \u2705 Clear and maintainable</p> <p>Alternative: Single Alembic with All Models</p> <p>Import all models in <code>env.py</code> and use autogenerate:</p> <pre><code># Import chapkit models\nfrom chapkit.config.models import Config\nfrom chapkit.artifact.models import Artifact\n\n# Import your models\nfrom myapp.models import User, Order\n\n# All models registered in Base.metadata\ntarget_metadata = Base.metadata\n</code></pre> <p>Then: <code>uv run alembic revision --autogenerate -m \"create all tables\"</code></p> <p>Choose the helper approach when: - You want explicit control - You're following the helper pattern - You don't need autogenerate for these tables</p> <p>Choose autogenerate when: - Models change frequently - You prefer automatic detection - You want Alembic to track differences</p>"},{"location":"guides/database-migrations/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/database-migrations/#models-not-detected","title":"Models Not Detected","text":"<p>Problem: <code>make migrate</code> doesn't generate changes for your new model.</p> <p>Cause: Model not imported or not inheriting from Base/Entity.</p> <p>Solution: <pre><code># 1. Verify model inherits from Entity or Base\nfrom servicekit.models import Entity\n\nclass User(Entity):  # Must inherit from Entity or Base\n    __tablename__ = \"users\"\n    # ...\n\n# 2. Import model in alembic/env.py\nfrom myapp.models import User  # Add this line\n\n# 3. Verify import works\npython -c \"from myapp.models import User; print(User.__tablename__)\"\n</code></pre></p>"},{"location":"guides/database-migrations/#migration-conflicts","title":"Migration Conflicts","text":"<p>Problem: \"Multiple heads\" or \"Can't determine base revision\".</p> <p>Cause: Branched revision history (two migrations with same parent).</p> <p>Solution: <pre><code># View all revisions\nuv run alembic branches\n\n# Merge branches\nuv run alembic merge -m \"merge branches\" head1 head2\n\n# Apply merged migration\nmake upgrade\n</code></pre></p>"},{"location":"guides/database-migrations/#import-errors-during-migration","title":"Import Errors During Migration","text":"<p>Problem: Migration fails with import errors.</p> <p>Cause: Model imports fail due to circular dependencies or missing packages.</p> <p>Solution: - Use relative imports in models - Avoid importing app-level code in models - Only import SQLAlchemy and servicekit in model files</p>"},{"location":"guides/database-migrations/#autogenerate-not-detecting-changes","title":"Autogenerate Not Detecting Changes","text":"<p>Problem: Changed a column but autogenerate ignores it.</p> <p>Cause: Alembic doesn't always detect all changes automatically.</p> <p>Solution: <pre><code># Create empty migration\nuv run alembic revision -m \"update user table\"\n\n# Manually add changes\ndef upgrade() -&gt; None:\n    op.alter_column('users', 'email', new_column_name='email_address')\n</code></pre></p>"},{"location":"guides/database-migrations/#database-locked-sqlite","title":"Database Locked (SQLite)","text":"<p>Problem: \"Database is locked\" during migration.</p> <p>Cause: Another process has the database open.</p> <p>Solution: <pre><code># Stop all running apps\n# Then apply migration\nmake upgrade\n</code></pre></p> <p>For production, use PostgreSQL instead of SQLite to avoid locking issues.</p>"},{"location":"guides/database-migrations/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/database-migrations/#backup-before-migrating","title":"Backup Before Migrating","text":"<pre><code># SQLite backup\ncp app.db app.db.backup\n\n# PostgreSQL backup\npg_dump -U user -d database &gt; backup.sql\n</code></pre>"},{"location":"guides/database-migrations/#test-migrations","title":"Test Migrations","text":"<pre><code># Apply migration\nmake upgrade\n\n# Verify schema\nsqlite3 app.db \".schema\"\n\n# Test rollback\nmake downgrade\n\n# Reapply\nmake upgrade\n</code></pre>"},{"location":"guides/database-migrations/#migration-in-cicd","title":"Migration in CI/CD","text":"<pre><code># In your deployment script\nuv run alembic upgrade head\n</code></pre>"},{"location":"guides/database-migrations/#multiple-environments","title":"Multiple Environments","text":"<p>Use environment variables for database URLs:</p> <pre><code># alembic/env.py\nimport os\n\nconfig.set_main_option(\n    \"sqlalchemy.url\",\n    os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n)\n</code></pre>"},{"location":"guides/database-migrations/#complete-example","title":"Complete Example","text":"<p>See <code>examples/custom_migrations/</code> for a working example with: - Custom User and Order models - Separate alembic setup - Reusable migration helpers - Integration with chapkit's infrastructure</p>"},{"location":"guides/database-migrations/#next-steps","title":"Next Steps","text":"<ul> <li>Database Guide: Learn about servicekit's Database and SqliteDatabase classes</li> <li>Models Guide: Deep dive into Entity and custom ORM patterns</li> <li>Testing: Test migrations in CI/CD pipelines</li> </ul>"},{"location":"guides/dataframe/","title":"DataFrame Data Interchange","text":"<p>Servicekit provides a universal DataFrame class for seamless data interchange between different data libraries (pandas, polars, xarray) and file formats (CSV, Parquet). It's designed to be lightweight, framework-agnostic, and easy to use in API services.</p>"},{"location":"guides/dataframe/#quick-start","title":"Quick Start","text":""},{"location":"guides/dataframe/#basic-usage","title":"Basic Usage","text":"<pre><code>from chapkit.data import DataFrame\n\n# Create from dictionary\ndf = DataFrame.from_dict({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"NYC\", \"SF\", \"LA\"]\n})\n\n# Inspect data\nprint(df.shape)  # (3, 3)\nprint(df.head(2))\n\n# Convert to other libraries\npandas_df = df.to_pandas()\npolars_df = df.to_polars()\n</code></pre>"},{"location":"guides/dataframe/#in-fastapi-services","title":"In FastAPI Services","text":"<pre><code>from fastapi import FastAPI, UploadFile\nfrom fastapi.responses import Response\nfrom chapkit.data import DataFrame\n\napp = FastAPI()\n\n@app.post(\"/data/$upload\")\nasync def upload_csv(file: UploadFile):\n    \"\"\"Accept CSV upload and process.\"\"\"\n    content = await file.read()\n    df = DataFrame.from_csv(csv_string=content.decode())\n\n    # Process data\n    df = df.select([\"name\", \"age\"]).head(100)\n\n    return {\"rows\": df.shape[0], \"columns\": df.columns}\n\n@app.get(\"/data/$download\")\nasync def download_csv():\n    \"\"\"Export data as CSV.\"\"\"\n    df = get_data()  # Your data source\n    csv_data = df.to_csv()\n    return Response(content=csv_data, media_type=\"text/csv\")\n</code></pre>"},{"location":"guides/dataframe/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/dataframe/#data-structure","title":"Data Structure","text":"<p>DataFrame uses a simple columnar structure:</p> <pre><code>df = DataFrame(\n    columns=[\"name\", \"age\"],\n    data=[\n        [\"Alice\", 25],\n        [\"Bob\", 30]\n    ]\n)\n</code></pre> <ul> <li>columns: List of column names (strings)</li> <li>data: List of rows, where each row is a list of values</li> <li>Type-agnostic: Values can be any Python type</li> </ul>"},{"location":"guides/dataframe/#design-principles","title":"Design Principles","text":"<ul> <li>Lightweight: No required dependencies beyond Pydantic</li> <li>Framework-agnostic: Works with any Python environment</li> <li>Lazy imports: Optional libraries loaded only when needed</li> <li>Immutable: Methods return new DataFrames (no in-place modification)</li> <li>API consistency: All methods follow <code>from_X()</code> / <code>to_X()</code> pattern</li> </ul>"},{"location":"guides/dataframe/#creating-dataframes","title":"Creating DataFrames","text":""},{"location":"guides/dataframe/#from-dictionary","title":"From Dictionary","text":"<pre><code># Column-oriented (dict of lists)\ndf = DataFrame.from_dict({\n    \"name\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 30]\n})\n</code></pre>"},{"location":"guides/dataframe/#from-records","title":"From Records","text":"<pre><code># Row-oriented (list of dicts)\ndf = DataFrame.from_records([\n    {\"name\": \"Alice\", \"age\": 25},\n    {\"name\": \"Bob\", \"age\": 30}\n])\n</code></pre>"},{"location":"guides/dataframe/#from-csv","title":"From CSV","text":"<pre><code># From file (with automatic type inference)\ndf = DataFrame.from_csv(\"data.csv\")\n\n# From string - values are automatically converted to int, float, bool, or None\ncsv_string = \"name,age,active\\nAlice,25,true\\nBob,30,false\"\ndf = DataFrame.from_csv(csv_string=csv_string)\n# Result: [[\"Alice\", 25, True], [\"Bob\", 30, False]]\n\n# Custom delimiter\ndf = DataFrame.from_csv(\"data.tsv\", delimiter=\"\\t\")\n\n# Without header\ndf = DataFrame.from_csv(\"data.csv\", has_header=False)\n# Generates columns: col_0, col_1, ...\n\n# Disable type inference (keep all values as strings)\ndf = DataFrame.from_csv(\"data.csv\", infer_types=False)\n</code></pre>"},{"location":"guides/dataframe/#from-other-libraries","title":"From Other Libraries","text":"<pre><code># From pandas\nimport pandas as pd\npandas_df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf = DataFrame.from_pandas(pandas_df)\n\n# From polars\nimport polars as pl\npolars_df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf = DataFrame.from_polars(polars_df)\n\n# From xarray (2D only)\nimport xarray as xr\ndata_array = xr.DataArray([[1, 2], [3, 4]])\ndf = DataFrame.from_xarray(data_array)\n</code></pre>"},{"location":"guides/dataframe/#exporting-dataframes","title":"Exporting DataFrames","text":""},{"location":"guides/dataframe/#to-csv","title":"To CSV","text":"<pre><code># To file\ndf.to_csv(\"output.csv\")\n\n# To string\ncsv_string = df.to_csv()\n\n# Without header\ndf.to_csv(\"output.csv\", include_header=False)\n\n# Custom delimiter\ndf.to_csv(\"output.tsv\", delimiter=\"\\t\")\n</code></pre>"},{"location":"guides/dataframe/#to-dictionary","title":"To Dictionary","text":"<pre><code># As dict of lists (default)\ndata = df.to_dict(orient=\"list\")\n# {\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]}\n\n# As list of records\ndata = df.to_dict(orient=\"records\")\n# [{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]\n\n# As dict of dicts\ndata = df.to_dict(orient=\"dict\")\n# {\"name\": {0: \"Alice\", 1: \"Bob\"}, \"age\": {0: 25, 1: 30}}\n</code></pre>"},{"location":"guides/dataframe/#to-other-libraries","title":"To Other Libraries","text":"<pre><code># To pandas\npandas_df = df.to_pandas()\n\n# To polars\npolars_df = df.to_polars()\n</code></pre>"},{"location":"guides/dataframe/#data-inspection","title":"Data Inspection","text":""},{"location":"guides/dataframe/#properties","title":"Properties","text":"<pre><code># Shape (rows, columns)\nprint(df.shape)  # (100, 5)\n\n# Number of rows\nprint(df.shape[0])  # 100\nprint(len(df))  # 100 - can also use len()\n\n# Number of columns\nprint(df.shape[1])  # 5\n\n# Check if empty\nprint(df.empty)  # False\n\n# Number of dimensions (always 2)\nprint(df.ndim)  # 2\n\n# Total elements\nprint(df.size)  # 500 (100 * 5)\n</code></pre>"},{"location":"guides/dataframe/#viewing-data","title":"Viewing Data","text":"<pre><code># First 5 rows (default)\ndf.head()\n\n# First n rows\ndf.head(10)\n\n# Last 5 rows (default)\ndf.tail()\n\n# Last n rows\ndf.tail(10)\n\n# Negative indexing (pandas-style)\ndf.head(-3)  # All except last 3 rows\ndf.tail(-3)  # All except first 3 rows\n</code></pre>"},{"location":"guides/dataframe/#random-sampling","title":"Random Sampling","text":"<pre><code># Sample n rows\nsample = df.sample(n=100)\n\n# Sample fraction\nsample = df.sample(frac=0.1)  # 10% of rows\n\n# Reproducible sampling\nsample = df.sample(n=50, random_state=42)\n</code></pre>"},{"location":"guides/dataframe/#iteration","title":"Iteration","text":"<pre><code># Iterate over rows as dictionaries\nfor row in df:\n    print(row)  # {'name': 'Alice', 'age': 25}\n\n# Get number of rows with len()\nnum_rows = len(df)\n\n# Use in list comprehensions\nnames = [row['name'] for row in df]\n</code></pre>"},{"location":"guides/dataframe/#column-operations","title":"Column Operations","text":""},{"location":"guides/dataframe/#accessing-columns","title":"Accessing Columns","text":"<pre><code># Get column values as list\nages = df.get_column(\"age\")  # [25, 30, 35]\nages = df[\"age\"]  # Same using [] syntax\n\n# Select multiple columns as DataFrame\ndf_subset = df[[\"name\", \"age\"]]\ndf_subset = df.select([\"name\", \"age\"])  # Equivalent\n</code></pre>"},{"location":"guides/dataframe/#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select specific columns\ndf_subset = df.select([\"name\", \"age\"])\n\n# Single column\ndf_single = df.select([\"age\"])\n</code></pre>"},{"location":"guides/dataframe/#dropping-columns","title":"Dropping Columns","text":"<pre><code># Drop specific columns\ndf_clean = df.drop([\"temp_column\", \"debug_field\"])\n\n# Drop multiple\ndf_clean = df.drop([\"col1\", \"col2\", \"col3\"])\n</code></pre>"},{"location":"guides/dataframe/#renaming-columns","title":"Renaming Columns","text":"<pre><code># Rename specific columns\ndf_renamed = df.rename({\n    \"old_name\": \"new_name\",\n    \"user_id\": \"id\"\n})\n\n# Partial rename (other columns unchanged)\ndf_renamed = df.rename({\"age\": \"years\"})\n\n# Alternative: use rename_columns() (same behavior)\ndf_renamed = df.rename_columns({\"age\": \"years\"})\n</code></pre> <p>The <code>rename_columns()</code> method is an alias for <code>rename()</code>, provided for improved code readability and discoverability.</p>"},{"location":"guides/dataframe/#validation-and-type-inference","title":"Validation and Type Inference","text":""},{"location":"guides/dataframe/#structure-validation","title":"Structure Validation","text":"<pre><code># Validate DataFrame structure\ntry:\n    df.validate_structure()\n    print(\"DataFrame is valid\")\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n\n# Checks performed:\n# - All rows have same length as columns\n# - Column names are unique\n# - No empty column names\n</code></pre>"},{"location":"guides/dataframe/#type-inference","title":"Type Inference","text":"<pre><code># Infer column data types\ntypes = df.infer_types()\nprint(types)\n# {\"age\": \"int\", \"name\": \"str\", \"score\": \"float\"}\n\n# Supported types:\n# - \"int\": All integers\n# - \"float\": All floats (or mix of int/float)\n# - \"str\": All strings\n# - \"bool\": All booleans\n# - \"null\": All None values\n# - \"mixed\": Multiple different types\n</code></pre>"},{"location":"guides/dataframe/#csv-type-inference","title":"CSV Type Inference","text":"<p>When loading CSV files with <code>from_csv()</code>, values are automatically inferred and converted:</p> <pre><code>csv_string = \"\"\"name,age,score,active\nAlice,25,95.5,true\nBob,30,87.0,false\n,35,,yes\"\"\"\n\ndf = DataFrame.from_csv(csv_string=csv_string)\n\n# Types are automatically inferred:\n# - name: str (text values)\n# - age: int (integer values)\n# - score: float (decimal values)\n# - active: bool (true/false/yes/no)\n\n# Empty strings become None\nprint(df.data[2])  # [None, 35, None, True]\n</code></pre> <p>Supported Type Conversions:</p> CSV Value Python Type Example Empty/whitespace <code>None</code> <code>\"\"</code>, <code>\"   \"</code> <code>true</code>, <code>false</code>, <code>yes</code>, <code>no</code> (case-insensitive) <code>bool</code> <code>\"true\"</code> -&gt; <code>True</code> Integer strings (no decimal) <code>int</code> <code>\"42\"</code> -&gt; <code>42</code> Decimal/scientific notation <code>float</code> <code>\"3.14\"</code>, <code>\"1e10\"</code> Other text <code>str</code> <code>\"hello\"</code> -&gt; <code>\"hello\"</code> <p>Type Promotion Rules: - Columns with mixed int/float values become <code>float</code> - Columns with mixed types (e.g., int and str) remain <code>str</code> - <code>None</code> values are compatible with any type</p> <p>Disabling Type Inference:</p> <pre><code># Keep all values as strings (original behavior)\ndf = DataFrame.from_csv(\"data.csv\", infer_types=False)\n</code></pre>"},{"location":"guides/dataframe/#null-detection","title":"Null Detection","text":"<pre><code># Check for None values per column\nnulls = df.has_nulls()\nprint(nulls)\n# {\"age\": False, \"email\": True, \"phone\": True}\n\n# Use for data quality checks\nif any(nulls.values()):\n    print(\"Warning: DataFrame contains null values\")\n</code></pre>"},{"location":"guides/dataframe/#sorting-and-analytics","title":"Sorting and Analytics","text":""},{"location":"guides/dataframe/#sorting","title":"Sorting","text":"<pre><code># Sort by column (ascending)\ndf_sorted = df.sort(\"age\")\n\n# Sort descending\ndf_sorted = df.sort(\"score\", ascending=False)\n\n# None values always sort to the end\ndf_sorted = df.sort(\"nullable_column\")\n</code></pre>"},{"location":"guides/dataframe/#unique-values","title":"Unique Values","text":"<pre><code># Get unique values from a column\ncategories = df.unique(\"category\")\n# ['A', 'B', 'C'] - preserves order of first appearance\n\n# Count unique values\nnum_unique = len(df.unique(\"category\"))\n</code></pre>"},{"location":"guides/dataframe/#value-counts","title":"Value Counts","text":"<pre><code># Count occurrences of each value\ncounts = df.value_counts(\"category\")\n# {'A': 3, 'B': 2, 'C': 1}\n\n# Find most common value\nmost_common = max(counts, key=counts.get)\n\n# Get distribution\ntotal = len(df)\ndistribution = {k: v/total for k, v in counts.items()}\n</code></pre>"},{"location":"guides/dataframe/#json-support","title":"JSON Support","text":""},{"location":"guides/dataframe/#creating-from-json","title":"Creating from JSON","text":"<pre><code># From JSON array of objects\njson_data = '[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]'\ndf = DataFrame.from_json(json_data)\n\n# From API response\nimport requests\nresponse = requests.get(\"https://api.example.com/data\")\ndf = DataFrame.from_json(response.text)\n</code></pre>"},{"location":"guides/dataframe/#exporting-to-json","title":"Exporting to JSON","text":"<pre><code># As array of objects (records format)\njson_str = df.to_json(orient=\"records\")\n# '[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]'\n\n# As object with arrays (columns format)\njson_str = df.to_json(orient=\"columns\")\n# '{\"name\": [\"Alice\", \"Bob\"], \"age\": [25, 30]}'\n\n# For API responses\nfrom fastapi import Response\n\n@app.get(\"/data\")\nasync def get_data():\n    df = get_dataframe()\n    return Response(content=df.to_json(), media_type=\"application/json\")\n</code></pre>"},{"location":"guides/dataframe/#row-filtering-and-transformation","title":"Row Filtering and Transformation","text":""},{"location":"guides/dataframe/#filtering-rows","title":"Filtering Rows","text":"<pre><code># Filter with predicate function\nadults = df.filter(lambda row: row['age'] &gt;= 18)\n\n# Multiple conditions\nactive_adults = df.filter(lambda row: row['age'] &gt;= 18 and row['active'])\n\n# Complex filtering\nhigh_scorers = df.filter(lambda row: row['score'] &gt; 90 or (row['score'] &gt; 80 and row['bonus_eligible']))\n</code></pre>"},{"location":"guides/dataframe/#applying-transformations","title":"Applying Transformations","text":"<pre><code># Transform column values\ndf_upper = df.apply(str.upper, 'name')\n\n# Apply custom function\ndf_doubled = df.apply(lambda x: x * 2, 'price')\n\n# Apply method\ndf_rounded = df.apply(round, 'price')\n</code></pre>"},{"location":"guides/dataframe/#adding-columns","title":"Adding Columns","text":"<pre><code># Add new column\ntotal = [x + y for x, y in zip(df['price'], df['tax'])]\ndf_with_total = df.add_column('total', total)\n\n# Chain column additions\ndf_enhanced = (\n    df.add_column('total', totals)\n      .add_column('formatted', formatted_values)\n)\n</code></pre>"},{"location":"guides/dataframe/#row-operations","title":"Row Operations","text":""},{"location":"guides/dataframe/#dropping-rows","title":"Dropping Rows","text":"<pre><code># Drop rows by index\ndf_cleaned = df.drop_rows([0, 5, 10])\n\n# Drop first row\ndf_no_header = df.drop_rows([0])\n\n# Drop multiple rows\ninvalid_indices = [i for i, row in enumerate(df) if row['status'] == 'invalid']\ndf_valid = df.drop_rows(invalid_indices)\n</code></pre>"},{"location":"guides/dataframe/#removing-duplicates","title":"Removing Duplicates","text":"<pre><code># Remove duplicate rows (all columns)\ndf_unique = df.drop_duplicates()\n\n# Remove duplicates by specific columns\ndf_unique_users = df.drop_duplicates(subset=['user_id'])\n\n# Remove duplicates considering multiple columns\ndf_unique_pairs = df.drop_duplicates(subset=['category', 'product'])\n</code></pre>"},{"location":"guides/dataframe/#filling-missing-values","title":"Filling Missing Values","text":"<pre><code># Fill all None with single value\ndf_filled = df.fillna(0)\n\n# Column-specific fill values\ndf_filled = df.fillna({\n    'age': 0,\n    'name': 'Unknown',\n    'score': -1\n})\n\n# Partial filling (only specified columns)\ndf_partial = df.fillna({'age': 0})  # Other columns keep None\n</code></pre>"},{"location":"guides/dataframe/#concatenating-dataframes","title":"Concatenating DataFrames","text":"<pre><code># Stack DataFrames vertically\ndf1 = DataFrame.from_dict({'name': ['Alice'], 'age': [25]})\ndf2 = DataFrame.from_dict({'name': ['Bob'], 'age': [30]})\ncombined = df1.concat(df2)\n\n# Combine multiple DataFrames\ndfs = [df1, df2, df3]\nresult = dfs[0]\nfor df in dfs[1:]:\n    result = result.concat(df)\n</code></pre>"},{"location":"guides/dataframe/#reshaping-operations","title":"Reshaping Operations","text":"<p>The <code>melt()</code> method transforms DataFrames from wide format (many columns) to long format (fewer columns, more rows). This is essential for preparing data for analysis, visualization, or API interchange.</p>"},{"location":"guides/dataframe/#understanding-wide-vs-long-format","title":"Understanding Wide vs Long Format","text":"<p>Wide Format: Multiple measurement columns <pre><code># Example: Student grades across subjects\ndf_wide = DataFrame.from_dict({\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'math': [90, 78, 95],\n    'science': [85, 92, 89],\n    'history': [88, 81, 93]\n})\n# name    | math | science | history\n# Alice   | 90   | 85      | 88\n# Bob     | 78   | 92      | 81\n# Charlie | 95   | 89      | 93\n</code></pre></p> <p>Long Format: Single measurement column with category identifier <pre><code># Melt to long format\ndf_long = df_wide.melt(\n    id_vars=['name'],\n    value_vars=['math', 'science', 'history'],\n    var_name='subject',\n    value_name='score'\n)\n# name    | subject | score\n# Alice   | math    | 90\n# Alice   | science | 85\n# Alice   | history | 88\n# Bob     | math    | 78\n# ...\n</code></pre></p>"},{"location":"guides/dataframe/#basic-melt-usage","title":"Basic melt() Usage","text":"<pre><code>from chapkit.data import DataFrame\n\n# Create wide format data\ndf = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget'],\n    'q1_sales': [1000, 800],\n    'q2_sales': [1100, 850],\n    'q3_sales': [1200, 900]\n})\n\n# Melt to long format\nmelted = df.melt(\n    id_vars=['product'],           # Columns to keep as identifiers\n    value_vars=['q1_sales', 'q2_sales', 'q3_sales'],  # Columns to unpivot\n    var_name='quarter',            # Name for variable column\n    value_name='sales'             # Name for value column\n)\n\n# Result:\n# product | quarter   | sales\n# Widget  | q1_sales  | 1000\n# Widget  | q2_sales  | 1100\n# Widget  | q3_sales  | 1200\n# Gadget  | q1_sales  | 800\n# Gadget  | q2_sales  | 850\n# Gadget  | q3_sales  | 900\n</code></pre>"},{"location":"guides/dataframe/#melt-parameters","title":"melt() Parameters","text":"Parameter Type Default Description <code>id_vars</code> <code>list[str] \\| None</code> <code>None</code> Columns to keep as identifiers (not melted) <code>value_vars</code> <code>list[str] \\| None</code> <code>None</code> Columns to unpivot (if None, uses all non-id columns) <code>var_name</code> <code>str</code> <code>\"variable\"</code> Name for the column containing former column names <code>value_name</code> <code>str</code> <code>\"value\"</code> Name for the column containing values"},{"location":"guides/dataframe/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/dataframe/#surveyquestionnaire-data","title":"Survey/Questionnaire Data","text":"<pre><code># Wide format: each question is a column\nsurvey = DataFrame.from_dict({\n    'respondent_id': [1, 2, 3],\n    'age': [25, 30, 35],\n    'q1_rating': [5, 4, 5],\n    'q2_rating': [4, 4, 5],\n    'q3_rating': [5, 3, 5]\n})\n\n# Melt to long format for analysis\nresponses = survey.melt(\n    id_vars=['respondent_id', 'age'],\n    value_vars=['q1_rating', 'q2_rating', 'q3_rating'],\n    var_name='question',\n    value_name='rating'\n)\n\n# Now easy to analyze: average rating per question\navg_by_question = responses.groupby('question').mean('rating')\n</code></pre>"},{"location":"guides/dataframe/#time-series-data","title":"Time Series Data","text":"<pre><code># Wide format: each month is a column\nsales = DataFrame.from_dict({\n    'region': ['North', 'South', 'East'],\n    'product': ['Widget', 'Widget', 'Widget'],\n    'jan': [1000, 1200, 900],\n    'feb': [1100, 1300, 950],\n    'mar': [1200, 1400, 1000]\n})\n\n# Melt to time series format\ntime_series = sales.melt(\n    id_vars=['region', 'product'],\n    value_vars=['jan', 'feb', 'mar'],\n    var_name='month',\n    value_name='sales'\n)\n\n# Now can analyze trends over time\ntotal_by_month = time_series.groupby('month').sum('sales')\n</code></pre>"},{"location":"guides/dataframe/#api-data-standardization","title":"API Data Standardization","text":"<pre><code># API returns different metrics as columns\nsensor_data = DataFrame.from_dict({\n    'sensor_id': ['s1', 's2'],\n    'location': ['room_a', 'room_b'],\n    'temp_c': [22.5, 23.1],\n    'humidity_pct': [45, 48],\n    'pressure_kpa': [101.3, 101.2]\n})\n\n# Standardize to key-value format\nmetrics = sensor_data.melt(\n    id_vars=['sensor_id', 'location'],\n    value_vars=['temp_c', 'humidity_pct', 'pressure_kpa'],\n    var_name='metric_type',\n    value_name='metric_value'\n)\n\n# Easier to store/process uniform metric records\n</code></pre>"},{"location":"guides/dataframe/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/dataframe/#combining-melt-with-groupby","title":"Combining melt() with groupby()","text":"<pre><code># Wide format sales data\ndf = DataFrame.from_dict({\n    'region': ['North', 'North', 'South', 'South'],\n    'product': ['Widget', 'Gadget', 'Widget', 'Gadget'],\n    'q1': [1000, 800, 1200, 900],\n    'q2': [1100, 850, 1300, 950]\n})\n\n# Melt then aggregate\nmelted = df.melt(\n    id_vars=['region', 'product'],\n    value_vars=['q1', 'q2'],\n    var_name='quarter',\n    value_name='sales'\n)\n\n# Total sales by region\nregion_totals = melted.groupby('region').sum('sales')\n\n# Average sales by product\nproduct_avg = melted.groupby('product').mean('sales')\n</code></pre>"},{"location":"guides/dataframe/#filtering-after-melt","title":"Filtering After melt()","text":"<pre><code># Melt then filter for specific conditions\nmelted = df.melt(id_vars=['product'], value_vars=['q1', 'q2', 'q3', 'q4'])\n\n# Only keep quarters with sales &gt; 1000\nhigh_sales = melted.filter(lambda row: row['value'] &gt; 1000)\n\n# Group filtered results\nsummary = high_sales.groupby('product').count()\n</code></pre>"},{"location":"guides/dataframe/#melt-design-notes","title":"melt() Design Notes","text":"<ul> <li>Stdlib only: No external dependencies, pure Python implementation</li> <li>Immutable: Returns new DataFrame, original unchanged</li> <li>None values: Preserved during transformation</li> <li>Column order: Results maintain row order and value_vars order</li> <li>Validation: Raises <code>KeyError</code> for non-existent columns, <code>ValueError</code> for name conflicts</li> </ul>"},{"location":"guides/dataframe/#pivot-long-to-wide-format","title":"pivot() - Long to Wide Format","text":"<p>The <code>pivot()</code> method is the inverse of <code>melt()</code> - it transforms data from long format to wide format by spreading row values into columns.</p>"},{"location":"guides/dataframe/#basic-pivot-usage","title":"Basic pivot() Usage","text":"<pre><code>from chapkit.data import DataFrame\n\n# Long format data\ndf_long = DataFrame.from_dict({\n    'date': ['2024-01', '2024-01', '2024-02', '2024-02'],\n    'metric': ['sales', 'profit', 'sales', 'profit'],\n    'value': [1000, 200, 1100, 220]\n})\n\n# Pivot to wide format\ndf_wide = df_long.pivot(index='date', columns='metric', values='value')\n\n# Result:\n# date    | profit | sales\n# 2024-01 | 200    | 1000\n# 2024-02 | 220    | 1100\n</code></pre>"},{"location":"guides/dataframe/#pivot-parameters","title":"pivot() Parameters","text":"Parameter Type Description <code>index</code> <code>str</code> Column to use as row index in result <code>columns</code> <code>str</code> Column whose unique values become new columns <code>values</code> <code>str</code> Column containing values to fill pivoted table"},{"location":"guides/dataframe/#pivot-use-cases","title":"Pivot Use Cases","text":"<p>Report Generation: <pre><code># Student grades in long format\ndf_long = DataFrame.from_dict({\n    'student': ['Alice', 'Alice', 'Alice', 'Bob', 'Bob', 'Bob'],\n    'subject': ['math', 'science', 'history', 'math', 'science', 'history'],\n    'score': [90, 85, 88, 78, 92, 81]\n})\n\n# Pivot for report card\nreport = df_long.pivot(index='student', columns='subject', values='score')\n# student | history | math | science\n# Alice   | 88      | 90   | 85\n# Bob     | 81      | 78   | 92\n</code></pre></p> <p>Time Series Restructuring: <pre><code># API returns time series in long format\ntime_series = DataFrame.from_dict({\n    'week': [1, 1, 2, 2],\n    'day': ['mon', 'tue', 'mon', 'tue'],\n    'hours': [8, 7, 9, 8]\n})\n\n# Pivot for weekly view\nweekly = time_series.pivot(index='week', columns='day', values='hours')\n# week | mon | tue\n# 1    | 8   | 7\n# 2    | 9   | 8\n</code></pre></p>"},{"location":"guides/dataframe/#combining-melt-and-pivot","title":"Combining melt() and pivot()","text":"<p>These operations are inverses - you can round-trip data:</p> <pre><code># Start with wide format\ndf_wide = DataFrame.from_dict({\n    'id': [1, 2],\n    'a': [10, 20],\n    'b': [30, 40]\n})\n\n# Melt to long format\ndf_long = df_wide.melt(id_vars=['id'], value_vars=['a', 'b'])\n# id | variable | value\n# 1  | a        | 10\n# 1  | b        | 30\n# 2  | a        | 20\n# 2  | b        | 40\n\n# Pivot back to wide format\ndf_restored = df_long.pivot(index='id', columns='variable', values='value')\n# id | a  | b\n# 1  | 10 | 30\n# 2  | 20 | 40\n</code></pre>"},{"location":"guides/dataframe/#pivot-design-notes","title":"pivot() Design Notes","text":"<ul> <li>Duplicate detection: Raises <code>ValueError</code> if index/column combinations are duplicated</li> <li>Sparse data: Missing combinations filled with <code>None</code></li> <li>Column ordering: Result columns are sorted alphabetically</li> <li>Validation: Raises <code>KeyError</code> for non-existent column names</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#transpose-swap-rows-and-columns","title":"transpose() - Swap Rows and Columns","text":"<p>The <code>transpose()</code> method swaps rows and columns (matrix transpose). The first column becomes column headers, and column names become the first column values.</p>"},{"location":"guides/dataframe/#basic-transpose-usage","title":"Basic transpose() Usage","text":"<pre><code>from chapkit.data import DataFrame\n\n# Metrics as rows\ndf = DataFrame.from_dict({\n    'metric': ['revenue', 'profit', 'growth'],\n    '2023': [1000, 200, 0.10],\n    '2024': [1200, 250, 0.20]\n})\n\n# Transpose to have metrics as columns\ndf_t = df.transpose()\n\n# Result:\n# index | revenue | profit | growth\n# 2023  | 1000    | 200    | 0.10\n# 2024  | 1200    | 250    | 0.20\n</code></pre>"},{"location":"guides/dataframe/#transpose-use-cases","title":"Transpose Use Cases","text":"<p>Report Formatting: <pre><code># API returns quarterly metrics by region\ndata = DataFrame.from_dict({\n    'region': ['North', 'South', 'East', 'West'],\n    'Q1': [100, 200, 150, 180],\n    'Q2': [110, 210, 160, 190],\n    'Q3': [120, 220, 170, 200],\n    'Q4': [130, 230, 180, 210]\n})\n\n# Transpose for quarterly view\nquarterly = data.transpose()\n# index | North | South | East | West\n# Q1    | 100   | 200   | 150  | 180\n# Q2    | 110   | 210   | 160  | 190\n# ...\n</code></pre></p> <p>Rotating Time Series: <pre><code># Monthly sales by product (wide format)\nmonthly = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget', 'Tool'],\n    'jan': [100, 80, 60],\n    'feb': [110, 85, 65],\n    'mar': [120, 90, 70]\n})\n\n# Transpose to have products as columns\nby_month = monthly.transpose()\n# index | Widget | Gadget | Tool\n# jan   | 100    | 80     | 60\n# feb   | 110    | 85     | 65\n# mar   | 120    | 90     | 70\n</code></pre></p> <p>Preparing for Visualization: <pre><code># Data with entities as rows\nentities = DataFrame.from_dict({\n    'entity': ['Team A', 'Team B', 'Team C'],\n    'score': [85, 92, 78],\n    'rank': [2, 1, 3]\n})\n\n# Transpose for chart libraries expecting columns as series\nchart_data = entities.transpose()\n# index | Team A | Team B | Team C\n# score | 85     | 92     | 78\n# rank  | 2      | 1      | 3\n</code></pre></p>"},{"location":"guides/dataframe/#combining-with-other-operations","title":"Combining with Other Operations","text":"<p>Transpose + Filter + Transpose: <pre><code># Start with wide format\ndf = DataFrame.from_dict({\n    'id': [1, 2, 3],\n    'metric_a': [100, 200, 300],\n    'metric_b': [10, 20, 30],\n    'metric_c': [5, 15, 25]\n})\n\n# Transpose to work with metrics as rows\ndf_t = df.transpose()\n\n# Filter for specific IDs (now columns)\n# Then transpose back\nfiltered = df_t.filter(lambda row: row['1'] &gt; 50)\nresult = filtered.transpose()\n</code></pre></p> <p>Transpose for Aggregation: <pre><code># Quarterly data by product\ndf = DataFrame.from_dict({\n    'product': ['Widget', 'Gadget'],\n    'Q1': [100, 80],\n    'Q2': [110, 85],\n    'Q3': [120, 90],\n    'Q4': [130, 95]\n})\n\n# Transpose and melt for time series analysis\ndf_t = df.transpose()\n# Now can use melt() or groupby() on transposed data\n</code></pre></p>"},{"location":"guides/dataframe/#transpose-design-notes","title":"transpose() Design Notes","text":"<ul> <li>First column as index: First column values become new column names (converted to strings)</li> <li>Column names as data: Original column names (except first) become first column values</li> <li>Immutable: Returns new DataFrame, original unchanged</li> <li>None values: Preserved during transformation</li> <li>Empty handling: Empty DataFrames return empty result</li> <li>Round-trip: Transposing twice restores original structure (with potential column name changes)</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#combining-dataframes","title":"Combining DataFrames","text":""},{"location":"guides/dataframe/#merge-database-style-joins","title":"merge() - Database-Style Joins","text":"<p>The <code>merge()</code> method combines two DataFrames using SQL-like join operations (inner, left, right, outer).</p>"},{"location":"guides/dataframe/#basic-merge-usage","title":"Basic merge() Usage","text":"<pre><code>from chapkit.data import DataFrame\n\n# Users data\nusers = DataFrame.from_dict({\n    'user_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\n# Orders data\norders = DataFrame.from_dict({\n    'user_id': [1, 1, 2],\n    'amount': [100, 150, 200]\n})\n\n# Join to get user names with orders\nresult = orders.merge(users, on='user_id', how='left')\n# user_id | amount | name\n# 1       | 100    | Alice\n# 1       | 150    | Alice\n# 2       | 200    | Bob\n</code></pre>"},{"location":"guides/dataframe/#merge-parameters","title":"merge() Parameters","text":"Parameter Type Default Description <code>other</code> <code>DataFrame</code> - DataFrame to merge with <code>on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) to join on (must exist in both) <code>how</code> <code>\"inner\" \\| \"left\" \\| \"right\" \\| \"outer\"</code> <code>\"inner\"</code> Type of join to perform <code>left_on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) from left DataFrame to join on <code>right_on</code> <code>str \\| list[str]</code> <code>None</code> Column(s) from right DataFrame to join on <code>suffixes</code> <code>tuple[str, str]</code> <code>(\"_x\", \"_y\")</code> Suffixes for overlapping column names"},{"location":"guides/dataframe/#join-types","title":"Join Types","text":"<p>Inner Join (default): Only rows with matching keys in both DataFrames <pre><code>left = DataFrame.from_dict({'key': [1, 2, 3], 'left_val': ['a', 'b', 'c']})\nright = DataFrame.from_dict({'key': [1, 2, 4], 'right_val': ['x', 'y', 'z']})\n\nresult = left.merge(right, on='key', how='inner')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n</code></pre></p> <p>Left Join: All rows from left, matched rows from right (None if no match) <pre><code>result = left.merge(right, on='key', how='left')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 3   | c        | None\n</code></pre></p> <p>Right Join: All rows from right, matched rows from left (None if no match) <pre><code>result = left.merge(right, on='key', how='right')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 4   | None     | z\n</code></pre></p> <p>Outer Join: All rows from both DataFrames <pre><code>result = left.merge(right, on='key', how='outer')\n# key | left_val | right_val\n# 1   | a        | x\n# 2   | b        | y\n# 3   | c        | None\n# 4   | None     | z\n</code></pre></p>"},{"location":"guides/dataframe/#common-merge-patterns","title":"Common Merge Patterns","text":"<p>Enriching Data from Another Service: <pre><code># Orders from one service\norders = DataFrame.from_dict({\n    'order_id': [101, 102, 103],\n    'user_id': [1, 2, 1],\n    'amount': [50, 75, 100]\n})\n\n# User details from another service\nusers = DataFrame.from_dict({\n    'user_id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'tier': ['gold', 'silver', 'gold']\n})\n\n# Enrich orders with user data\nenriched = orders.merge(users, on='user_id', how='left')\n# order_id | user_id | amount | name  | tier\n# 101      | 1       | 50     | Alice | gold\n# 102      | 2       | 75     | Bob   | silver\n# 103      | 1       | 100    | Alice | gold\n</code></pre></p> <p>Different Column Names: <pre><code>products = DataFrame.from_dict({\n    'product_id': [1, 2, 3],\n    'product_name': ['Widget', 'Gadget', 'Tool']\n})\n\nsales = DataFrame.from_dict({\n    'item_id': [1, 2, 1],\n    'quantity': [10, 5, 8]\n})\n\n# Join on different column names\nresult = sales.merge(\n    products,\n    left_on='item_id',\n    right_on='product_id',\n    how='left'\n)\n</code></pre></p> <p>Multiple Join Keys: <pre><code># Join on multiple columns for composite keys\nresult = df1.merge(df2, on=['region', 'product'], how='inner')\n</code></pre></p> <p>Handling Column Conflicts: <pre><code># Both DataFrames have 'value' column\nleft = DataFrame.from_dict({'key': [1, 2], 'value': [10, 20]})\nright = DataFrame.from_dict({'key': [1, 2], 'value': [100, 200]})\n\n# Use suffixes to distinguish\nresult = left.merge(right, on='key', suffixes=('_old', '_new'))\n# key | value_old | value_new\n# 1   | 10        | 100\n# 2   | 20        | 200\n</code></pre></p>"},{"location":"guides/dataframe/#merge-design-notes","title":"merge() Design Notes","text":"<ul> <li>One-to-many joins: Cartesian product of matching rows</li> <li>None handling: None values in keys can match None</li> <li>Validation: Raises <code>KeyError</code> for non-existent columns, <code>ValueError</code> for invalid parameters</li> <li>Performance: Uses dict-based lookup for efficient joins</li> <li>Stdlib only: No external dependencies</li> </ul>"},{"location":"guides/dataframe/#missing-data-operations","title":"Missing Data Operations","text":""},{"location":"guides/dataframe/#detecting-missing-values","title":"Detecting Missing Values","text":"<pre><code># Check for None values (returns DataFrame of booleans)\nis_null = df.isna()\nprint(is_null.data)  # [[False, True], [True, False], ...]\n\n# Check for non-None values\nnot_null = df.notna()\n\n# Check if columns have None values\nnulls = df.has_nulls()\nprint(nulls)  # {'age': False, 'email': True}\n</code></pre>"},{"location":"guides/dataframe/#removing-missing-values","title":"Removing Missing Values","text":"<pre><code># Drop rows with any None values (default)\nclean_df = df.dropna()\n\n# Drop rows only if all values are None\ndf.dropna(axis=0, how='all')\n\n# Drop columns with any None values\ndf.dropna(axis=1, how='any')\n\n# Drop columns only if all values are None\ndf.dropna(axis=1, how='all')\n</code></pre>"},{"location":"guides/dataframe/#filling-missing-values_1","title":"Filling Missing Values","text":"<pre><code># Fill all None with a single value\ndf.fillna(0)\n\n# Fill with column-specific values\ndf.fillna({\n    'age': 0,\n    'name': 'Unknown',\n    'score': -1\n})\n</code></pre>"},{"location":"guides/dataframe/#complete-data-cleaning-pipeline","title":"Complete Data Cleaning Pipeline","text":"<pre><code># Clean data by removing bad rows and filling missing values\nclean_df = (\n    df.dropna(axis=1, how='all')     # Remove empty columns\n    .fillna({'age': 0, 'name': ''})  # Fill remaining None\n    .filter(lambda row: row['age'] &gt;= 0)  # Remove invalid rows\n)\n</code></pre>"},{"location":"guides/dataframe/#dataframe-utilities","title":"DataFrame Utilities","text":""},{"location":"guides/dataframe/#comparing-dataframes","title":"Comparing DataFrames","text":"<pre><code># Check if two DataFrames are identical\ndf1 = DataFrame.from_dict({'a': [1, 2], 'b': [3, 4]})\ndf2 = DataFrame.from_dict({'a': [1, 2], 'b': [3, 4]})\n\nassert df1.equals(df2)  # True\n\n# Order matters\ndf3 = DataFrame.from_dict({'a': [2, 1], 'b': [4, 3]})\nassert not df1.equals(df3)  # False\n</code></pre>"},{"location":"guides/dataframe/#copying-dataframes","title":"Copying DataFrames","text":"<pre><code># Create independent copy\ndf_copy = df.deepcopy()\n\n# Modifications to copy don't affect original\ndf_copy.data[0][0] = 'modified'\nassert df.data[0][0] != 'modified'\n</code></pre>"},{"location":"guides/dataframe/#counting-unique-values","title":"Counting Unique Values","text":"<pre><code># Count unique values in a column\nunique_count = df.nunique('category')\nprint(f\"Found {unique_count} unique categories\")\n\n# Get the actual unique values\nunique_values = df.unique('category')\nprint(f\"Categories: {unique_values}\")\n\n# Count occurrences of each value\nvalue_counts = df.value_counts('status')\nprint(value_counts)  # {'active': 10, 'inactive': 5}\n</code></pre>"},{"location":"guides/dataframe/#statistical-analysis","title":"Statistical Analysis","text":""},{"location":"guides/dataframe/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Generate statistical summary\nstats = df.describe()\n\n# Results include: count, mean, std, min, 25%, 50%, 75%, max\n# Non-numeric columns show None for statistics\nprint(stats.get_column('age'))  # [5, 32.5, 4.2, 25, 28, 31, 36, 45]\nprint(stats.get_column('stat'))  # ['count', 'mean', 'std', ...]\n</code></pre>"},{"location":"guides/dataframe/#group-by-operations","title":"Group By Operations","text":"<p>The <code>groupby()</code> method provides SQL-like aggregation capabilities. It returns a <code>GroupBy</code> helper object that builds groups internally and provides aggregation methods.</p> <p>How it works: - Groups rows by unique values in the specified column - Filters out rows where the grouping column is None - Provides aggregation methods that return new DataFrames - Uses eager evaluation (groups are built immediately) - Uses only Python stdlib (statistics module for mean)</p> <p>Available aggregation methods:</p> Method Description Returns <code>count()</code> Count rows per group DataFrame with <code>[group_col, 'count']</code> <code>sum(col)</code> Sum numeric column per group DataFrame with <code>[group_col, 'col_sum']</code> <code>mean(col)</code> Average numeric column per group DataFrame with <code>[group_col, 'col_mean']</code> <code>min(col)</code> Minimum value per group DataFrame with <code>[group_col, 'col_min']</code> <code>max(col)</code> Maximum value per group DataFrame with <code>[group_col, 'col_max']</code> <p>Basic usage:</p> <pre><code>from chapkit.data import DataFrame\n\n# Sample sales data\ndf = DataFrame(\n    columns=['region', 'product', 'sales', 'quantity'],\n    data=[\n        ['North', 'Widget', 1000, 10],\n        ['North', 'Gadget', 1500, 15],\n        ['South', 'Widget', 800, 8],\n        ['South', 'Gadget', 1200, 12],\n        ['North', 'Widget', 1100, 11],\n    ]\n)\n\n# Count rows per group\nregion_counts = df.groupby('region').count()\n# Returns: DataFrame({'region': ['North', 'South'], 'count': [3, 2]})\n\n# Sum sales by region\ntotal_sales = df.groupby('region').sum('sales')\n# Returns: DataFrame({'region': ['North', 'South'], 'sales_sum': [3600, 2000]})\n\n# Average quantity by product\navg_qty = df.groupby('product').mean('quantity')\n# Returns: DataFrame({'product': ['Widget', 'Gadget'], 'quantity_mean': [9.67, 13.5]})\n\n# Find min/max sales by region\nmin_sales = df.groupby('region').min('sales')\nmax_sales = df.groupby('region').max('sales')\n</code></pre> <p>Advanced patterns:</p> <pre><code># Multiple aggregations on same grouping\ngrouped = df.groupby('region')\nsummary = DataFrame(\n    columns=['region', 'count', 'total_sales', 'avg_sales'],\n    data=[\n        [\n            group,\n            grouped.count().filter(lambda r: r['region'] == group)[0]['count'],\n            grouped.sum('sales').filter(lambda r: r['region'] == group)[0]['sales_sum'],\n            grouped.mean('sales').filter(lambda r: r['region'] == group)[0]['sales_mean'],\n        ]\n        for group in df.unique('region')\n    ]\n)\n\n# Combine with filtering\nhigh_sales = df.filter(lambda r: r['sales'] &gt; 1000)\nsummary = high_sales.groupby('region').count()\n\n# Chain with other operations\ndf.groupby('product').sum('sales').sort('sales_sum', reverse=True).head(5)\n</code></pre> <p>Design notes: - Groups are built eagerly when <code>groupby()</code> is called - Aggregation methods filter out None values automatically - All methods return new DataFrame instances (immutable pattern) - Uses stdlib only (no pandas/numpy dependencies) - Raises KeyError if column not found</p>"},{"location":"guides/dataframe/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/dataframe/#data-pipeline","title":"Data Pipeline","text":"<pre><code># Read, process, write\ndf = (\n    DataFrame.from_csv(\"input.csv\")\n    .select([\"name\", \"age\", \"score\"])\n    .rename({\"score\": \"grade\"})\n    .filter(lambda row: row['age'] &gt;= 18)\n    .drop_duplicates(subset=['name'])\n    .fillna({'grade': 0})\n    .head(1000)\n)\ndf.to_csv(\"output.csv\")\n\n# Advanced pipeline with transformations\ndf = (\n    DataFrame.from_csv(\"sales.csv\")\n    .drop(['internal_id', 'debug_flag'])\n    .rename({'product_name': 'product', 'sale_amount': 'amount'})\n    .filter(lambda row: row['amount'] &gt; 0)\n    .apply(str.upper, 'product')\n    .drop_duplicates(subset=['order_id'])\n    .sort('amount', ascending=False)\n)\n</code></pre>"},{"location":"guides/dataframe/#api-data-validation","title":"API Data Validation","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom chapkit.data import DataFrame\n\n@app.post(\"/data/$validate\")\nasync def validate_data(file: UploadFile):\n    \"\"\"Validate uploaded CSV data.\"\"\"\n    content = await file.read()\n    df = DataFrame.from_csv(csv_string=content.decode())\n\n    # Validate structure\n    try:\n        df.validate_structure()\n    except ValueError as e:\n        raise HTTPException(400, f\"Invalid structure: {e}\")\n\n    # Check required columns\n    required = [\"user_id\", \"timestamp\", \"value\"]\n    missing = set(required) - set(df.columns)\n    if missing:\n        raise HTTPException(400, f\"Missing columns: {missing}\")\n\n    # Check for nulls\n    nulls = df.has_nulls()\n    if any(nulls.get(col, False) for col in required):\n        raise HTTPException(400, \"Required columns contain null values\")\n\n    # Return metadata\n    return {\n        \"rows\": df.shape[0],\n        \"columns\": df.columns,\n        \"types\": df.infer_types(),\n        \"sample\": df.head(5).to_dict(orient=\"records\")\n    }\n</code></pre>"},{"location":"guides/dataframe/#data-transformation","title":"Data Transformation","text":"<pre><code>def clean_dataframe(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Clean and standardize DataFrame.\"\"\"\n    # Remove unnecessary columns\n    df = df.drop([\"temp\", \"debug\"])\n\n    # Rename for consistency\n    df = df.rename({\n        \"user_name\": \"name\",\n        \"user_age\": \"age\"\n    })\n\n    # Validate\n    df.validate_structure()\n\n    return df\n</code></pre>"},{"location":"guides/dataframe/#format-conversion","title":"Format Conversion","text":"<pre><code># CSV to Pandas\ndf = DataFrame.from_csv(\"data.csv\")\npandas_df = df.to_pandas()\n\n# Pandas to CSV\ndf = DataFrame.from_pandas(pandas_df)\ndf.to_csv(\"output.csv\")\n\n# Cross-library conversion\npolars_df = DataFrame.from_pandas(pandas_df).to_polars()\n</code></pre>"},{"location":"guides/dataframe/#api-response-formats","title":"API Response Formats","text":""},{"location":"guides/dataframe/#json-response","title":"JSON Response","text":"<pre><code>from fastapi import FastAPI\nfrom chapkit.data import DataFrame\n\n@app.get(\"/data\")\nasync def get_data():\n    \"\"\"Return data as JSON.\"\"\"\n    df = get_dataframe()\n    return df.to_dict(orient=\"records\")\n</code></pre>"},{"location":"guides/dataframe/#csv-download","title":"CSV Download","text":"<pre><code>from fastapi.responses import Response\n\n@app.get(\"/data/export.csv\")\nasync def download_csv():\n    \"\"\"Download data as CSV.\"\"\"\n    df = get_dataframe()\n    csv_data = df.to_csv()\n\n    return Response(\n        content=csv_data,\n        media_type=\"text/csv\",\n        headers={\n            \"Content-Disposition\": \"attachment; filename=data.csv\"\n        }\n    )\n</code></pre>"},{"location":"guides/dataframe/#paginated-response","title":"Paginated Response","text":"<pre><code>from pydantic import BaseModel\n\nclass PaginatedData(BaseModel):\n    \"\"\"Paginated DataFrame response.\"\"\"\n    total: int\n    page: int\n    page_size: int\n    data: list[dict]\n\n@app.get(\"/data\", response_model=PaginatedData)\nasync def get_paginated_data(page: int = 1, page_size: int = 100):\n    \"\"\"Return paginated data.\"\"\"\n    df = get_dataframe()\n\n    # Calculate pagination\n    total = df.shape[0]\n    start = (page - 1) * page_size\n\n    # Get page using head/tail\n    if start + page_size &lt; total:\n        page_df = df.tail(-start).head(page_size)\n    else:\n        page_df = df.tail(-start)\n\n    return PaginatedData(\n        total=total,\n        page=page,\n        page_size=page_size,\n        data=page_df.to_dict(orient=\"records\")\n    )\n</code></pre>"},{"location":"guides/dataframe/#error-handling","title":"Error Handling","text":""},{"location":"guides/dataframe/#import-errors","title":"Import Errors","text":"<p>DataFrames with optional dependencies raise clear errors:</p> <pre><code>try:\n    df.to_pandas()\nexcept ImportError as e:\n    print(e)\n    # \"pandas is required for to_pandas(). Install with: uv add pandas\"\n</code></pre>"},{"location":"guides/dataframe/#validation-errors","title":"Validation Errors","text":"<pre><code># Column not found\ntry:\n    df.select([\"nonexistent\"])\nexcept KeyError as e:\n    print(e)  # \"Column 'nonexistent' not found in DataFrame\"\n\n# Invalid structure\ntry:\n    df = DataFrame(columns=[\"a\", \"b\"], data=[[1, 2, 3]])\n    df.validate_structure()\nexcept ValueError as e:\n    print(e)  # \"Row 0 has 3 values, expected 2\"\n</code></pre>"},{"location":"guides/dataframe/#csv-errors","title":"CSV Errors","text":"<pre><code># File not found\ntry:\n    df = DataFrame.from_csv(\"missing.csv\")\nexcept FileNotFoundError as e:\n    print(e)  # \"File not found: missing.csv\"\n\n# Invalid parameters\ntry:\n    df = DataFrame.from_csv()  # Neither path nor csv_string\nexcept ValueError as e:\n    print(e)  # \"Either path or csv_string must be provided\"\n</code></pre>"},{"location":"guides/dataframe/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/dataframe/#when-to-use-dataframe","title":"When to Use DataFrame","text":"<p>Good use cases: - API data interchange (JSON \u2194 DataFrame \u2194 CSV) - Small to medium datasets (&lt;100k rows) - Format conversion between libraries - Data validation and inspection - Prototyping and development</p> <p>Not recommended for: - Large datasets (&gt;1M rows) - use pandas/polars directly - Heavy data transformations - use specialized libraries - Production analytics - use pandas/polars/DuckDB - High-performance computing - use NumPy/pandas</p>"},{"location":"guides/dataframe/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code># DataFrame stores data as list of lists (row-oriented)\n# For large datasets, convert to columnar format:\npandas_df = df.to_pandas()  # More efficient for operations\n\n# For very large files, consider streaming:\n# - Read in chunks with pandas\n# - Process incrementally\n# - Use DataFrame for API boundaries only\n</code></pre>"},{"location":"guides/dataframe/#testing-with-dataframe","title":"Testing with DataFrame","text":""},{"location":"guides/dataframe/#test-data-creation","title":"Test Data Creation","text":"<pre><code>import pytest\nfrom chapkit.data import DataFrame\n\n@pytest.fixture\ndef sample_data():\n    \"\"\"Create sample DataFrame for testing.\"\"\"\n    return DataFrame.from_dict({\n        \"id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"score\": [95, 87, 92]\n    })\n\ndef test_data_processing(sample_data):\n    \"\"\"Test data processing pipeline.\"\"\"\n    result = sample_data.select([\"name\", \"score\"])\n    assert result.shape == (3, 2)\n    assert \"id\" not in result.columns\n</code></pre>"},{"location":"guides/dataframe/#csv-round-trip-testing","title":"CSV Round-Trip Testing","text":"<pre><code>def test_csv_roundtrip(tmp_path):\n    \"\"\"Test CSV export and import.\"\"\"\n    # Create test data\n    original = DataFrame.from_dict({\n        \"name\": [\"Alice\", \"Bob\"],\n        \"age\": [25, 30]\n    })\n\n    # Write to file\n    csv_file = tmp_path / \"test.csv\"\n    original.to_csv(csv_file)\n\n    # Read back\n    restored = DataFrame.from_csv(csv_file)\n\n    # Verify (note: CSV makes all values strings)\n    assert restored.columns == original.columns\n    assert restored.shape == original.shape\n</code></pre>"},{"location":"guides/dataframe/#best-practices","title":"Best Practices","text":""},{"location":"guides/dataframe/#recommended-practices","title":"Recommended Practices","text":"<ul> <li>Validate early: Call <code>validate_structure()</code> after data ingestion</li> <li>Check types: Use <code>infer_types()</code> to understand your data</li> <li>Handle nulls: Use <code>has_nulls()</code> to detect data quality issues</li> <li>Immutable pattern: Chain operations without modifying originals</li> <li>Small data: Use DataFrame for API boundaries, not heavy processing</li> <li>Clear errors: Let ImportError guide users to install dependencies</li> <li>CSV for interchange: Use CSV for human-readable data exchange</li> </ul>"},{"location":"guides/dataframe/#avoid","title":"Avoid","text":"<ul> <li>Large datasets: Don't use for &gt;100k rows (use pandas/polars instead)</li> <li>Heavy operations: Don't use for joins, aggregations, complex queries</li> <li>In-place modification: Don't try to modify DataFrames (they're immutable)</li> <li>Type assumptions: CSV imports make all values strings</li> <li>Missing validation: Always validate data from external sources</li> </ul>"},{"location":"guides/dataframe/#dependencies","title":"Dependencies","text":""},{"location":"guides/dataframe/#core-required","title":"Core (Required)","text":"<ul> <li>pydantic: For data validation and serialization</li> </ul>"},{"location":"guides/dataframe/#optional-no-dependencies","title":"Optional (No Dependencies)","text":"<ul> <li>CSV support: Uses Python stdlib <code>csv</code> module</li> <li>Data inspection: Uses Python stdlib <code>random</code> module</li> <li>Column operations: Pure Python (no dependencies)</li> <li>Validation: Pure Python (no dependencies)</li> </ul>"},{"location":"guides/dataframe/#optional-with-dependencies","title":"Optional (With Dependencies)","text":"<p>Install only what you need:</p> <pre><code># For pandas support\nuv add chapkit[pandas]\n\n# For polars support\nuv add chapkit[polars]\n\n# For xarray support (includes pandas, required for conversions)\nuv add chapkit[xarray]\n\n# For all dataframe libraries\nuv add chapkit[dataframe]\n\n# For PyArrow/Parquet support (future)\nuv add 'servicekit[arrow]'\n</code></pre>"},{"location":"guides/dataframe/#examples","title":"Examples","text":""},{"location":"guides/dataframe/#example-files","title":"Example Files","text":"<p>See <code>examples/</code> directory: - Basic DataFrame operations - API integration patterns - CSV upload/download - Data validation workflows</p>"},{"location":"guides/dataframe/#interactive-session","title":"Interactive Session","text":"<pre><code>from chapkit.data import DataFrame\n\n# Create sample data\ndf = DataFrame.from_dict({\n    \"product\": [\"Apple\", \"Banana\", \"Cherry\", \"Date\"],\n    \"price\": [1.2, 0.5, 3.0, 2.5],\n    \"stock\": [100, 150, 80, 60]\n})\n\n# Explore\nprint(f\"Shape: {df.shape}\")\nprint(f\"Columns: {df.columns}\")\nprint(df.head(2))\n\n# Analyze\nprint(f\"Types: {df.infer_types()}\")\nprint(f\"Has nulls: {df.has_nulls()}\")\n\n# Transform\nexpensive = df.select([\"product\", \"price\"])\nprint(expensive.to_dict(orient=\"records\"))\n\n# Export\nexpensive.to_csv(\"expensive_items.csv\")\n</code></pre>"},{"location":"guides/dataframe/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dataframe/#importerror-for-optional-libraries","title":"ImportError for Optional Libraries","text":"<p>Problem: Getting ImportError when using pandas/polars methods.</p> <p>Solution: <pre><code># Install required library\nuv add pandas  # or polars, xarray\n</code></pre></p>"},{"location":"guides/dataframe/#csv-values-all-strings","title":"CSV Values All Strings","text":"<p>Problem: After <code>from_csv()</code>, all values are strings.</p> <p>Solution: CSV format doesn't preserve types. Either: 1. Convert manually after import 2. Use <code>infer_types()</code> to detect types 3. Use Parquet for type preservation (future feature)</p>"},{"location":"guides/dataframe/#column-not-found-errors","title":"Column Not Found Errors","text":"<p>Problem: <code>KeyError: Column 'x' not found</code>.</p> <p>Solution: <pre><code># Check available columns\nprint(df.columns)\n\n# Case-sensitive matching\ndf.select([\"Name\"])  # Error if column is \"name\"\n</code></pre></p>"},{"location":"guides/dataframe/#memory-issues-with-large-data","title":"Memory Issues with Large Data","text":"<p>Problem: Running out of memory with large DataFrames.</p> <p>Solution: DataFrame is designed for small-medium data. For large datasets: <pre><code># Use pandas directly for large data\nimport pandas as pd\npandas_df = pd.read_csv(\"large_file.csv\", chunksize=10000)\n\n# Or use polars for better performance\nimport polars as pl\npolars_df = pl.read_csv(\"large_file.csv\")\n</code></pre></p>"},{"location":"guides/dataframe/#next-steps","title":"Next Steps","text":"<ul> <li>Learn More: See other guides for integrating DataFrame with APIs</li> <li>Contribute: Submit PRs for new format support (Parquet, Arrow, etc.)</li> <li>Examples: Check <code>examples/</code> directory for real-world usage</li> </ul> <p>For related features, see: - Servicekit Repository - Building services with servicekit</p>"},{"location":"guides/ml-workflows/","title":"ML Workflows","text":"<p>Chapkit provides a complete ML workflow system for training models and making predictions with artifact-based model storage, job scheduling, and hierarchical model lineage tracking.</p>"},{"location":"guides/ml-workflows/#quick-start","title":"Quick Start","text":""},{"location":"guides/ml-workflows/#functional-approach-recommended-for-simple-models","title":"Functional Approach (Recommended for Simple Models)","text":"<pre><code>from chapkit.artifact import ArtifactHierarchy\n\nfrom chapkit import BaseConfig\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo\nfrom chapkit.ml import FunctionalModelRunner\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nclass ModelConfig(BaseConfig):\n    prediction_periods: int = 3\n\nasync def on_train(config: ModelConfig, data: pd.DataFrame, geo=None):\n    X = data[[\"feature1\", \"feature2\"]]\n    y = data[\"target\"]\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\nasync def on_predict(config: ModelConfig, model, historic, future, geo=None):\n    X = future[[\"feature1\", \"feature2\"]]\n    future[\"sample_0\"] = model.predict(X)\n    return future\n\nrunner = FunctionalModelRunner(on_train=on_train, on_predict=on_predict)\n\napp = (\n    MLServiceBuilder(\n        info=MLServiceInfo(id=\"my-ml-service\", display_name=\"My ML Service\"),\n        config_schema=ModelConfig,\n        hierarchy=ArtifactHierarchy(name=\"ml\", level_labels={0: \"ml_training_workspace\", 1: \"ml_prediction\"}),\n        runner=runner,\n    )\n    .build()\n)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/ml-workflows/#class-based-approach-recommended-for-complex-models","title":"Class-Based Approach (Recommended for Complex Models)","text":"<pre><code>from chapkit.ml import BaseModelRunner\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomModelRunner(BaseModelRunner[ModelConfig]):\n    def __init__(self):\n        self.scaler = StandardScaler()\n\n    async def on_train(self, config: ModelConfig, data, geo=None):\n        X = data[[\"feature1\", \"feature2\"]]\n        y = data[\"target\"]\n\n        X_scaled = self.scaler.fit_transform(X)\n        model = LinearRegression()\n        model.fit(X_scaled, y)\n\n        return {\"model\": model, \"scaler\": self.scaler}\n\n    async def on_predict(self, config: ModelConfig, model, historic, future, geo=None):\n        X = future[[\"feature1\", \"feature2\"]]\n        X_scaled = model[\"scaler\"].transform(X)\n        future[\"sample_0\"] = model[\"model\"].predict(X_scaled)\n        return future\n\nrunner = CustomModelRunner()\n# Use same MLServiceBuilder setup as above\n</code></pre>"},{"location":"guides/ml-workflows/#shell-based-approach-language-agnostic","title":"Shell-Based Approach (Language-Agnostic)","text":"<pre><code>from chapkit.ml import ShellModelRunner\n\n# Use any command - python, Rscript, julia, or custom binaries\ntrain_command = \"python scripts/train.py --data {data_file}\"\n\npredict_command = (\n    \"python scripts/predict.py \"\n    \"--future {future_file} --output {output_file}\"\n)\n\nrunner = ShellModelRunner(\n    train_command=train_command,\n    predict_command=predict_command,\n)\n# Use same MLServiceBuilder setup as above\n</code></pre>"},{"location":"guides/ml-workflows/#architecture","title":"Architecture","text":""},{"location":"guides/ml-workflows/#trainpredict-flow","title":"Train/Predict Flow","text":"<pre><code>1. TRAIN                           2. PREDICT\n   POST /api/v1/ml/$train             POST /api/v1/ml/$predict\n   \u251c\u2500&gt; Submit job                     \u251c\u2500&gt; Load trained model artifact\n   \u251c\u2500&gt; Load config                    \u251c\u2500&gt; Load config\n   \u251c\u2500&gt; Execute runner.on_train()      \u251c\u2500&gt; Execute runner.on_predict()\n   \u2514\u2500&gt; Store model in artifact        \u2514\u2500&gt; Store predictions in artifact\n       (level 0, parent_id=None)           (level 1, parent_id=model_id)\n</code></pre>"},{"location":"guides/ml-workflows/#artifact-hierarchy","title":"Artifact Hierarchy","text":"<pre><code>Config\n  \u2514\u2500&gt; Trained Model (level 0)\n       \u251c\u2500&gt; Predictions 1 (level 1)\n       \u2502    \u2514\u2500&gt; Workspace 1 (level 2)\n       \u251c\u2500&gt; Predictions 2 (level 1)\n       \u2502    \u2514\u2500&gt; Workspace 2 (level 2)\n       \u2514\u2500&gt; Predictions 3 (level 1)\n            \u2514\u2500&gt; Workspace 3 (level 2)\n</code></pre> <p>Benefits: - Complete model lineage tracking - Multiple predictions from same model - Config linked to all model artifacts - Immutable model versioning - Debug workspaces linked to predictions (all runners with workspace enabled)</p>"},{"location":"guides/ml-workflows/#job-scheduling","title":"Job Scheduling","text":"<p>All train/predict operations are asynchronous: - Submit returns immediately with <code>job_id</code> and <code>artifact_id</code> - Monitor progress via Job API or SSE streaming - Results stored in artifacts when complete</p>"},{"location":"guides/ml-workflows/#model-runners","title":"Model Runners","text":""},{"location":"guides/ml-workflows/#basemodelrunner","title":"BaseModelRunner","text":"<p>Abstract base class for custom model runners with lifecycle hooks.</p> <pre><code>from chapkit.ml import BaseModelRunner\nfrom chapkit import BaseConfig\n\nclass MyConfig(BaseConfig):\n    \"\"\"Your config schema.\"\"\"\n    prediction_periods: int = 3\n\nclass MyRunner(BaseModelRunner[MyConfig]):\n    async def on_init(self):\n        \"\"\"Called before train or predict (optional).\"\"\"\n        pass\n\n    async def on_cleanup(self):\n        \"\"\"Called after train or predict (optional).\"\"\"\n        pass\n\n    async def on_train(self, config: MyConfig, data, geo=None):\n        \"\"\"Train and return model (must be pickleable).\"\"\"\n        # config is typed as MyConfig - no casting needed!\n        # Your training logic\n        return trained_model\n\n    async def on_predict(self, config: MyConfig, model, historic, future, geo=None):\n        \"\"\"Make predictions and return DataFrame.\"\"\"\n        # config is typed as MyConfig - autocomplete works!\n        # Your prediction logic\n        return predictions_df\n</code></pre> <p>Key Points: - Model must be pickleable (stored in artifact) - Return value from <code>on_train</code> is passed to <code>on_predict</code> as <code>model</code> parameter - <code>historic</code> parameter is required (must be provided, can be empty DataFrame) - GeoJSON support via <code>geo</code> parameter</p>"},{"location":"guides/ml-workflows/#functionalmodelrunner","title":"FunctionalModelRunner","text":"<p>Wraps train/predict functions for functional-style ML workflows.</p> <pre><code>from chapkit.ml import FunctionalModelRunner\n\nasync def train_fn(config, data, geo=None):\n    # Training logic\n    return model\n\nasync def predict_fn(config, model, historic, future, geo=None):\n    # Prediction logic\n    return predictions\n\nrunner = FunctionalModelRunner(on_train=train_fn, on_predict=predict_fn)\n</code></pre> <p>Workspace Feature: - Training and prediction artifacts include complete workspace ZIPs (same as ShellModelRunner) - Workspace contains: config.yml, data.csv, geo.json (if provided), model.pickle, predictions.csv - Enables debugging by inspecting exact inputs and outputs</p> <p>Use Cases: - Simple models without state - Quick prototypes - Pure function workflows</p>"},{"location":"guides/ml-workflows/#shellmodelrunner","title":"ShellModelRunner","text":"<p>Executes external scripts for language-agnostic ML workflows.</p> <pre><code>from chapkit.ml import ShellModelRunner\n\nrunner = ShellModelRunner(\n    train_command=\"python train.py --data {data_file}\",\n    predict_command=\"python predict.py --future {future_file} --output {output_file}\",\n)\n</code></pre> <p>Variable Substitution: - <code>{data_file}</code> - Training data CSV - <code>{future_file}</code> - Future data CSV - <code>{historic_file}</code> - Historic data CSV - <code>{output_file}</code> - Predictions output CSV - <code>{geo_file}</code> - GeoJSON file (if provided)</p> <p>Execution Environment: - Runner copies entire project directory to isolated temp workspace - Commands execute with workspace as current directory - Scripts can use relative paths and imports - Excludes build artifacts (.venv, node_modules, pycache, .git, etc.) - Config always written to <code>config.yml</code> in workspace - Scripts can directly access <code>config.yml</code> and create/use model files (e.g. <code>model.pickle</code>)</p> <p>Script Requirements: - Training script: Read data from arguments, read config from <code>config.yml</code>, train model, optionally save to <code>model.pickle</code>   - Model file creation is optional - workspace is preserved regardless of exit code   - Training artifacts store the entire workspace (files, logs, intermediate results) - Prediction script: Read data from arguments, read config from <code>config.yml</code>, load model from <code>model.pickle</code>, make predictions, save to <code>{output_file}</code>   - Prediction artifacts store the entire workspace (like training)   - Includes all prediction outputs, logs, and intermediate files - Exit code 0 on success, non-zero on failure - Use stderr for logging - Can use relative imports from project modules</p> <p>Example Training Script (Python): <pre><code>#!/usr/bin/env python3\nimport argparse, pickle\nimport pandas as pd\nimport yaml\nfrom sklearn.linear_model import LinearRegression\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--data\", required=True)\nargs = parser.parse_args()\n\n# Load config (always available as config.yml)\nwith open(\"config.yml\") as f:\n    config = yaml.safe_load(f)\n\n# Load data\ndata = pd.read_csv(args.data)\n\n# Train\nX = data[[\"feature1\", \"feature2\"]]\ny = data[\"target\"]\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Save (use any filename, model.pickle is conventional)\nwith open(\"model.pickle\", \"wb\") as f:\n    pickle.dump(model, f)\n</code></pre></p> <p>Note: Model file creation is optional for ShellModelRunner. The training workspace (including all files, logs, and artifacts created during training) is automatically preserved as a compressed artifact. Training scripts can create model files, multiple files, directories, or rely entirely on generated artifacts in the workspace.</p> <p>Use Cases: - Integration with R, Julia, or other languages - Legacy scripts without modification - Containerized ML pipelines - Team collaboration across languages</p> <p>Project Structure and Relative Imports:</p> <p>The ShellModelRunner copies your entire project directory to an isolated workspace and executes scripts with the workspace as the current directory. This enables:</p> <ol> <li> <p>Relative script paths: Reference scripts using simple relative paths    <pre><code>train_command=\"python scripts/train_model.py --data {data_file}\"\n</code></pre></p> </li> <li> <p>Relative imports: Scripts can import from project modules    <pre><code># In scripts/train_model.py\nfrom lib.preprocessing import clean_data\nfrom lib.models import CustomModel\n</code></pre></p> </li> <li> <p>Project organization: <pre><code>your_project/\n\u251c\u2500\u2500 main.py              # FastAPI app with ShellModelRunner\n\u251c\u2500\u2500 lib/                 # Shared utilities\n\u2502   \u251c\u2500\u2500 preprocessing.py\n\u2502   \u2514\u2500\u2500 models.py\n\u2514\u2500\u2500 scripts/             # ML scripts\n    \u251c\u2500\u2500 train_model.py\n    \u2514\u2500\u2500 predict_model.py\n</code></pre></p> </li> <li> <p>What gets copied: Entire project directory (excluding .venv, node_modules, pycache, .git, build artifacts)</p> </li> <li> <p>What doesn't get copied: Build artifacts, virtual environments, version control files</p> </li> </ol> <p>Note: Run your app from the project root directory (where main.py is located) using <code>fastapi dev main.py</code> so the runner can correctly identify and copy your project files.</p>"},{"location":"guides/ml-workflows/#servicebuilder-setup","title":"ServiceBuilder Setup","text":""},{"location":"guides/ml-workflows/#mlservicebuilder-recommended","title":"MLServiceBuilder (Recommended)","text":"<p>Bundles health, config, artifacts, jobs, and ML in one builder.</p> <pre><code>from chapkit.artifact import ArtifactHierarchy\n\nfrom chapkit.api import MLServiceBuilder, MLServiceInfo, AssessedStatus, ModelMetadata, PeriodType\n\ninfo = MLServiceInfo(\n    id=\"disease-prediction-service\",\n    display_name=\"Disease Prediction Service\",\n    version=\"1.0.0\",\n    description=\"Train and predict disease cases using weather data\",\n    model_metadata=ModelMetadata(\n        author=\"ML Team\",\n        author_assessed_status=AssessedStatus.green,\n        contact_email=\"ml-team@example.com\",\n    ),\n    period_type=PeriodType.monthly,\n    requires_geo=True,  # Model requires GeoJSON spatial data\n)\n\nhierarchy = ArtifactHierarchy(\n    name=\"ml_pipeline\",\n    level_labels={0: \"ml_training_workspace\", 1: \"ml_prediction\"},\n)\n\napp = (\n    MLServiceBuilder(\n        info=info,\n        config_schema=ModelConfig,\n        hierarchy=hierarchy,\n        runner=runner,\n    )\n    .with_monitoring()  # Optional: Prometheus metrics\n    .build()\n)\n</code></pre> <p>MLServiceBuilder automatically includes: - Health check (<code>/health</code>) - Config CRUD (<code>/api/v1/configs</code>) - Artifact CRUD (<code>/api/v1/artifacts</code>) - Job scheduler (<code>/api/v1/jobs</code>) with concurrency control - ML endpoints (<code>/api/v1/ml/$train</code>, <code>/api/v1/ml/$predict</code>)</p>"},{"location":"guides/ml-workflows/#servicebuilder-manual-configuration","title":"ServiceBuilder (Manual Configuration)","text":"<p>For fine-grained control:</p> <pre><code>from chapkit.api import ServiceBuilder, ServiceInfo\n\napp = (\n    ServiceBuilder(info=ServiceInfo(id=\"custom-ml-service\", display_name=\"Custom ML Service\"))\n    .with_health()\n    .with_config(ModelConfig)\n    .with_artifacts(hierarchy=hierarchy)\n    .with_jobs(max_concurrency=3)\n    .with_ml(runner=runner)\n    .build()\n)\n</code></pre> <p>Requirements: - <code>.with_config()</code> must be called before <code>.with_ml()</code> - <code>.with_artifacts()</code> must be called before <code>.with_ml()</code> - <code>.with_jobs()</code> must be called before <code>.with_ml()</code></p>"},{"location":"guides/ml-workflows/#configuration-options","title":"Configuration Options","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=YourConfig,\n    hierarchy=hierarchy,\n    runner=runner,\n    max_concurrency=5,       # Limit concurrent jobs (default: unlimited)\n    database_url=\"ml.db\",    # Persistent storage (default: in-memory)\n)\n</code></pre>"},{"location":"guides/ml-workflows/#mlserviceinfo-fields","title":"MLServiceInfo Fields","text":"Field Type Description <code>id</code> str Service identifier (required) - lowercase letters, numbers, and hyphens only <code>display_name</code> str Service display name (required) <code>version</code> str Service version <code>description</code> str Service description <code>model_metadata</code> ModelMetadata Model documentation (required) <code>period_type</code> PeriodType Period type: weekly or monthly (required) <code>min_prediction_periods</code> int Minimum prediction periods (default: 0) <code>max_prediction_periods</code> int Maximum prediction periods (default: 100) <code>allow_free_additional_continuous_covariates</code> bool Allow extra covariates beyond required <code>required_covariates</code> list[str] Required input covariate names <code>requires_geo</code> bool Whether the model requires GeoJSON spatial data for training/prediction <p>Note: The <code>id</code> field should be a URL-safe slug derived from your service name. Use lowercase letters, numbers, and hyphens only. Example: <code>display_name=\"Disease Prediction Service\"</code> -&gt; <code>id=\"disease-prediction-service\"</code>.</p>"},{"location":"guides/ml-workflows/#modelmetadata-fields","title":"ModelMetadata Fields","text":"Field Type Description <code>author</code> str Model author or team <code>author_note</code> str Additional author notes <code>author_assessed_status</code> AssessedStatus Model maturity (red/orange/yellow/green/white) <code>contact_email</code> EmailStr Contact email address <code>organization</code> str Organization name <code>organization_logo_url</code> HttpUrl URL to organization logo <code>citation_info</code> str How to cite this model <code>repository_url</code> HttpUrl URL to source code repository <code>documentation_url</code> HttpUrl URL to documentation"},{"location":"guides/ml-workflows/#api-reference","title":"API Reference","text":""},{"location":"guides/ml-workflows/#post-apiv1mltrain","title":"POST /api/v1/ml/$train","text":"<p>Train a model asynchronously.</p> <p>Request: <pre><code>{\n  \"config_id\": \"01JCONFIG...\",\n  \"data\": {\n    \"columns\": [\"feature1\", \"feature2\", \"target\"],\n    \"data\": [\n      [1.0, 2.0, 10.0],\n      [2.0, 3.0, 15.0],\n      [3.0, 4.0, 20.0]\n    ]\n  },\n  \"geo\": null\n}\n</code></pre></p> <p>Response (202 Accepted): <pre><code>{\n  \"job_id\": \"01JOB123...\",\n  \"artifact_id\": \"01MODEL456...\",\n  \"message\": \"Training job submitted. Job ID: 01JOB123...\"\n}\n</code></pre></p> <p>cURL Example: <pre><code># Create config first\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"my_config\", \"data\": {}}' | jq -r '.id')\n\n# Submit training job\ncurl -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {\n      \"columns\": [\"rainfall\", \"temperature\", \"cases\"],\n      \"data\": [[10.0, 25.0, 5.0], [15.0, 28.0, 8.0]]\n    }\n  }' | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#post-apiv1mlpredict","title":"POST /api/v1/ml/$predict","text":"<p>Make predictions using a trained model.</p> <p>Request: <pre><code>{\n  \"artifact_id\": \"01MODEL456...\",\n  \"historic\": {\n    \"columns\": [\"feature1\", \"feature2\"],\n    \"data\": []\n  },\n  \"future\": {\n    \"columns\": [\"feature1\", \"feature2\"],\n    \"data\": [\n      [1.5, 2.5],\n      [2.5, 3.5]\n    ]\n  },\n  \"geo\": null\n}\n</code></pre></p> <p>Response (202 Accepted): <pre><code>{\n  \"job_id\": \"01JOB789...\",\n  \"artifact_id\": \"01PRED012...\",\n  \"message\": \"Prediction job submitted. Job ID: 01JOB789...\"\n}\n</code></pre></p> <p>cURL Example: <pre><code># Use model from training\ncurl -X POST http://localhost:8000/api/v1/ml/\\$predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"artifact_id\": \"'$MODEL_ARTIFACT_ID'\",\n    \"historic\": {\n      \"columns\": [\"rainfall\", \"temperature\"],\n      \"data\": []\n    },\n    \"future\": {\n      \"columns\": [\"rainfall\", \"temperature\"],\n      \"data\": [[12.0, 26.0], [18.0, 29.0]]\n    }\n  }' | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#monitor-job-status","title":"Monitor Job Status","text":"<pre><code># Poll job status\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq\n\n# Stream status updates (SSE)\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n\n# Get results from artifact\nARTIFACT_ID=$(curl -s http://localhost:8000/api/v1/jobs/$JOB_ID | jq -r '.artifact_id')\ncurl http://localhost:8000/api/v1/artifacts/$ARTIFACT_ID | jq\n</code></pre>"},{"location":"guides/ml-workflows/#data-formats","title":"Data Formats","text":""},{"location":"guides/ml-workflows/#dataframe-schema","title":"DataFrame Schema","text":"<p>All tabular data uses the <code>DataFrame</code> schema:</p> <pre><code>{\n  \"columns\": [\"col1\", \"col2\", \"col3\"],\n  \"data\": [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0]\n  ],\n  \"index\": null,\n  \"column_types\": null\n}\n</code></pre> <p>Python Usage: <pre><code>from servicekit.data import DataFrame\n\n# Create from DataFrame\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndata_frame = DataFrame.from_pandas(df)\n\n# Convert to DataFrame\ndf = data_frame.to_pandas()\n</code></pre></p>"},{"location":"guides/ml-workflows/#geojson-support","title":"GeoJSON Support","text":"<p>Optional geospatial data via <code>geojson-pydantic</code>:</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [-122.4194, 37.7749]\n      },\n      \"properties\": {\n        \"name\": \"San Francisco\",\n        \"population\": 883305\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"guides/ml-workflows/#artifact-structure","title":"Artifact Structure","text":"<p>Chapkit uses typed artifact data schemas for consistent ML artifact storage with structured metadata.</p>"},{"location":"guides/ml-workflows/#ml-training-artifact","title":"ML Training Artifact","text":"<p>Stored at hierarchy level 0 using <code>MLTrainingWorkspaceArtifactData</code>. Both <code>FunctionalModelRunner</code> and <code>ShellModelRunner</code> store compressed workspace ZIP artifacts:</p> <pre><code>{\n  \"type\": \"ml_training_workspace\",\n  \"metadata\": {\n    \"status\": \"success\",\n    \"exit_code\": 0,\n    \"stdout\": \"Training completed successfully\\\\n\",\n    \"stderr\": \"\",\n    \"config_id\": \"01CONFIG...\",\n    \"started_at\": \"2025-10-14T10:00:00Z\",\n    \"completed_at\": \"2025-10-14T10:00:15Z\",\n    \"duration_seconds\": 15.23\n  },\n  \"content\": \"&lt;Zip file bytes&gt;\",\n  \"content_type\": \"application/zip\",\n  \"content_size\": 5242880\n}\n</code></pre> <p>Schema Structure: - <code>type</code>: Discriminator field - always <code>\"ml_training_workspace\"</code> - <code>metadata</code>: Structured execution metadata   - <code>status</code>: \"success\" or \"failed\" (based on exit code)   - <code>exit_code</code>: Training script exit code (0 = success)   - <code>stdout</code>: Standard output from training script   - <code>stderr</code>: Standard error from training script   - <code>config_id</code>: Config used for training   - <code>started_at</code>, <code>completed_at</code>: ISO 8601 timestamps   - <code>duration_seconds</code>: Training duration - <code>content</code>: Compressed workspace (all files, logs, artifacts created during training) - <code>content_type</code>: \"application/zip\" - <code>content_size</code>: Zip file size in bytes</p> <p>Workspace Contents: - All files created by training script (model files, logs, checkpoints, etc.) - Data files (config.yml, data.csv, geo.json if provided) - Any intermediate artifacts or debug output - Complete project directory structure preserved</p>"},{"location":"guides/ml-workflows/#ml-prediction-artifact","title":"ML Prediction Artifact","text":"<p>Stored at hierarchy level 1 using <code>MLPredictionArtifactData</code> (linked to training artifact). All runners store the prediction DataFrame the same way:</p>"},{"location":"guides/ml-workflows/#prediction-artifact-level-1-all-runners","title":"Prediction Artifact (level 1, all runners)","text":"<p>Stores prediction DataFrame directly:</p> <pre><code>{\n  \"type\": \"ml_prediction\",\n  \"metadata\": {\n    \"status\": \"success\",\n    \"config_id\": \"01CONFIG...\",\n    \"started_at\": \"2025-10-14T10:05:00Z\",\n    \"completed_at\": \"2025-10-14T10:05:02Z\",\n    \"duration_seconds\": 2.15\n  },\n  \"content\": {\n    \"columns\": [\"feature1\", \"feature2\", \"sample_0\"],\n    \"data\": [[1.5, 2.5, 12.3], [2.5, 3.5, 17.8]]\n  },\n  \"content_type\": \"application/vnd.chapkit.dataframe+json\",\n  \"content_size\": null\n}\n</code></pre> <p>Schema Structure: - <code>type</code>: Discriminator field - always <code>\"ml_prediction\"</code> - <code>metadata</code>: Structured execution metadata (same as training) - <code>content</code>: Prediction DataFrame with results - <code>content_type</code>: \"application/vnd.chapkit.dataframe+json\"</p>"},{"location":"guides/ml-workflows/#workspace-artifact-level-2-when-workspace-enabled","title":"Workspace Artifact (level 2, when workspace enabled)","text":"<p>Both FunctionalModelRunner (default) and ShellModelRunner create an additional workspace artifact (level 2) as a child of the prediction artifact for debugging:</p> <pre><code>{\n  \"type\": \"ml_prediction_workspace\",\n  \"metadata\": {\n    \"status\": \"success\",\n    \"exit_code\": 0,\n    \"stdout\": \"Prediction completed successfully\\\\n\",\n    \"stderr\": \"\",\n    \"config_id\": \"01CONFIG...\",\n    \"started_at\": \"2025-10-14T10:05:00Z\",\n    \"completed_at\": \"2025-10-14T10:05:02Z\",\n    \"duration_seconds\": 2.15\n  },\n  \"content\": \"&lt;Zip file bytes&gt;\",\n  \"content_type\": \"application/zip\",\n  \"content_size\": 1048576\n}\n</code></pre> <p>Workspace Artifact Schema: - <code>type</code>: Discriminator field - always <code>\"ml_prediction_workspace\"</code> - <code>metadata</code>: Structured execution metadata   - <code>status</code>: \"success\" or \"failed\" (based on exit code for ShellModelRunner)   - <code>exit_code</code>: Script exit code (ShellModelRunner only, null for FunctionalModelRunner)   - <code>stdout</code>: Standard output (ShellModelRunner only, null for FunctionalModelRunner)   - <code>stderr</code>: Standard error (ShellModelRunner only, null for FunctionalModelRunner)   - <code>config_id</code>: Config used for prediction   - <code>started_at</code>, <code>completed_at</code>: ISO 8601 timestamps   - <code>duration_seconds</code>: Prediction duration - <code>content</code>: Compressed workspace (all files, logs, artifacts created during prediction) - <code>content_type</code>: \"application/zip\" - <code>content_size</code>: Zip file size in bytes</p> <p>Workspace Contents: - predictions.csv (output predictions) - config.yml (configuration used) - historic.csv, future.csv (input data) - geo.json (if provided) - model.pickle (trained model, FunctionalModelRunner) - Additional files created during execution (logs, intermediate results)</p> <p>Accessing workspace: Use <code>GET /api/v1/artifacts/{prediction_id}/$tree</code> to find the workspace artifact ID, then retrieve it directly.</p>"},{"location":"guides/ml-workflows/#workspace-file-structure","title":"Workspace File Structure","text":"<p>Training workspace ZIP contains: <pre><code>workspace/\n  config.yml          # Model configuration (YAML serialized)\n  data.csv            # Training data with all columns\n  geo.json            # GeoJSON features (if requires_geo=True)\n  model.pickle        # Trained model (FunctionalModelRunner only)\n  [script outputs]    # Any files created by training script\n</code></pre></p> <p>Prediction workspace ZIP contains: <pre><code>workspace/\n  config.yml          # Model configuration (YAML serialized)\n  historic.csv        # Historical observations\n  future.csv          # Future periods to predict\n  geo.json            # GeoJSON features (if requires_geo=True)\n  predictions.csv     # Model predictions\n  model.pickle        # Trained model (FunctionalModelRunner only)\n  [script outputs]    # Any files created by prediction script\n</code></pre></p>"},{"location":"guides/ml-workflows/#extracting-and-inspecting-workspaces","title":"Extracting and Inspecting Workspaces","text":"<p>Download and extract workspace for debugging:</p> <pre><code>import zipfile\nfrom io import BytesIO\n\n# Get workspace artifact\nartifact = await artifact_manager.find_by_id(workspace_artifact_id)\nworkspace_zip = artifact.data[\"content\"]\n\n# Extract to directory\nwith zipfile.ZipFile(BytesIO(workspace_zip), \"r\") as zf:\n    zf.extractall(\"/tmp/workspace_debug\")\n\n# Or list contents\nwith zipfile.ZipFile(BytesIO(workspace_zip), \"r\") as zf:\n    for name in zf.namelist():\n        print(name)\n</code></pre> <p>Using curl: <pre><code># Download workspace artifact content\ncurl -o workspace.zip \\\n  http://localhost:8000/api/v1/artifacts/$WORKSPACE_ID/\\$download\n\n# Extract and inspect\nunzip workspace.zip -d workspace_debug/\ncat workspace_debug/config.yml\nhead workspace_debug/predictions.csv\n</code></pre></p>"},{"location":"guides/ml-workflows/#storage-considerations","title":"Storage Considerations","text":"<p>Both <code>FunctionalModelRunner</code> and <code>ShellModelRunner</code> always create workspace ZIP artifacts containing all inputs and outputs. This provides full debugging capability and audit trails.</p> <p>Typical artifact sizes: - Training workspace: 1-100+ MB (depends on project size) - Prediction workspace: 0.5-50+ MB - Prediction DataFrame: Small (KB range)</p> <p>Managing storage: - Use artifact retention policies to auto-delete old workspaces - Level 2 workspace artifacts are primarily for debugging - short retention OK - See Artifact Retention Strategies for cleanup examples</p>"},{"location":"guides/ml-workflows/#accessing-artifact-data","title":"Accessing Artifact Data","text":"<pre><code># Get training artifact\nartifact = await artifact_manager.find_by_id(model_artifact_id)\n\n# Access typed data\nassert artifact.data[\"type\"] == \"ml_training_workspace\"\nmetadata = artifact.data[\"metadata\"]\ntrained_model = artifact.data[\"content\"]\n\n# Metadata fields\nconfig_id = metadata[\"config_id\"]\nduration = metadata[\"duration_seconds\"]\nstatus = metadata[\"status\"]\n\n# Get prediction artifact\npred_artifact = await artifact_manager.find_by_id(prediction_artifact_id)\npredictions_df = pred_artifact.data[\"content\"]  # DataFrame dict\n</code></pre>"},{"location":"guides/ml-workflows/#download-endpoints","title":"Download Endpoints","text":"<p>Download artifact content as files:</p> <pre><code># Download predictions as JSON\ncurl -O -J http://localhost:8000/api/v1/artifacts/$PRED_ARTIFACT_ID/\\$download\n\n# Get metadata only (no binary content)\ncurl http://localhost:8000/api/v1/artifacts/$MODEL_ARTIFACT_ID/\\$metadata | jq\n</code></pre> <p>See Artifact Storage Guide for more details.</p>"},{"location":"guides/ml-workflows/#complete-workflow-examples","title":"Complete Workflow Examples","text":""},{"location":"guides/ml-workflows/#basic-functional-workflow","title":"Basic Functional Workflow","text":"<pre><code># 1. Start service\nfastapi dev examples/ml_basic.py\n\n# 2. Create config\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"weather_model\", \"data\": {}}' | jq -r '.id')\n\n# 3. Train model\nTRAIN_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"config_id\": \"'$CONFIG_ID'\",\n    \"data\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\", \"disease_cases\"],\n      \"data\": [\n        [10.0, 25.0, 5.0],\n        [15.0, 28.0, 8.0],\n        [8.0, 22.0, 3.0],\n        [20.0, 30.0, 12.0],\n        [12.0, 26.0, 6.0]\n      ]\n    }\n  }')\n\nJOB_ID=$(echo $TRAIN_RESPONSE | jq -r '.job_id')\nMODEL_ARTIFACT_ID=$(echo $TRAIN_RESPONSE | jq -r '.artifact_id')\n\necho \"Training Job ID: $JOB_ID\"\necho \"Model Artifact ID: $MODEL_ARTIFACT_ID\"\n\n# 4. Wait for training completion\ncurl -N http://localhost:8000/api/v1/jobs/$JOB_ID/\\$stream\n\n# 5. View trained model\ncurl http://localhost:8000/api/v1/artifacts/$MODEL_ARTIFACT_ID | jq\n\n# 6. Make predictions\nPREDICT_RESPONSE=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"artifact_id\": \"'$MODEL_ARTIFACT_ID'\",\n    \"historic\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\"],\n      \"data\": []\n    },\n    \"future\": {\n      \"columns\": [\"rainfall\", \"mean_temperature\"],\n      \"data\": [\n        [11.0, 26.0],\n        [14.0, 27.0],\n        [9.0, 24.0]\n      ]\n    }\n  }')\n\nPRED_JOB_ID=$(echo $PREDICT_RESPONSE | jq -r '.job_id')\nPRED_ARTIFACT_ID=$(echo $PREDICT_RESPONSE | jq -r '.artifact_id')\n\n# 7. Wait for predictions\ncurl -N http://localhost:8000/api/v1/jobs/$PRED_JOB_ID/\\$stream\n\n# 8. View predictions\ncurl http://localhost:8000/api/v1/artifacts/$PRED_ARTIFACT_ID | jq '.data.content'\n</code></pre>"},{"location":"guides/ml-workflows/#class-based-with-preprocessing","title":"Class-Based with Preprocessing","text":"<pre><code># examples/ml_class.py demonstrates:\n# - StandardScaler for feature normalization\n# - State management (scaler shared between train/predict)\n# - Lifecycle hooks (on_init, on_cleanup)\n# - Model artifact containing multiple objects\n\nfrom chapkit.ml import BaseModelRunner\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\nclass WeatherModelRunner(BaseModelRunner[WeatherConfig]):\n    def __init__(self):\n        self.feature_names = [\"rainfall\", \"mean_temperature\", \"humidity\"]\n        self.scaler = None\n\n    async def on_train(self, config: WeatherConfig, data, geo=None):\n        X = data[self.feature_names].fillna(0)\n        y = data[\"disease_cases\"].fillna(0)\n\n        # Normalize features\n        self.scaler = StandardScaler()\n        X_scaled = self.scaler.fit_transform(X)\n\n        # Train model\n        model = LinearRegression()\n        model.fit(X_scaled, y)\n\n        # Return dict with model and preprocessing artifacts\n        return {\n            \"model\": model,\n            \"scaler\": self.scaler,\n            \"feature_names\": self.feature_names,\n        }\n\n    async def on_predict(self, config: WeatherConfig, model, historic, future, geo=None):\n        # Extract artifacts\n        trained_model = model[\"model\"]\n        scaler = model[\"scaler\"]\n        feature_names = model[\"feature_names\"]\n\n        # Apply same preprocessing\n        X = future[feature_names].fillna(0)\n        X_scaled = scaler.transform(X)\n\n        # Predict\n        future[\"sample_0\"] = trained_model.predict(X_scaled)\n        return future\n</code></pre> <p>Benefits: - Consistent preprocessing between train/predict - Model artifacts include all necessary objects - Type safety and validation - Easy testing and debugging</p>"},{"location":"guides/ml-workflows/#shell-based-language-agnostic","title":"Shell-Based Language-Agnostic","text":"<pre><code># examples/ml_shell.py demonstrates:\n# - External R/Julia/Python scripts\n# - File-based data interchange\n# - No code modification required\n# - Container-friendly workflows\n\nfrom chapkit.ml import ShellModelRunner\n\n# Python example - just use \"python\"\nrunner = ShellModelRunner(\n    train_command=\"python scripts/train_model.py --data {data_file}\",\n    predict_command=\"python scripts/predict_model.py --future {future_file} --output {output_file}\",\n)\n\n# Or use any other language - Rscript, julia, etc.\n# runner = ShellModelRunner(\n#     train_command=\"Rscript scripts/train.R --data {data_file}\",\n#     predict_command=\"Rscript scripts/predict.R --future {future_file} --output {output_file}\",\n# )\n</code></pre> <p>External Script Example (R): <pre><code>#!/usr/bin/env Rscript\nlibrary(yaml)\n\nargs &lt;- commandArgs(trailingOnly = TRUE)\ndata_file &lt;- args[which(args == \"--data\") + 1]\n\n# Load config (always available as config.yml)\nconfig &lt;- yaml.load_file(\"config.yml\")\ndata &lt;- read.csv(data_file)\n\n# Train model\nmodel &lt;- lm(disease_cases ~ rainfall + mean_temperature, data = data)\n\n# Save model (use any filename, model.rds is conventional)\nsaveRDS(model, \"model.rds\")\ncat(\"SUCCESS: Model trained\\n\")\n</code></pre></p>"},{"location":"guides/ml-workflows/#testing","title":"Testing","text":""},{"location":"guides/ml-workflows/#manual-testing","title":"Manual Testing","text":"<p>Terminal 1: <pre><code>fastapi dev examples/ml_basic.py\n</code></pre></p> <p>Terminal 2: <pre><code># Complete workflow test\nCONFIG_ID=$(curl -s -X POST http://localhost:8000/api/v1/configs \\\n  -d '{\"name\":\"test\",\"data\":{}}' | jq -r '.id')\n\nTRAIN=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$train -d '{\n  \"config_id\":\"'$CONFIG_ID'\",\n  \"data\":{\"columns\":[\"a\",\"b\",\"y\"],\"data\":[[1,2,10],[2,3,15],[3,4,20]]}\n}')\n\nMODEL_ID=$(echo $TRAIN | jq -r '.artifact_id')\nJOB_ID=$(echo $TRAIN | jq -r '.job_id')\n\n# Wait for completion\nsleep 2\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.status'\n\n# Predict\nPRED=$(curl -s -X POST http://localhost:8000/api/v1/ml/\\$predict -d '{\n  \"artifact_id\":\"'$MODEL_ID'\",\n  \"historic\":{\"columns\":[\"a\",\"b\"],\"data\":[]},\n  \"future\":{\"columns\":[\"a\",\"b\"],\"data\":[[1.5,2.5],[2.5,3.5]]}\n}')\n\nPRED_ID=$(echo $PRED | jq -r '.artifact_id')\nsleep 2\n\n# View results\ncurl http://localhost:8000/api/v1/artifacts/$PRED_ID | jq '.data.content'\n</code></pre></p>"},{"location":"guides/ml-workflows/#automated-testing","title":"Automated Testing","text":"<pre><code>import time\nfrom fastapi.testclient import TestClient\n\ndef wait_for_job(client: TestClient, job_id: str, timeout: float = 5.0):\n    \"\"\"Poll until job completes.\"\"\"\n    start = time.time()\n    while time.time() - start &lt; timeout:\n        job = client.get(f\"/api/v1/jobs/{job_id}\").json()\n        if job[\"status\"] in [\"completed\", \"failed\", \"canceled\"]:\n            return job\n        time.sleep(0.1)\n    raise TimeoutError(f\"Job {job_id} timeout\")\n\n\ndef test_train_predict_workflow(client: TestClient):\n    \"\"\"Test complete ML workflow.\"\"\"\n    # Create config\n    config_resp = client.post(\"/api/v1/configs\", json={\n        \"name\": \"test_config\",\n        \"data\": {}\n    })\n    config_id = config_resp.json()[\"id\"]\n\n    # Train\n    train_resp = client.post(\"/api/v1/ml/$train\", json={\n        \"config_id\": config_id,\n        \"data\": {\n            \"columns\": [\"x1\", \"x2\", \"y\"],\n            \"data\": [[1, 2, 10], [2, 3, 15], [3, 4, 20]]\n        }\n    })\n    assert train_resp.status_code == 202\n\n    train_data = train_resp.json()\n    job_id = train_data[\"job_id\"]\n    model_id = train_data[\"artifact_id\"]\n\n    # Wait for training\n    job = wait_for_job(client, job_id)\n    assert job[\"status\"] == \"completed\"\n\n    # Verify model artifact\n    model_artifact = client.get(f\"/api/v1/artifacts/{model_id}\").json()\n    assert model_artifact[\"data\"][\"type\"] == \"ml_training_workspace\"\n    assert model_artifact[\"level\"] == 0\n\n    # Predict\n    pred_resp = client.post(\"/api/v1/ml/$predict\", json={\n        \"artifact_id\": model_id,\n        \"historic\": {\n            \"columns\": [\"x1\", \"x2\"],\n            \"data\": []\n        },\n        \"future\": {\n            \"columns\": [\"x1\", \"x2\"],\n            \"data\": [[1.5, 2.5], [2.5, 3.5]]\n        }\n    })\n    assert pred_resp.status_code == 202\n\n    pred_data = pred_resp.json()\n    pred_job_id = pred_data[\"job_id\"]\n    pred_id = pred_data[\"artifact_id\"]\n\n    # Wait for prediction\n    pred_job = wait_for_job(client, pred_job_id)\n    assert pred_job[\"status\"] == \"completed\"\n\n    # Verify predictions\n    pred_artifact = client.get(f\"/api/v1/artifacts/{pred_id}\").json()\n    assert pred_artifact[\"data\"][\"type\"] == \"ml_prediction\"\n    assert pred_artifact[\"parent_id\"] == model_id\n    assert pred_artifact[\"level\"] == 1\n    assert \"sample_0\" in pred_artifact[\"data\"][\"content\"][\"columns\"]\n</code></pre>"},{"location":"guides/ml-workflows/#browser-testing-swagger-ui","title":"Browser Testing (Swagger UI)","text":"<ol> <li>Open http://localhost:8000/docs</li> <li>Create config via POST <code>/api/v1/configs</code></li> <li>Train via POST <code>/api/v1/ml/$train</code></li> <li>Monitor job via GET <code>/api/v1/jobs/{job_id}</code></li> <li>Predict via POST <code>/api/v1/ml/$predict</code></li> <li>View artifacts via GET <code>/api/v1/artifacts/{artifact_id}</code></li> </ol>"},{"location":"guides/ml-workflows/#production-deployment","title":"Production Deployment","text":""},{"location":"guides/ml-workflows/#concurrency-control","title":"Concurrency Control","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=config_schema,\n    hierarchy=hierarchy,\n    runner=runner,\n    max_concurrency=3,  # Limit concurrent training jobs\n)\n</code></pre> <p>Recommendations: - CPU-intensive models: Set to CPU core count (4-8) - GPU models: Set to GPU count (1-4) - Memory-intensive: Lower limits (2-3) - I/O-bound: Higher limits OK (10-20)</p>"},{"location":"guides/ml-workflows/#database-configuration","title":"Database Configuration","text":"<pre><code>MLServiceBuilder(\n    info=info,\n    config_schema=config_schema,\n    hierarchy=hierarchy,\n    runner=runner,\n    database_url=\"/data/ml.db\",  # Persistent storage\n)\n</code></pre> <p>Best Practices: - Mount persistent volume for <code>/data</code> - Regular backups (models + artifacts) - Monitor database size growth - Implement artifact retention policies</p>"},{"location":"guides/ml-workflows/#model-versioning","title":"Model Versioning","text":"<pre><code># Use config name for version tracking\nconfig = {\n    \"name\": \"weather_model_v1.2.3\",\n    \"data\": {\n        \"version\": \"1.2.3\",\n        \"features\": [\"rainfall\", \"temperature\"],\n        \"hyperparameters\": {\"alpha\": 0.01}\n    }\n}\n</code></pre> <p>Artifact Hierarchy for Versions: <pre><code>weather_model_v1.0.0 (config)\n  \u2514\u2500&gt; trained_model_1 (artifact level 0)\n       \u2514\u2500&gt; predictions_* (artifact level 1)\n\nweather_model_v1.1.0 (config)\n  \u2514\u2500&gt; trained_model_2 (artifact level 0)\n       \u2514\u2500&gt; predictions_* (artifact level 1)\n</code></pre></p>"},{"location":"guides/ml-workflows/#monitoring","title":"Monitoring","text":"<pre><code>app = (\n    MLServiceBuilder(info=info, config_schema=config, hierarchy=hierarchy, runner=runner)\n    .with_monitoring()  # Prometheus metrics at /metrics\n    .build()\n)\n</code></pre> <p>Available Metrics: - <code>ml_train_jobs_total</code> - Total training jobs submitted - <code>ml_predict_jobs_total</code> - Total prediction jobs submitted - Job scheduler metrics (see Job Scheduler guide)</p> <p>Custom Metrics: <pre><code>from prometheus_client import Histogram\n\nmodel_training_duration = Histogram(\n    'model_training_duration_seconds',\n    'Model training duration'\n)\n\n# Training durations already tracked in artifact metadata\n# Query via artifact API\n</code></pre></p>"},{"location":"guides/ml-workflows/#artifact-retention-strategies","title":"Artifact Retention Strategies","text":"<p>ML artifacts, especially workspace ZIPs from <code>ShellModelRunner</code> and <code>FunctionalModelRunner</code> (when workspace is enabled), can consume significant storage. Implement retention policies to manage disk space while preserving important artifacts.</p> <p>Artifact Size Considerations: - Training workspace ZIPs: 1-100+ MB (depends on project size) - Prediction workspace ZIPs: 0.5-50+ MB - Prediction DataFrames: Typically small (KB range) - Models (pickled): Varies widely (KB to GB)</p> <p>Retention Strategies:</p> <ol> <li>Time-based cleanup: Delete artifacts older than N days</li> </ol> <pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI\n\nasync def cleanup_old_artifacts(app: FastAPI, days: int = 30) -&gt; None:\n    \"\"\"Delete artifacts older than specified days.\"\"\"\n    from chapkit.artifact import ArtifactRepository, ArtifactManager\n\n    cutoff = datetime.now() - timedelta(days=days)\n\n    async with app.state.database.session() as session:\n        artifact_repository = ArtifactRepository(session)\n        artifact_manager = ArtifactManager(artifact_repository)\n\n        # Find old artifacts (implement find_older_than in repository)\n        old_artifacts = await artifact_manager.find_all()\n\n        deleted_count = 0\n        for artifact in old_artifacts:\n            if artifact.created_at &lt; cutoff:\n                await artifact_manager.delete(artifact.id)\n                deleted_count += 1\n\n        await artifact_repository.commit()\n\n    print(f\"Deleted {deleted_count} artifacts older than {days} days\")\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Run cleanup on startup (optional)\n    await cleanup_old_artifacts(app, days=30)\n    yield\n</code></pre> <ol> <li>Level-based retention: Keep training artifacts longer than prediction workspaces</li> </ol> <pre><code>async def cleanup_by_level(app: FastAPI) -&gt; None:\n    \"\"\"Different retention periods by artifact level.\"\"\"\n    retention_days = {\n        0: 365,   # Training artifacts: keep 1 year\n        1: 90,    # Prediction results: keep 90 days\n        2: 7,     # Workspace ZIPs: keep 7 days (debug only)\n    }\n\n    async with app.state.database.session() as session:\n        artifact_repository = ArtifactRepository(session)\n        artifact_manager = ArtifactManager(artifact_repository)\n\n        for artifact in await artifact_manager.find_all():\n            days = retention_days.get(artifact.level, 30)\n            cutoff = datetime.now() - timedelta(days=days)\n            if artifact.created_at &lt; cutoff:\n                await artifact_manager.delete(artifact.id)\n\n        await artifact_repository.commit()\n</code></pre> <ol> <li>Type-based retention: Keep predictions, delete workspaces</li> </ol> <pre><code>async def cleanup_workspace_artifacts(app: FastAPI, days: int = 7) -&gt; None:\n    \"\"\"Delete workspace artifacts older than N days, keep predictions.\"\"\"\n    cutoff = datetime.now() - timedelta(days=days)\n\n    async with app.state.database.session() as session:\n        artifact_repository = ArtifactRepository(session)\n        artifact_manager = ArtifactManager(artifact_repository)\n\n        for artifact in await artifact_manager.find_all():\n            artifact_type = artifact.data.get(\"type\", \"\")\n            # Only delete workspace artifacts, not predictions\n            if artifact_type in (\"ml_training_workspace\", \"ml_prediction_workspace\"):\n                if artifact.data.get(\"content_type\") == \"application/zip\":\n                    if artifact.created_at &lt; cutoff:\n                        await artifact_manager.delete(artifact.id)\n\n        await artifact_repository.commit()\n</code></pre> <p>Scheduled cleanup with APScheduler:</p> <pre><code>from apscheduler.schedulers.asyncio import AsyncIOScheduler\nfrom apscheduler.triggers.cron import CronTrigger\n\nscheduler = AsyncIOScheduler()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Schedule daily cleanup at 3 AM\n    scheduler.add_job(\n        cleanup_old_artifacts,\n        CronTrigger(hour=3, minute=0),\n        args=[app, 30],\n        id=\"artifact_cleanup\",\n    )\n    scheduler.start()\n    yield\n    scheduler.shutdown()\n</code></pre> <p>Best Practices: - Keep training artifacts (level 0) longer than prediction workspaces (level 2) - Prediction DataFrames (level 1) are small - retain longer for audit trails - Workspace ZIPs (level 2) are primarily for debugging - short retention OK - Monitor database size with alerts (e.g., when exceeding 80% capacity) - Consider external storage (S3, GCS) for large models instead of SQLite - Back up critical artifacts before cleanup runs</p>"},{"location":"guides/ml-workflows/#docker-deployment","title":"Docker Deployment","text":"<p>Dockerfile: <pre><code>FROM python:3.13-slim\n\nWORKDIR /app\nCOPY . /app\n\nRUN pip install --no-cache-dir -e .\n\n# Create non-root user\nRUN useradd -m -u 1000 mluser &amp;&amp; chown -R mluser:mluser /app\nUSER mluser\n\nEXPOSE 8000\n\nCMD [\"fastapi\", \"run\", \"ml_service.py\", \"--host\", \"0.0.0.0\"]\n</code></pre></p> <p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  ml-service:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ml-data:/data\n    environment:\n      - DATABASE_URL=/data/ml.db\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 8G\n\nvolumes:\n  ml-data:\n</code></pre></p>"},{"location":"guides/ml-workflows/#gpu-support","title":"GPU Support","text":"<pre><code>FROM nvidia/cuda:12.0-runtime-ubuntu22.04\nFROM python:3.13\n\n# Install ML libraries with GPU support\nRUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cu120\n\n# Your ML code\nCOPY . /app\n</code></pre> <p>docker-compose.yml: <pre><code>services:\n  ml-service:\n    build: .\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre></p>"},{"location":"guides/ml-workflows/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ml-workflows/#config-not-found-error","title":"\"Config not found\" Error","text":"<p>Problem: Training fails with \"Config {id} not found\"</p> <p>Cause: Invalid or deleted config ID</p> <p>Solution: <pre><code># List configs\ncurl http://localhost:8000/api/v1/configs | jq\n\n# Verify config exists\ncurl http://localhost:8000/api/v1/configs/$CONFIG_ID\n</code></pre></p>"},{"location":"guides/ml-workflows/#model-artifact-not-found-error","title":"\"Model artifact not found\" Error","text":"<p>Problem: Prediction fails with \"Model artifact {id} not found\"</p> <p>Cause: Invalid model artifact ID or training failed</p> <p>Solution: <pre><code># Check training job status\ncurl http://localhost:8000/api/v1/jobs/$TRAIN_JOB_ID | jq\n\n# If training failed, check error\ncurl http://localhost:8000/api/v1/jobs/$TRAIN_JOB_ID | jq '.error'\n\n# List artifacts\ncurl http://localhost:8000/api/v1/artifacts | jq\n</code></pre></p>"},{"location":"guides/ml-workflows/#training-job-fails-immediately","title":"Training Job Fails Immediately","text":"<p>Problem: Job status shows \"failed\" right after submission</p> <p>Causes: 1. Model not pickleable 2. Missing required columns in data 3. Insufficient training data 4. Config validation errors</p> <p>Solution: <pre><code># Check job error message\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.error, .error_traceback'\n\n# Common fixes:\n# - Ensure model is pickleable (no lambda functions, local classes)\n# - Verify DataFrame columns match feature expectations\n# - Check config schema validation\n</code></pre></p>"},{"location":"guides/ml-workflows/#prediction-returns-wrong-shape","title":"Prediction Returns Wrong Shape","text":"<p>Problem: Predictions DataFrame has incorrect columns</p> <p>Cause: <code>on_predict</code> must add prediction columns to input DataFrame</p> <p>Solution: <pre><code>async def on_predict(self, config, model, historic, future, geo=None):\n    X = future[[\"feature1\", \"feature2\"]]\n    predictions = model.predict(X)\n\n    # IMPORTANT: Add predictions to future DataFrame\n    future[\"sample_0\"] = predictions  # Required column name\n\n    return future  # Return modified DataFrame\n</code></pre></p>"},{"location":"guides/ml-workflows/#shell-runner-script-fails","title":"Shell Runner Script Fails","text":"<p>Problem: ShellModelRunner returns \"script failed with exit code 1\"</p> <p>Causes: 1. Script not executable 2. Wrong interpreter 3. Missing dependencies 4. File path issues</p> <p>Solution: <pre><code># Make script executable\nchmod +x scripts/train_model.py\n\n# Test script manually\npython scripts/train_model.py \\\n  --config /tmp/test_config.json \\\n  --data /tmp/test_data.csv \\\n  --model /tmp/test_model.pkl\n\n# Check script stderr output\ncurl http://localhost:8000/api/v1/jobs/$JOB_ID | jq '.error'\n</code></pre></p>"},{"location":"guides/ml-workflows/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Service consuming excessive memory</p> <p>Causes: 1. Large models in memory 2. Too many concurrent jobs 3. Artifact accumulation</p> <p>Solution: <pre><code># Limit concurrent jobs\nMLServiceBuilder(..., max_concurrency=2)\n\n# Implement artifact cleanup\nasync def cleanup_old_artifacts(app):\n    # Delete artifacts older than 30 days\n    cutoff = datetime.now() - timedelta(days=30)\n    # Implementation depends on your needs\n\napp.on_startup(cleanup_old_artifacts)\n</code></pre></p>"},{"location":"guides/ml-workflows/#model-size-too-large","title":"Model Size Too Large","text":"<p>Problem: \"Model size exceeds limit\" or slow artifact storage</p> <p>Cause: Large models (&gt;100MB) stored in SQLite</p> <p>Solution: <pre><code># Option 1: External model storage\nasync def on_train(self, config, data, geo=None):\n    model = train_large_model(data)\n\n    # Save to external storage (S3, etc.)\n    model_url = save_to_s3(model)\n\n    # Return metadata instead of model\n    return {\n        \"model_url\": model_url,\n        \"model_metadata\": {...}\n    }\n\n# Option 2: Use PostgreSQL instead of SQLite\nMLServiceBuilder(..., database_url=\"postgresql://...\")\n</code></pre></p>"},{"location":"guides/ml-workflows/#dataframe-validation-errors","title":"DataFrame Validation Errors","text":"<p>Problem: \"Invalid DataFrame schema\" during train/predict</p> <p>Cause: Incorrect data format in request</p> <p>Solution: <pre><code>// Correct format\n{\n  \"columns\": [\"col1\", \"col2\"],\n  \"data\": [\n    [1.0, 2.0],\n    [3.0, 4.0]\n  ]\n}\n\n// Wrong formats:\n// {\"col1\": [1, 3], \"col2\": [2, 4]}  (dict format - not supported)\n// [{\"col1\": 1, \"col2\": 2}]  (records format - not supported)\n</code></pre></p>"},{"location":"guides/ml-workflows/#prediction-blocked-by-failed-training","title":"Prediction Blocked by Failed Training","text":"<p>Problem: \"Cannot predict using failed training artifact\"</p> <p>Cause: Training script exited with non-zero code, artifact has status=\"failed\"</p> <p>Solution: <pre><code># Check training artifact status\ncurl http://localhost:8000/api/v1/artifacts/$TRAINING_ARTIFACT_ID | jq '.data.metadata'\n\n# If status is \"failed\", check stdout/stderr\ncurl http://localhost:8000/api/v1/artifacts/$TRAINING_ARTIFACT_ID | \\\n  jq '.data.metadata | {status, exit_code, stdout, stderr}'\n\n# Re-train with fixed script\ncurl -X POST http://localhost:8000/api/v1/ml/\\$train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"config_id\": \"...\", \"data\": {...}}'\n</code></pre></p>"},{"location":"guides/ml-workflows/#pickle-compatibility-errors","title":"Pickle Compatibility Errors","text":"<p>Problem: \"corrupted or incompatible pickle file\" during prediction</p> <p>Causes: 1. Model trained with different Python version 2. Model class definition changed 3. Required package version mismatch</p> <p>Solution: <pre><code># Check Python version compatibility\n# Models pickled in Python 3.10 may not load in Python 3.12\n\n# Ensure same environment for train and predict\n# - Same Python version\n# - Same scikit-learn/numpy versions\n# - Same class definitions\n\n# If model class changed, re-train:\ncurl -X POST http://localhost:8000/api/v1/ml/\\$train ...\n</code></pre></p>"},{"location":"guides/ml-workflows/#workspace-extraction-fails","title":"Workspace Extraction Fails","text":"<p>Problem: Cannot unzip or read workspace artifact</p> <p>Causes: 1. Artifact content is not a ZIP (pickled model instead) 2. Corrupted workspace during creation</p> <p>Solution: <pre><code># Check artifact content_type\ncurl http://localhost:8000/api/v1/artifacts/$ARTIFACT_ID | jq '.data.content_type'\n\n# If \"application/x-pickle\" - not a workspace, use pickle.loads()\n# If \"application/zip\" - workspace, use zipfile\n\n# Python inspection:\nimport pickle\nimport zipfile\nfrom io import BytesIO\n\nartifact = await artifact_manager.find_by_id(artifact_id)\ncontent_type = artifact.data[\"content_type\"]\n\nif content_type == \"application/zip\":\n    with zipfile.ZipFile(BytesIO(artifact.data[\"content\"]), \"r\") as zf:\n        print(zf.namelist())\nelse:\n    model = pickle.loads(artifact.data[\"content\"])\n</code></pre></p>"},{"location":"guides/ml-workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Job Monitoring: See Job Scheduler guide for SSE streaming</li> <li>Task Execution: Combine with Tasks for preprocessing pipelines</li> <li>Authentication: Secure ML endpoints with API keys</li> <li>Monitoring: Track model performance with Prometheus metrics</li> </ul> <p>For more examples: - <code>examples/ml_basic.py</code> - Functional runner with LinearRegression - <code>examples/ml_class.py</code> - Class-based runner with preprocessing - <code>examples/ml_shell.py</code> - Shell-based runner with external scripts - <code>tests/test_example_ml_basic.py</code> - Complete test suite</p>"},{"location":"guides/task-execution/","title":"Task Execution","text":"<p>Chapkit provides a registry-based task execution system for running Python functions synchronously with dependency injection and tag-based organization.</p>"},{"location":"guides/task-execution/#quick-start","title":"Quick Start","text":""},{"location":"guides/task-execution/#basic-task-registration","title":"Basic Task Registration","text":"<pre><code>from chapkit.task import TaskRegistry, TaskExecutor, TaskRouter\nfrom chapkit.api import ServiceBuilder, ServiceInfo\nfrom servicekit import Database\nfrom servicekit.api.dependencies import get_database\nfrom fastapi import Depends\n\n# Clear registry on module reload (for development)\nTaskRegistry.clear()\n\n# Register Python task with tags\n@TaskRegistry.register(\"greet_user\", tags=[\"demo\", \"simple\"])\nasync def greet_user(name: str = \"World\") -&gt; dict[str, str]:\n    \"\"\"Simple task that returns a greeting.\"\"\"\n    return {\"message\": f\"Hello, {name}!\"}\n\n# Task with dependency injection\n@TaskRegistry.register(\"process_data\", tags=[\"demo\", \"injection\"])\nasync def process_data(database: Database) -&gt; dict[str, object]:\n    \"\"\"Dependencies are automatically injected.\"\"\"\n    return {\"status\": \"processed\", \"database_url\": str(database.url)}\n\n# Build service\ninfo = ServiceInfo(id=\"task-service\", display_name=\"Task Service\")\n\ndef get_task_executor(database: Database = Depends(get_database)) -&gt; TaskExecutor:\n    \"\"\"Provide task executor for dependency injection.\"\"\"\n    return TaskExecutor(database)\n\ntask_router = TaskRouter.create(\n    prefix=\"/api/v1/tasks\",\n    tags=[\"Tasks\"],\n    executor_factory=get_task_executor,\n)\n\napp = (\n    ServiceBuilder(info=info)\n    .with_health()\n    .include_router(task_router.router)\n    .build()\n)\n</code></pre> <p>Run: <code>fastapi dev your_file.py</code></p>"},{"location":"guides/task-execution/#architecture","title":"Architecture","text":""},{"location":"guides/task-execution/#task-registration","title":"Task Registration","text":"<p>Python Functions: - Registered with <code>@TaskRegistry.register(name, tags=[])</code> - URL-safe names (alphanumeric, underscore, hyphen only) - Automatic dependency injection - Return dict with results - Async or sync functions supported</p>"},{"location":"guides/task-execution/#execution-flow","title":"Execution Flow","text":"<pre><code>1. Task Registered (in-memory)\n   @TaskRegistry.register(\"task_name\", tags=[\"tag1\"])\n\n2. Task Discovery\n   GET /api/v1/tasks\n   GET /api/v1/tasks/task_name\n\n3. Task Execution\n   POST /api/v1/tasks/task_name/$execute\n   \u251c\u2500&gt; Dependencies injected\n   \u251c\u2500&gt; Function executed synchronously\n   \u2514\u2500&gt; Result returned in response (200 OK)\n</code></pre>"},{"location":"guides/task-execution/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/task-execution/#taskregistry","title":"TaskRegistry","text":"<p>Global in-memory registry for Python task functions.</p> <pre><code>from chapkit.task import TaskRegistry\n\n# Register with decorator\n@TaskRegistry.register(\"my_task\", tags=[\"processing\", \"etl\"])\nasync def my_task(param: str) -&gt; dict[str, object]:\n    \"\"\"Task docstring.\"\"\"\n    return {\"result\": param.upper()}\n\n# Or register imperatively\ndef another_task(x: int) -&gt; dict[str, int]:\n    \"\"\"Another task.\"\"\"\n    return {\"doubled\": x * 2}\n\nTaskRegistry.register_function(\"double_it\", another_task, tags=[\"math\"])\n\n# Check registration\nassert TaskRegistry.has(\"my_task\")\n\n# Get task metadata\ninfo = TaskRegistry.get_info(\"my_task\")\nprint(info.signature)  # (param: str) -&gt; dict[str, object]\nprint(info.tags)       # [\"processing\", \"etl\"]\n\n# List all tasks\nall_tasks = TaskRegistry.list_all()  # [\"my_task\", \"double_it\"]\n\n# Filter by tags (requires ALL tags)\nmath_tasks = TaskRegistry.list_by_tags([\"math\"])  # [\"double_it\"]\n</code></pre> <p>Rules: - Task names must be URL-safe: <code>^[a-zA-Z0-9_-]+$</code> - Task names must be unique - Functions should return dict or None - Both async and sync functions supported - Parameters can have defaults</p>"},{"location":"guides/task-execution/#tags","title":"Tags","text":"<p>Tasks can be tagged for organization:</p> <pre><code>@TaskRegistry.register(\"extract_data\", tags=[\"data\", \"etl\", \"extract\"])\nasync def extract_data() -&gt; dict:\n    \"\"\"Extract data from source.\"\"\"\n    return {\"records\": 100}\n\n@TaskRegistry.register(\"transform_data\", tags=[\"data\", \"etl\", \"transform\"])\nasync def transform_data() -&gt; dict:\n    \"\"\"Transform extracted data.\"\"\"\n    return {\"transformed\": True}\n\n# Filter tasks that have ALL specified tags\netl_tasks = TaskRegistry.list_by_tags([\"data\", \"etl\"])\n# Returns: [\"extract_data\", \"transform_data\"]\n\nextract_tasks = TaskRegistry.list_by_tags([\"etl\", \"extract\"])\n# Returns: [\"extract_data\"]\n</code></pre>"},{"location":"guides/task-execution/#dependency-injection","title":"Dependency Injection","text":"<p>Tasks can request framework dependencies as function parameters:</p> <pre><code>from servicekit import Database\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\n@TaskRegistry.register(\"with_dependencies\")\nasync def with_dependencies(\n    database: Database,\n    session: AsyncSession,\n    custom_param: str = \"default\"\n) -&gt; dict[str, object]:\n    \"\"\"Dependencies automatically injected at runtime.\"\"\"\n    # Framework types are injected, user params come from request\n    return {\"database_url\": str(database.url), \"custom_param\": custom_param}\n</code></pre> <p>Available Injectable Types: - <code>AsyncSession</code> - Database session (always available) - <code>Database</code> - Database instance (always available) - <code>ChapkitScheduler</code> - Job scheduler (available when <code>.with_jobs()</code> is configured) - <code>ArtifactManager</code> - Artifact manager (available when <code>.with_artifacts()</code> is configured and passed to TaskExecutor)</p> <p>Note: Parameters are categorized automatically: - Framework types (in INJECTABLE_TYPES) are injected when available - All other parameters must be provided in execution request</p> <p>Simple setup (default): <pre><code>def get_task_executor(database: Database = Depends(get_database)) -&gt; TaskExecutor:\n    return TaskExecutor(database)\n</code></pre></p> <p>Advanced setup (with scheduler + artifacts): <pre><code>from servicekit.api.dependencies import get_scheduler\nfrom chapkit.artifact import ArtifactHierarchy, ArtifactManager, ArtifactRepository\n\nTASK_HIERARCHY = ArtifactHierarchy(name=\"task_results\", level_labels={0: \"task_run\"})\n\nasync def get_task_executor(\n    database: Database = Depends(get_database),\n    scheduler = Depends(get_scheduler),\n) -&gt; TaskExecutor:\n    async with database.session() as session:\n        artifact_repo = ArtifactRepository(session)\n        artifact_manager = ArtifactManager(artifact_repo, hierarchy=TASK_HIERARCHY)\n        return TaskExecutor(database, scheduler, artifact_manager)\n\n# Also add to service builder:\napp = (\n    ServiceBuilder(info=info)\n    .with_jobs(max_concurrency=5)\n    .with_artifacts(hierarchy=TASK_HIERARCHY)\n    .include_router(task_router.router)\n    .build()\n)\n</code></pre></p>"},{"location":"guides/task-execution/#taskinfo-schema","title":"TaskInfo Schema","text":"<p>When you retrieve task metadata, you get a TaskInfo object:</p> <pre><code>class TaskInfo(BaseModel):\n    name: str                        # URL-safe task name\n    docstring: str | None            # Function docstring\n    signature: str                   # Function signature string\n    parameters: list[ParameterInfo]  # Parameter metadata\n    tags: list[str]                  # Task tags\n\nclass ParameterInfo(BaseModel):\n    name: str                 # Parameter name\n    annotation: str | None    # Type annotation as string\n    default: str | None       # Default value as string\n    required: bool            # Whether parameter is required\n</code></pre>"},{"location":"guides/task-execution/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/task-execution/#get-apiv1tasks","title":"GET /api/v1/tasks","text":"<p>List all registered tasks with metadata.</p> <p>Response: <pre><code>[\n  {\n    \"name\": \"greet_user\",\n    \"docstring\": \"Simple task that returns a greeting.\",\n    \"signature\": \"(name: str = 'World') -&gt; dict[str, str]\",\n    \"parameters\": [\n      {\n        \"name\": \"name\",\n        \"annotation\": \"&lt;class 'str'&gt;\",\n        \"default\": \"'World'\",\n        \"required\": false\n      }\n    ],\n    \"tags\": [\"demo\", \"simple\"]\n  }\n]\n</code></pre></p> <p>Example: <pre><code># List all tasks\ncurl http://localhost:8000/api/v1/tasks\n</code></pre></p>"},{"location":"guides/task-execution/#get-apiv1tasksname","title":"GET /api/v1/tasks/{name}","text":"<p>Get task metadata by URL-safe name.</p> <p>Response: <pre><code>{\n  \"name\": \"greet_user\",\n  \"docstring\": \"Simple task that returns a greeting.\",\n  \"signature\": \"(name: str = 'World') -&gt; dict[str, str]\",\n  \"parameters\": [\n    {\n      \"name\": \"name\",\n      \"annotation\": \"&lt;class 'str'&gt;\",\n      \"default\": \"'World'\",\n      \"required\": false\n    }\n  ],\n  \"tags\": [\"demo\", \"simple\"]\n}\n</code></pre></p> <p>Errors: - 404 Not Found: Task not registered</p>"},{"location":"guides/task-execution/#post-apiv1tasksnameexecute","title":"POST /api/v1/tasks/{name}/$execute","text":"<p>Execute task by name with runtime parameters.</p> <p>Request: <pre><code>{\n  \"params\": {\n    \"name\": \"Alice\"\n  }\n}\n</code></pre></p> <p>Response (200 OK): <pre><code>{\n  \"task_name\": \"greet_user\",\n  \"params\": {\"name\": \"Alice\"},\n  \"result\": {\"message\": \"Hello, Alice!\"},\n  \"error\": null\n}\n</code></pre></p> <p>Response on Error (200 OK): <pre><code>{\n  \"task_name\": \"greet_user\",\n  \"params\": {\"name\": \"Alice\"},\n  \"result\": null,\n  \"error\": {\n    \"type\": \"ValueError\",\n    \"message\": \"Invalid parameter\",\n    \"traceback\": \"Traceback (most recent call last)...\"\n  }\n}\n</code></pre></p> <p>Errors: - 404 Not Found: Task not registered</p> <p>Examples: <pre><code># Execute without parameters\ncurl -X POST http://localhost:8000/api/v1/tasks/greet_user/\\$execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n\n# Execute with parameters\ncurl -X POST http://localhost:8000/api/v1/tasks/greet_user/\\$execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": {\"name\": \"Bob\"}}'\n\n# Execute task with multiple parameters\ncurl -X POST http://localhost:8000/api/v1/tasks/multiply_numbers/\\$execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": {\"a\": 6, \"b\": 7}}'\n</code></pre></p>"},{"location":"guides/task-execution/#task-patterns","title":"Task Patterns","text":""},{"location":"guides/task-execution/#simple-task","title":"Simple Task","text":"<pre><code>@TaskRegistry.register(\"hello\", tags=[\"demo\"])\nasync def hello() -&gt; dict[str, str]:\n    \"\"\"Simple hello world task.\"\"\"\n    return {\"message\": \"Hello!\"}\n</code></pre>"},{"location":"guides/task-execution/#task-with-parameters","title":"Task with Parameters","text":"<pre><code>@TaskRegistry.register(\"add\", tags=[\"math\"])\nasync def add(a: int, b: int) -&gt; dict[str, int]:\n    \"\"\"Add two numbers.\"\"\"\n    return {\"result\": a + b}\n\n# Execute:\n# POST /api/v1/tasks/add/$execute\n# {\"params\": {\"a\": 5, \"b\": 3}}\n</code></pre>"},{"location":"guides/task-execution/#task-with-optional-parameters","title":"Task with Optional Parameters","text":"<pre><code>@TaskRegistry.register(\"greet\", tags=[\"demo\"])\nasync def greet(name: str = \"World\", greeting: str = \"Hello\") -&gt; dict[str, str]:\n    \"\"\"Greet someone.\"\"\"\n    return {\"message\": f\"{greeting}, {name}!\"}\n\n# Execute with defaults:\n# POST /api/v1/tasks/greet/$execute\n# {}\n\n# Execute with custom values:\n# POST /api/v1/tasks/greet/$execute\n# {\"params\": {\"name\": \"Alice\", \"greeting\": \"Hi\"}}\n</code></pre>"},{"location":"guides/task-execution/#task-with-dependency-injection","title":"Task with Dependency Injection","text":"<pre><code>@TaskRegistry.register(\"store_result\", tags=[\"storage\"])\nasync def store_result(\n    artifact_manager: ArtifactManager,\n    data: dict\n) -&gt; dict[str, object]:\n    \"\"\"Store result in artifact.\"\"\"\n    artifact = await artifact_manager.save(ArtifactIn(data=data))\n    return {\"artifact_id\": str(artifact.id)}\n\n# Execute:\n# POST /api/v1/tasks/store_result/$execute\n# {\"params\": {\"data\": {\"key\": \"value\"}}}\n# Note: artifact_manager is injected, only data needs to be provided\n</code></pre>"},{"location":"guides/task-execution/#database-query-task","title":"Database Query Task","text":"<pre><code>@TaskRegistry.register(\"count_users\", tags=[\"database\", \"reporting\"])\nasync def count_users(database: Database) -&gt; dict[str, int]:\n    \"\"\"Count users in database.\"\"\"\n    async with database.session() as session:\n        from sqlalchemy import select, func\n        from myapp.models import User\n\n        stmt = select(func.count(User.id))\n        result = await session.execute(stmt)\n        count = result.scalar()\n\n    return {\"user_count\": count}\n</code></pre>"},{"location":"guides/task-execution/#file-processing-task","title":"File Processing Task","text":"<pre><code>@TaskRegistry.register(\"process_csv\", tags=[\"data\", \"processing\"])\nasync def process_csv(filepath: str) -&gt; dict[str, object]:\n    \"\"\"Process CSV file.\"\"\"\n    import pandas as pd\n\n    df = pd.read_csv(filepath)\n    summary = {\n        \"rows\": len(df),\n        \"columns\": list(df.columns),\n        \"summary\": df.describe().to_dict()\n    }\n\n    return summary\n</code></pre>"},{"location":"guides/task-execution/#synchronous-task","title":"Synchronous Task","text":"<pre><code>@TaskRegistry.register(\"multiply\", tags=[\"math\"])\ndef multiply(a: int, b: int) -&gt; dict[str, int]:\n    \"\"\"Synchronous task that multiplies numbers.\"\"\"\n    # Sync functions are automatically wrapped for async execution\n    return {\"result\": a * b}\n</code></pre>"},{"location":"guides/task-execution/#shell-command-task","title":"Shell Command Task","text":"<pre><code>from chapkit.task import run_shell\n\n@TaskRegistry.register(\"backup_database\", tags=[\"admin\", \"backup\"])\nasync def backup_database(database_url: str, s3_bucket: str) -&gt; dict[str, object]:\n    \"\"\"Backup database to S3 using shell commands.\"\"\"\n    # Dump database\n    dump_result = await run_shell(\n        f\"pg_dump {database_url} | gzip &gt; /tmp/backup.sql.gz\",\n        timeout=300.0\n    )\n\n    if dump_result[\"returncode\"] != 0:\n        return {\n            \"status\": \"failed\",\n            \"step\": \"dump\",\n            \"error\": dump_result[\"stderr\"]\n        }\n\n    # Upload to S3\n    upload_result = await run_shell(\n        f\"aws s3 cp /tmp/backup.sql.gz s3://{s3_bucket}/backup.sql.gz\",\n        timeout=60.0\n    )\n\n    if upload_result[\"returncode\"] != 0:\n        return {\n            \"status\": \"failed\",\n            \"step\": \"upload\",\n            \"error\": upload_result[\"stderr\"]\n        }\n\n    return {\n        \"status\": \"success\",\n        \"size\": len(dump_result[\"stdout\"])\n    }\n\n# Simple shell command task\n@TaskRegistry.register(\"run_command\", tags=[\"demo\", \"subprocess\"])\nasync def run_command(command: str) -&gt; dict[str, object]:\n    \"\"\"Run a shell command.\"\"\"\n    return await run_shell(command)\n\n# Shell command with custom working directory and timeout\n@TaskRegistry.register(\"list_files\", tags=[\"filesystem\"])\nasync def list_files(directory: str = \".\") -&gt; dict[str, object]:\n    \"\"\"List files in directory.\"\"\"\n    result = await run_shell(\"ls -la\", cwd=directory, timeout=5.0)\n    return {\n        \"directory\": directory,\n        \"output\": result[\"stdout\"],\n        \"success\": result[\"returncode\"] == 0\n    }\n</code></pre> <p>run_shell() options: - <code>command: str</code> - Shell command to execute - <code>timeout: float | None</code> - Optional timeout in seconds - <code>cwd: str | Path | None</code> - Optional working directory - <code>env: dict[str, str] | None</code> - Optional environment variables</p> <p>Returns dict with: - <code>command: str</code> - The command that was executed - <code>stdout: str</code> - Standard output (decoded) - <code>stderr: str</code> - Standard error (decoded) - <code>returncode: int</code> - Exit code (0 = success, -1 = timeout)</p> <p>Note: <code>run_shell()</code> never raises exceptions for non-zero exit codes. Always check <code>returncode</code> in the result.</p>"},{"location":"guides/task-execution/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/task-execution/#when-to-use-simple-vs-advanced-setup","title":"When to Use Simple vs Advanced Setup","text":"<p>Use Simple Setup (default) when: - Tasks execute quickly (&lt; 5 seconds) - Results can be returned directly in HTTP response - No need for background job scheduling - No need for persistent artifact storage</p> <p>Use Advanced Setup when: - Tasks need to spawn background jobs for long-running operations - Tasks need to store results in artifact hierarchy for audit/retrieval - Tasks need job scheduling capabilities (retry, scheduling, etc.) - Building a system with complex task orchestration</p> <p>Use ML Module instead when: - Building train/predict workflows - Need versioned model storage - Need experiment tracking - Need standardized ML pipeline</p>"},{"location":"guides/task-execution/#task-with-background-job-scheduling","title":"Task with Background Job Scheduling","text":"<p>When a task needs to spawn long-running background work:</p> <pre><code>from chapkit.scheduler import ChapkitScheduler\n\n@TaskRegistry.register(\"spawn_background_job\", tags=[\"admin\", \"background\"])\nasync def spawn_background_job(\n    scheduler: ChapkitScheduler,\n    processing_time: int = 60\n) -&gt; dict[str, object]:\n    \"\"\"Task that spawns a background job for long-running work.\"\"\"\n\n    async def background_work():\n        \"\"\"The actual long-running work.\"\"\"\n        import asyncio\n        await asyncio.sleep(processing_time)\n        return {\"status\": \"completed\", \"processing_time\": processing_time}\n\n    # Spawn background job\n    job_id = await scheduler.spawn(\n        background_work(),\n        description=f\"Background processing ({processing_time}s)\"\n    )\n\n    return {\n        \"message\": \"Background job started\",\n        \"job_id\": str(job_id),\n        \"check_status\": f\"/api/v1/jobs/{job_id}\"\n    }\n\n# Execute:\n# POST /api/v1/tasks/spawn_background_job/$execute\n# {\"params\": {\"processing_time\": 120}}\n#\n# Response (immediate):\n# {\n#   \"task_name\": \"spawn_background_job\",\n#   \"result\": {\n#     \"message\": \"Background job started\",\n#     \"job_id\": \"01234567-89ab-cdef-0123-456789abcdef\",\n#     \"check_status\": \"/api/v1/jobs/01234567-89ab-cdef-0123-456789abcdef\"\n#   }\n# }\n#\n# Then check job status:\n# GET /api/v1/jobs/01234567-89ab-cdef-0123-456789abcdef\n</code></pre>"},{"location":"guides/task-execution/#task-with-artifact-storage","title":"Task with Artifact Storage","text":"<p>When a task needs to store results in the artifact hierarchy:</p> <pre><code>from chapkit.artifact import ArtifactManager, ArtifactIn\n\n@TaskRegistry.register(\"store_analysis_results\", tags=[\"analytics\", \"storage\"])\nasync def store_analysis_results(\n    artifact_manager: ArtifactManager,\n    dataset_name: str,\n    analysis_type: str\n) -&gt; dict[str, object]:\n    \"\"\"Task that stores analysis results as artifacts.\"\"\"\n\n    # Perform analysis\n    results = {\n        \"dataset\": dataset_name,\n        \"type\": analysis_type,\n        \"metrics\": {\n            \"accuracy\": 0.95,\n            \"precision\": 0.92,\n            \"recall\": 0.89\n        },\n        \"timestamp\": \"2024-01-15T10:30:00Z\"\n    }\n\n    # Store in artifact hierarchy\n    artifact = await artifact_manager.save(\n        ArtifactIn(\n            data=results,\n            metadata={\n                \"dataset\": dataset_name,\n                \"analysis_type\": analysis_type\n            }\n        )\n    )\n\n    return {\n        \"status\": \"stored\",\n        \"artifact_id\": str(artifact.id),\n        \"retrieve_url\": f\"/api/v1/artifacts/{artifact.id}\"\n    }\n\n# Execute:\n# POST /api/v1/tasks/store_analysis_results/$execute\n# {\"params\": {\"dataset_name\": \"customer_churn\", \"analysis_type\": \"classification\"}}\n#\n# Response:\n# {\n#   \"task_name\": \"store_analysis_results\",\n#   \"result\": {\n#     \"status\": \"stored\",\n#     \"artifact_id\": \"01234567-89ab-cdef-0123-456789abcdef\",\n#     \"retrieve_url\": \"/api/v1/artifacts/01234567-89ab-cdef-0123-456789abcdef\"\n#   }\n# }\n</code></pre>"},{"location":"guides/task-execution/#task-with-both-scheduler-and-artifacts","title":"Task with Both Scheduler and Artifacts","text":"<p>Combining job scheduling with artifact storage for complex workflows:</p> <pre><code>@TaskRegistry.register(\"orchestrate_pipeline\", tags=[\"pipeline\", \"orchestration\"])\nasync def orchestrate_pipeline(\n    scheduler: ChapkitScheduler,\n    artifact_manager: ArtifactManager,\n    pipeline_config: dict\n) -&gt; dict[str, object]:\n    \"\"\"Task that orchestrates multi-step pipeline with job tracking and artifact storage.\"\"\"\n\n    # Store pipeline configuration as artifact\n    config_artifact = await artifact_manager.save(\n        ArtifactIn(\n            data=pipeline_config,\n            metadata={\"type\": \"pipeline_config\"}\n        )\n    )\n\n    # Define pipeline steps as background jobs\n    async def step_1():\n        result = {\"step\": 1, \"status\": \"completed\", \"data\": [1, 2, 3]}\n        # Store step result\n        await artifact_manager.save(ArtifactIn(\n            data=result,\n            metadata={\"step\": 1, \"pipeline_config_id\": str(config_artifact.id)}\n        ))\n        return result\n\n    async def step_2():\n        result = {\"step\": 2, \"status\": \"completed\", \"data\": [4, 5, 6]}\n        await artifact_manager.save(ArtifactIn(\n            data=result,\n            metadata={\"step\": 2, \"pipeline_config_id\": str(config_artifact.id)}\n        ))\n        return result\n\n    # Spawn jobs for each step\n    job_1 = await scheduler.spawn(step_1(), description=\"Pipeline Step 1\")\n    job_2 = await scheduler.spawn(step_2(), description=\"Pipeline Step 2\")\n\n    return {\n        \"message\": \"Pipeline started\",\n        \"config_artifact_id\": str(config_artifact.id),\n        \"jobs\": {\n            \"step_1\": str(job_1),\n            \"step_2\": str(job_2)\n        },\n        \"monitor\": {\n            \"jobs\": \"/api/v1/jobs\",\n            \"artifacts\": f\"/api/v1/artifacts/{config_artifact.id}/$expand\"\n        }\n    }\n</code></pre>"},{"location":"guides/task-execution/#advanced-setup-configuration","title":"Advanced Setup Configuration","text":"<p>Complete service setup with scheduler and artifact injection:</p> <pre><code>from fastapi import Depends, FastAPI\nfrom servicekit import Database\nfrom servicekit.api.dependencies import get_database, get_scheduler\n\nfrom chapkit.api import ServiceBuilder, ServiceInfo\nfrom chapkit.artifact import ArtifactHierarchy, ArtifactManager, ArtifactRepository\nfrom chapkit.scheduler import ChapkitScheduler\nfrom chapkit.task import TaskExecutor, TaskRouter\n\n# Define artifact hierarchy for task results\nTASK_HIERARCHY = ArtifactHierarchy(\n    name=\"task_results\",\n    level_labels={0: \"task_run\"}\n)\n\n# Advanced executor factory with scheduler and artifacts\nasync def get_task_executor(\n    database: Database = Depends(get_database),\n    scheduler = Depends(get_scheduler),\n) -&gt; TaskExecutor:\n    \"\"\"Provide task executor with scheduler and artifact manager.\"\"\"\n    async with database.session() as session:\n        artifact_repo = ArtifactRepository(session)\n        artifact_manager = ArtifactManager(artifact_repo, hierarchy=TASK_HIERARCHY)\n        if isinstance(scheduler, ChapkitScheduler):\n            return TaskExecutor(database, scheduler, artifact_manager)\n        return TaskExecutor(database)\n\n# Create task router\ntask_router = TaskRouter.create(\n    prefix=\"/api/v1/tasks\",\n    tags=[\"Tasks\"],\n    executor_factory=get_task_executor,\n)\n\n# Build service with jobs and artifacts\napp: FastAPI = (\n    ServiceBuilder(info=ServiceInfo(id=\"advanced-task-service\", display_name=\"Advanced Task Service\"))\n    .with_landing_page()\n    .with_logging()\n    .with_health()\n    .with_system()\n    .with_jobs(max_concurrency=5)          # Enable job scheduler\n    .with_artifacts(hierarchy=TASK_HIERARCHY)  # Enable artifact storage\n    .include_router(task_router.router)\n    .build()\n)\n</code></pre>"},{"location":"guides/task-execution/#decision-guide-tasks-vs-ml-module","title":"Decision Guide: Tasks vs ML Module","text":"<p>Use Task Execution when: - Running general-purpose Python functions - Need flexible dependency injection - Building custom workflows - Tasks are ephemeral or self-contained - Mix of data processing, admin, ETL, etc.</p> <p>Use ML Module when: - Specifically doing ML train/predict - Need standardized ML workflow - Need model versioning - Need experiment tracking - Want automatic model artifact management</p> <p>Example: When to use each <pre><code># Use Task Execution for this:\n@TaskRegistry.register(\"process_user_data\")\nasync def process_user_data(database: Database, user_id: str):\n    \"\"\"Custom business logic.\"\"\"\n    # ... custom processing\n    return {\"processed\": True}\n\n# Use ML Module for this:\nfrom chapkit.ml import MLManager\nml_manager = MLManager(...)\nawait ml_manager.train(TrainRequest(\n    model_name=\"customer_churn\",\n    parameters={\"n_estimators\": 100}\n))\n</code></pre></p>"},{"location":"guides/task-execution/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code># Start service\nfastapi dev main.py\n\n# List all tasks\ncurl http://localhost:8000/api/v1/tasks | jq\n\n# Get task metadata\ncurl http://localhost:8000/api/v1/tasks/greet_user | jq\n\n# Execute task and get result immediately\ncurl -s -X POST http://localhost:8000/api/v1/tasks/greet_user/\\$execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"params\": {\"name\": \"Alice\"}}' | jq\n\n# Expected response:\n# {\n#   \"task_name\": \"greet_user\",\n#   \"params\": {\"name\": \"Alice\"},\n#   \"result\": {\"message\": \"Hello, Alice!\"},\n#   \"error\": null\n# }\n</code></pre>"},{"location":"guides/task-execution/#result-storage","title":"Result Storage","text":"<p>Task execution results are ephemeral - they are returned directly in the HTTP response and not persisted.</p> <p>For task history/persistence needs: Use the ML module instead, which provides artifact-based storage for train/predict workflows.</p> <p>Response structure for successful execution: <pre><code>{\n    \"task_name\": \"greet_user\",\n    \"params\": {\"name\": \"Alice\"},\n    \"result\": {\"message\": \"Hello, Alice!\"},\n    \"error\": null\n}\n</code></pre></p> <p>Response structure for failed execution: <pre><code>{\n    \"task_name\": \"failing_task\",\n    \"params\": {},\n    \"result\": null,\n    \"error\": {\n        \"type\": \"ValueError\",\n        \"message\": \"Something went wrong\",\n        \"traceback\": \"Traceback (most recent call last)...\"\n    }\n}\n</code></pre></p>"},{"location":"guides/task-execution/#testing","title":"Testing","text":""},{"location":"guides/task-execution/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom servicekit import Database, SqliteDatabaseBuilder\nfrom chapkit.task import TaskRegistry, TaskExecutor\n\n# Clear registry before tests\nTaskRegistry.clear()\n\n@TaskRegistry.register(\"test_task\", tags=[\"test\"])\nasync def test_task(value: str) -&gt; dict[str, str]:\n    \"\"\"Test task.\"\"\"\n    return {\"result\": value.upper()}\n\n@pytest.fixture\nasync def database() -&gt; Database:\n    \"\"\"Create in-memory database for testing.\"\"\"\n    db = SqliteDatabaseBuilder().in_memory().build()\n    await db.init()\n    return db\n\n@pytest.fixture\ndef task_executor(database: Database) -&gt; TaskExecutor:\n    \"\"\"Create task executor.\"\"\"\n    return TaskExecutor(database)\n\n@pytest.mark.asyncio\nasync def test_task_execution(task_executor: TaskExecutor):\n    \"\"\"Test task execution returns result directly.\"\"\"\n    # Execute task\n    result = await task_executor.execute(\"test_task\", {\"value\": \"hello\"})\n\n    # Verify result\n    assert result[\"result\"] == \"HELLO\"\n\n@pytest.mark.asyncio\nasync def test_task_registry():\n    \"\"\"Test task registry.\"\"\"\n    # Verify registration\n    assert TaskRegistry.has(\"test_task\")\n\n    # Get metadata\n    info = TaskRegistry.get_info(\"test_task\")\n    assert info.name == \"test_task\"\n    assert info.tags == [\"test\"]\n\n@pytest.mark.asyncio\nasync def test_dependency_injection(database: Database, task_executor: TaskExecutor):\n    \"\"\"Test dependency injection.\"\"\"\n    @TaskRegistry.register(\"test_injection\")\n    async def task_with_db(db: Database, param: str) -&gt; dict[str, str]:\n        \"\"\"Task with injected database.\"\"\"\n        return {\"param\": param, \"db_injected\": db is not None}\n\n    result = await task_executor.execute(\"test_injection\", {\"param\": \"test\"})\n    assert result[\"param\"] == \"test\"\n    assert result[\"db_injected\"] is True\n</code></pre>"},{"location":"guides/task-execution/#integration-tests","title":"Integration Tests","text":"<pre><code>from fastapi.testclient import TestClient\n\ndef test_task_workflow(client: TestClient):\n    \"\"\"Test complete task workflow.\"\"\"\n    # List tasks\n    response = client.get(\"/api/v1/tasks\")\n    assert response.status_code == 200\n    tasks = response.json()\n    assert len(tasks) &gt; 0\n\n    # Get task metadata\n    task_name = tasks[0][\"name\"]\n    response = client.get(f\"/api/v1/tasks/{task_name}\")\n    assert response.status_code == 200\n    task_info = response.json()\n    assert task_info[\"name\"] == task_name\n\n    # Execute task - gets result immediately\n    exec_response = client.post(\n        f\"/api/v1/tasks/{task_name}/$execute\",\n        json={\"params\": {\"name\": \"Test\"}}\n    )\n    assert exec_response.status_code == 200\n    data = exec_response.json()\n\n    # Verify response structure\n    assert data[\"task_name\"] == task_name\n    assert data[\"params\"] == {\"name\": \"Test\"}\n    assert data[\"result\"] is not None\n    assert data[\"error\"] is None\n\ndef test_task_error_handling(client: TestClient):\n    \"\"\"Test task error handling.\"\"\"\n    # Execute task that will fail\n    response = client.post(\n        \"/api/v1/tasks/failing_task/$execute\",\n        json={\"params\": {}}\n    )\n    assert response.status_code == 200\n    data = response.json()\n\n    # Error captured in response\n    assert data[\"result\"] is None\n    assert data[\"error\"] is not None\n    assert \"type\" in data[\"error\"]\n    assert \"message\" in data[\"error\"]\n    assert \"traceback\" in data[\"error\"]\n</code></pre>"},{"location":"guides/task-execution/#production-considerations","title":"Production Considerations","text":""},{"location":"guides/task-execution/#error-handling","title":"Error Handling","text":"<p>Tasks should handle errors gracefully:</p> <pre><code>@TaskRegistry.register(\"safe_task\", tags=[\"production\"])\nasync def safe_task(risky_param: str) -&gt; dict[str, object]:\n    \"\"\"Task with error handling.\"\"\"\n    try:\n        result = process_risky_operation(risky_param)\n        return {\"status\": \"success\", \"result\": result}\n    except Exception as e:\n        # Error will be captured in response automatically\n        # but you can also return error status for app-level handling\n        return {\"status\": \"error\", \"error\": str(e)}\n</code></pre> <p>Note: Even if a task raises an exception, the TaskExecutor will catch it and return the error in the HTTP response with full traceback. Tasks execute synchronously and return results/errors directly.</p>"},{"location":"guides/task-execution/#execution-timeout","title":"Execution Timeout","text":"<p>Since tasks execute synchronously, consider the HTTP request timeout:</p> <pre><code>@TaskRegistry.register(\"quick_task\", tags=[\"production\"])\nasync def quick_task(data: dict) -&gt; dict[str, object]:\n    \"\"\"Task should complete quickly (&lt; 30s recommended).\"\"\"\n    # Process data quickly\n    result = fast_processing(data)\n    return {\"result\": result}\n\n# For longer operations, use advanced setup with scheduler\n@TaskRegistry.register(\"long_operation\", tags=[\"production\"])\nasync def long_operation(scheduler: ChapkitScheduler, data: dict) -&gt; dict[str, object]:\n    \"\"\"Spawn background job for long-running work.\"\"\"\n    async def background_work():\n        return slow_processing(data)\n\n    job_id = await scheduler.spawn(background_work(), description=\"Long operation\")\n    return {\n        \"status\": \"started\",\n        \"job_id\": str(job_id),\n        \"check_at\": f\"/api/v1/jobs/{job_id}\"\n    }\n</code></pre>"},{"location":"guides/task-execution/#concurrency-control-advanced-setup-only","title":"Concurrency Control (Advanced Setup Only)","text":"<p>When using advanced setup with <code>.with_jobs()</code>, you can limit concurrent background jobs:</p> <pre><code>app = (\n    ServiceBuilder(info=ServiceInfo(id=\"task-service\", display_name=\"Task Service\"))\n    .with_jobs(max_concurrency=5)  # Max 5 concurrent background jobs\n    .with_artifacts(hierarchy=TASK_HIERARCHY)\n    .build()\n)\n</code></pre> <p>Note: This only limits background jobs spawned by tasks, not the synchronous task execution itself. FastAPI handles concurrent HTTP requests based on its own worker configuration.</p>"},{"location":"guides/task-execution/#long-running-tasks-advanced-setup","title":"Long-Running Tasks (Advanced Setup)","text":"<p>For tasks that need progress tracking, use advanced setup with artifacts:</p> <pre><code>from chapkit.artifact import ArtifactManager, ArtifactIn\n\n@TaskRegistry.register(\"long_task\", tags=[\"processing\", \"batch\"])\nasync def long_task(artifact_manager: ArtifactManager) -&gt; dict[str, object]:\n    \"\"\"Task with progress tracking via artifacts.\"\"\"\n    total_steps = 10\n    results = []\n\n    for i in range(total_steps):\n        # Do work\n        step_result = await process_step(i)\n        results.append(step_result)\n\n        # Store intermediate progress\n        await artifact_manager.save(ArtifactIn(\n            data={\"step\": i, \"total\": total_steps, \"result\": step_result}\n        ))\n\n    return {\n        \"status\": \"complete\",\n        \"steps_completed\": total_steps,\n        \"results\": results\n    }\n</code></pre> <p>Or spawn a background job:</p> <pre><code>@TaskRegistry.register(\"long_batch\", tags=[\"processing\", \"batch\"])\nasync def long_batch(\n    scheduler: ChapkitScheduler,\n    artifact_manager: ArtifactManager\n) -&gt; dict[str, object]:\n    \"\"\"Spawn background job for truly long operations.\"\"\"\n\n    async def batch_work():\n        results = []\n        for i in range(100):\n            result = await process_step(i)\n            results.append(result)\n        # Store final result\n        artifact = await artifact_manager.save(ArtifactIn(data={\"results\": results}))\n        return {\"artifact_id\": str(artifact.id)}\n\n    job_id = await scheduler.spawn(batch_work(), description=\"Batch processing\")\n    return {\"job_id\": str(job_id), \"check_at\": f\"/api/v1/jobs/{job_id}\"}\n</code></pre>"},{"location":"guides/task-execution/#task-organization-with-tags","title":"Task Organization with Tags","text":"<p>Use tags for effective task organization:</p> <pre><code># By functionality\n@TaskRegistry.register(\"extract_data\", tags=[\"etl\", \"extract\"])\n@TaskRegistry.register(\"transform_data\", tags=[\"etl\", \"transform\"])\n@TaskRegistry.register(\"load_data\", tags=[\"etl\", \"load\"])\n\n# By environment\n@TaskRegistry.register(\"dev_setup\", tags=[\"dev\", \"setup\"])\n@TaskRegistry.register(\"prod_setup\", tags=[\"prod\", \"setup\"])\n\n# By priority\n@TaskRegistry.register(\"urgent_task\", tags=[\"high-priority\"])\n@TaskRegistry.register(\"batch_task\", tags=[\"low-priority\", \"batch\"])\n</code></pre>"},{"location":"guides/task-execution/#hot-reload-during-development","title":"Hot Reload During Development","text":"<p>Clear the registry when your module reloads:</p> <pre><code># At top of your main.py\nTaskRegistry.clear()\n\n# Then register tasks\n@TaskRegistry.register(\"my_task\")\nasync def my_task():\n    ...\n</code></pre> <p>This prevents duplicate registration errors during development.</p>"},{"location":"guides/task-execution/#complete-example","title":"Complete Example","text":"<p>See <code>examples/task_execution/main.py</code> for a complete working example with: - Multiple task types (simple, with parameters, with injection) - Tag-based organization - Dependency injection - Artifact integration - Service configuration</p>"},{"location":"guides/task-execution/#next-steps","title":"Next Steps","text":"<ul> <li>Job Scheduler: Learn about job monitoring and concurrency control</li> <li>Artifact Storage: Understand artifact hierarchies and result storage</li> <li>Service Builder: Configure services with multiple features</li> <li>Monitoring: Track task execution metrics</li> </ul>"},{"location":"guides/testing-ml-services/","title":"Testing ML Services","text":"<p>This guide covers how to test chapkit ML services during development.</p>"},{"location":"guides/testing-ml-services/#using-the-chapkit-test-command","title":"Using the <code>chapkit test</code> Command","text":"<p>The <code>chapkit test</code> command runs end-to-end tests against your ML service, verifying the complete workflow from config creation through training and prediction.</p> <p>Note: This command only appears when running <code>chapkit</code> from inside a chapkit project directory (a directory containing <code>main.py</code> with chapkit imports).</p>"},{"location":"guides/testing-ml-services/#basic-usage","title":"Basic Usage","text":"<p>First, start your service:</p> <pre><code>uv run python main.py\n</code></pre> <p>Then in another terminal, run the test:</p> <pre><code>chapkit test\n</code></pre>"},{"location":"guides/testing-ml-services/#auto-starting-the-service","title":"Auto-Starting the Service","text":"<p>Use <code>--start-service</code> to automatically start the service with an in-memory database:</p> <pre><code>chapkit test --start-service\n</code></pre> <p>This is the easiest way to test your service - it handles starting and stopping the service automatically.</p>"},{"location":"guides/testing-ml-services/#command-options","title":"Command Options","text":"Option Short Default Description <code>--url</code> <code>-u</code> <code>http://localhost:8000</code> Service URL <code>--configs</code> <code>-c</code> <code>1</code> Number of configs to create <code>--trainings</code> <code>-t</code> <code>1</code> Training jobs per config <code>--predictions</code> <code>-p</code> <code>1</code> Predictions per trained model <code>--rows</code> <code>-r</code> <code>100</code> Target rows in training data (locations x periods) <code>--timeout</code> <code>60.0</code> Job completion timeout (seconds) <code>--delay</code> <code>-d</code> <code>1.0</code> Delay between job submissions (seconds) <code>--verbose</code> <code>-v</code> <code>false</code> Show detailed output <code>--start-service</code> <code>false</code> Auto-start service with in-memory DB <code>--save-data</code> <code>false</code> Save generated test data files <code>--save-data-dir</code> <code>target</code> Directory for saved test data <code>--parallel</code> <code>1</code> Number of jobs to run in parallel (experimental) <code>--debug</code> <code>false</code> Show full stack traces on errors <code>--period-type</code> <code>monthly</code> Period format: <code>monthly</code> (YYYY-mm) or <code>weekly</code> (YYYY-Wxx) <code>--geo-type</code> <code>polygon</code> Geometry type: <code>polygon</code> or <code>point</code>"},{"location":"guides/testing-ml-services/#examples","title":"Examples","text":"<p>Run a quick test with auto-start:</p> <pre><code>chapkit test --start-service\n</code></pre> <p>Run multiple configs, trainings, and predictions:</p> <pre><code>chapkit test --start-service -c 2 -t 2 -p 5 -v\n</code></pre> <p>Test against a remote service:</p> <pre><code>chapkit test --url http://my-service:8000\n</code></pre> <p>Save generated test data for inspection:</p> <pre><code>chapkit test --start-service --save-data\nls target/  # Contains JSON and CSV files for training/prediction data\n</code></pre> <p>The <code>--save-data</code> option creates: - <code>config_*.json</code> - Configuration data - <code>training_*.json</code> / <code>training_*.csv</code> - Training panel data - <code>prediction_*_historic.json</code> / <code>.csv</code> - Historic data for prediction - <code>prediction_*_future.json</code> / <code>.csv</code> - Future data for prediction - <code>geo.json</code> - GeoJSON with polygon or point geometries (if service requires geo)</p> <p>Run jobs in parallel (experimental):</p> <pre><code>chapkit test --start-service -c 2 -t 4 -p 4 --parallel 4\n</code></pre> <p>Use weekly periods instead of monthly:</p> <pre><code>chapkit test --start-service --period-type weekly --save-data\n# Generates periods like 2020-W01, 2020-W02, etc.\n</code></pre> <p>Use point geometries instead of polygons:</p> <pre><code>chapkit test --start-service --geo-type point --save-data\n# Generates Point geometries instead of Polygon in geo.json\n</code></pre>"},{"location":"guides/testing-ml-services/#generated-data-structure","title":"Generated Data Structure","text":"<p>The test data generator creates panel data for climate-health correlation analysis:</p> <pre><code>time_period, location, disease_cases, feature_0, feature_1, feature_2\n2020-01,     location_0, 42.0,        23.1,      45.2,      67.3\n2020-01,     location_1, 38.0,        25.3,      41.8,      62.1\n2020-01,     location_2, 51.0,        18.7,      52.1,      71.4\n2020-02,     location_0, 35.0,        21.4,      48.9,      65.8\n...\n</code></pre> <ul> <li>time_period: Monthly (YYYY-mm) or weekly (YYYY-Wxx) format</li> <li>location: Matches GeoJSON <code>properties.id</code> values</li> <li>disease_cases: Health outcome (positive integer as float)</li> <li>feature_N: Climate/covariate data</li> </ul> <p>Training data uses periods starting from 2020, prediction future data uses 2025.</p>"},{"location":"guides/testing-ml-services/#manual-service-startup","title":"Manual Service Startup","text":"<p>For more control, you can start the service manually with specific configurations.</p>"},{"location":"guides/testing-ml-services/#using-in-memory-database","title":"Using In-Memory Database","text":"<p>For faster testing without persistent data:</p> <pre><code>DATABASE_URL=\"sqlite+aiosqlite:///:memory:\" uv run python main.py\n</code></pre>"},{"location":"guides/testing-ml-services/#using-a-test-database-file","title":"Using a Test Database File","text":"<p>To persist test data for debugging:</p> <pre><code>DATABASE_URL=\"sqlite+aiosqlite:///test_data/test.db\" uv run python main.py\n</code></pre>"},{"location":"guides/testing-ml-services/#testing-with-docker","title":"Testing with Docker","text":""},{"location":"guides/testing-ml-services/#build-and-run","title":"Build and Run","text":"<pre><code>docker build -t my-ml-service .\ndocker run -p 8000:8000 -e DATABASE_URL=\"sqlite+aiosqlite:///:memory:\" my-ml-service\n</code></pre> <p>Then test from the host:</p> <pre><code>chapkit test --url http://localhost:8000\n</code></pre>"},{"location":"guides/testing-ml-services/#docker-compose","title":"Docker Compose","text":"<pre><code># compose.test.yml\nservices:\n  service:\n    build: .\n    environment:\n      - DATABASE_URL=sqlite+aiosqlite:///:memory:\n    ports:\n      - \"8000:8000\"\n</code></pre> <pre><code>docker compose -f compose.test.yml up -d\nchapkit test\ndocker compose -f compose.test.yml down\n</code></pre>"},{"location":"guides/testing-ml-services/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"guides/testing-ml-services/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Test ML Service\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.13'\n\n      - name: Install uv\n        run: pip install uv\n\n      - name: Install dependencies\n        run: uv sync\n\n      - name: Run ML service tests\n        run: uv run chapkit test --start-service -c 2 -t 2 -p 5\n</code></pre>"},{"location":"guides/testing-ml-services/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/testing-ml-services/#database-lock-errors","title":"Database Lock Errors","text":"<p>If you see SQLite \"database is locked\" errors when running many predictions:</p> <ol> <li>Use <code>--start-service</code> which uses an in-memory database</li> <li>Or manually start with in-memory: <code>DATABASE_URL=\"sqlite+aiosqlite:///:memory:\"</code></li> <li>Increase the delay between jobs: <code>--delay 2</code></li> </ol>"},{"location":"guides/testing-ml-services/#service-not-ready","title":"Service Not Ready","text":"<p>If the service takes a long time to start:</p> <ol> <li>The default wait timeout is 30 seconds</li> <li>Check service logs for startup errors</li> <li>Ensure all dependencies are installed</li> </ol>"},{"location":"guides/testing-ml-services/#connection-refused","title":"Connection Refused","text":"<p>If you get \"Cannot connect\" errors:</p> <ol> <li>Verify the service is running</li> <li>Check the URL matches the service address</li> <li>Ensure no firewall is blocking the port</li> </ol>"}]}